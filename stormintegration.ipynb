{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIWVrk5xLcR9a1ZH05d/Xz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshal292004/KGPDSH/blob/master/stormintegration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "F2Mzv87hb_iG",
        "outputId": "0a677525-eab2-43f0-b4ec-b17f0ef55b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-0cc3953f7e89>:17: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator('score')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing some input keys: {'0, 1, . . . , N− 1', '1, 2, . . . , T', '0, 1, . . . , T− 1', '1, 2, . . . , N'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0cc3953f7e89>\u001b[0m in \u001b[0;36m<cell line: 179>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Publishability Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mpublishability_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPublishabilityEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mpublishability_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpublishability_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_publishability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpublishability_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublishable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0cc3953f7e89>\u001b[0m in \u001b[0;36mevaluate_publishability\u001b[0;34m(self, paper_text)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"paper_text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpaper_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mcleaned_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\x00-\\x1F\\x7F]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove control characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    387\u001b[0m         }\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         )\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[1;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m_validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mmissing_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing some input keys: {missing_keys}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'0, 1, . . . , N− 1', '1, 2, . . . , T', '0, 1, . . . , T− 1', '1, 2, . . . , N'}"
          ]
        }
      ],
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pydantic import BaseModel, Field, validator, PrivateAttr\n",
        "from typing import List, Dict\n",
        "from google.colab import userdata\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.chains import LLMChain\n",
        "import re, json\n",
        "import uuid\n",
        "\n",
        "class ScoreDetails(BaseModel):\n",
        "    score: float = Field(description=\"Score of the aspect (0-10)\")\n",
        "    justification: str = Field(description=\"Explanation for the score\")\n",
        "\n",
        "    @validator('score')\n",
        "    def validate_score(cls, v: float) -> float:\n",
        "        if v < 0 or v > 10:\n",
        "            raise ValueError('Score must be between 0 and 10')\n",
        "        return round(v, 1)\n",
        "\n",
        "# Evaluation Categories for Publishability Assessment\n",
        "class MethodologyEvaluation(BaseModel):\n",
        "    research_design: ScoreDetails = Field(description=\"Appropriateness and quality of research design\")\n",
        "    reproducibility: ScoreDetails = Field(description=\"Clarity and completeness of methods for reproduction\")\n",
        "\n",
        "class CoherenceEvaluation(BaseModel):\n",
        "    logical_flow: ScoreDetails = Field(description=\"Logical flow and structure of the paper\")\n",
        "    clarity: ScoreDetails = Field(description=\"Clarity and effectiveness of writing\")\n",
        "\n",
        "class ValidityEvaluation(BaseModel):\n",
        "    evidence_strength: ScoreDetails = Field(description=\"Strength and relevance of evidence provided\")\n",
        "    validation: ScoreDetails = Field(description=\"Validation of claims and findings\")\n",
        "\n",
        "class PublishabilityEvaluation(BaseModel):\n",
        "    methodology: MethodologyEvaluation = Field(description=\"Detailed methodology assessment\")\n",
        "    coherence: CoherenceEvaluation = Field(description=\"Coherence and clarity assessment\")\n",
        "    validity: ValidityEvaluation = Field(description=\"Validity of claims and evidence\")\n",
        "\n",
        "class PaperEvaluation(BaseModel):\n",
        "    paper_id: str = Field(description=\"Unique identifier for the paper\")\n",
        "    publishable: bool = Field(description=\"Whether the paper is deemed publishable\")\n",
        "    scores: PublishabilityEvaluation = Field(description=\"Scores for publishability evaluation\")\n",
        "    justification: str = Field(description=\"Overall justification for publishability decision\")\n",
        "    summary_of_paper: str = Field(description=\"Summary of the paper for retrieval and further analysis\")\n",
        "\n",
        "# Implementing STORM Architecture for Conference Selection\n",
        "class ConferenceAgent:\n",
        "    def __init__(self, name: str, chat_model: ChatGroq, target_conference: str , conference_themes: str, conference_context:str):\n",
        "        self.name = name\n",
        "        self.chat_model = chat_model\n",
        "        self.target_conference = target_conference\n",
        "        self.conference_themes = conference_themes\n",
        "        self.conference_context = conference_context\n",
        "\n",
        "    def evaluate_paper(self, paper_summary: str ) -> Dict:\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            ('system',  f\"\"\"\n",
        "            You are a representative for the conference {self.target_conference}. Strictly Focus Specially on the conference themes: {self.conference_themes}\n",
        "            Use the following additional context to guide your evaluation:\n",
        "            {self.conference_context}\n",
        "\n",
        "            Evaluate the research paper based on:\n",
        "            - Relevance to conference themes\n",
        "            - Quality of methodology\n",
        "            - Novelty of contribution\n",
        "            Provide a score (0-10) and a justification for why the paper fits this conference.\n",
        "            Respond is json format with the following keys\n",
        "            - score: float\n",
        "            - justification: str\n",
        "            \"\"\"),\n",
        "            ('user', \"Paper summary: {paper_summary}\")\n",
        "        ])\n",
        "        chain = LLMChain(llm = self.chat_model, prompt=prompt)\n",
        "        response = chain.run({\"paper_summary\": paper_summary})\n",
        "        try:\n",
        "            # Preprocess the response to clean up invalid characters\n",
        "            cleaned_response = re.sub(r'[\\x00-\\x1F\\x7F]', '', response)  # Remove control characters\n",
        "            json_match = re.search(r'\\{.*\\}', cleaned_response, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_response = json_match.group(0)\n",
        "                response_dict = json.loads(json_response)\n",
        "                return {\n",
        "                    \"score\": float(response_dict[\"score\"]),\n",
        "                    \"justification\": response_dict[\"justification\"],\n",
        "                }\n",
        "            else:\n",
        "                raise ValueError(f\"No JSON object found in the response: {response}\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Failed to parse response as JSON: {response}\") from e\n",
        "\n",
        "class STORMSystem:\n",
        "    def __init__(self, agents: List[ConferenceAgent]):\n",
        "        self.agents = agents\n",
        "\n",
        "    def discuss_and_decide(self, paper_summary: str) -> Dict:\n",
        "        evaluations = []\n",
        "        for agent in self.agents:\n",
        "            evaluation = agent.evaluate_paper(paper_summary)\n",
        "            evaluations.append({\n",
        "                \"conference\": agent.target_conference,\n",
        "                \"score\": evaluation[\"score\"],\n",
        "                \"justification\": evaluation[\"justification\"],\n",
        "            })\n",
        "\n",
        "        # Determine the best conference based on scores\n",
        "        best_conference = max(evaluations, key=lambda x: x[\"score\"])\n",
        "        return {\n",
        "            \"best_conference\": best_conference[\"conference\"],\n",
        "            \"justification\": best_conference[\"justification\"],\n",
        "        }\n",
        "\n",
        "# Integrating Publishability Assessment\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "\n",
        "class PublishabilityEvaluator:\n",
        "    def __init__(self, chat_model: ChatGroq):\n",
        "        self.chat_model = chat_model\n",
        "\n",
        "    def evaluate_publishability(self, paper_text: str) -> dict:\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", \"\"\"\n",
        "                You are an expert evaluator tasked with determining if a research paper is publishable.\n",
        "                Evaluate the following criteria and provide scores (0-10) with justifications:\n",
        "                - Methodology: Research design and reproducibility\n",
        "                - Coherence: Logical flow and clarity\n",
        "                - Validity: Evidence strength and validation of claims\n",
        "\n",
        "                Based on the scores, decide if the paper is publishable and provide an overall justification.\n",
        "                \"\"\"),\n",
        "                (\"user\", \"Paper Text: {paper_text}\")\n",
        "            ])\n",
        "\n",
        "            chain = LLMChain(llm=self.chat_model, prompt=prompt)\n",
        "            response = chain.run({\"paper_text\": paper_text})\n",
        "            cleaned_response = re.sub(r'[\\x00-\\x1F\\x7F]', '', response)  # Remove control characters\n",
        "\n",
        "            # Extract the scores and justifications using regex\n",
        "            methodology_match = re.search(r\"1\\. Methodology.*?Score: (\\d+)/10\\s+(.*?)2\\. Coherence\", cleaned_response, re.DOTALL)\n",
        "            coherence_match = re.search(r\"2\\. Coherence.*?Score: (\\d+)/10\\s+(.*?)3\\. Validity\", cleaned_response, re.DOTALL)\n",
        "            validity_match = re.search(r\"3\\. Validity.*?Score: (\\d+)/10\\s+(.*?)Overall Score\", cleaned_response, re.DOTALL)\n",
        "            overall_match = re.search(r\"Overall Score: ([\\d.]+)/10.*?Publishability:\\s*(.*?)\\n\", cleaned_response, re.DOTALL)\n",
        "\n",
        "            if not (methodology_match and coherence_match and validity_match and overall_match):\n",
        "                raise ValueError(f\"Missing components in the response: {cleaned_response}\")\n",
        "\n",
        "            return {\n",
        "                \"methodology\": {\n",
        "                    \"score\": int(methodology_match.group(1)),\n",
        "                    \"justification\": methodology_match.group(2).strip()\n",
        "                },\n",
        "                \"coherence\": {\n",
        "                    \"score\": int(coherence_match.group(1)),\n",
        "                    \"justification\": coherence_match.group(2).strip()\n",
        "                },\n",
        "                \"validity\": {\n",
        "                    \"score\": int(validity_match.group(1)),\n",
        "                    \"justification\": validity_match.group(2).strip()\n",
        "                },\n",
        "                \"overall\": {\n",
        "                    \"score\": float(overall_match.group(1)),\n",
        "                    \"publishable\": overall_match.group(2).strip()\n",
        "                }\n",
        "            }\n",
        "\n",
        "# Loading and Chunking the Paper\n",
        "def load_and_chunk_paper(file_path: str, chunk_size: int = 500, overlap: int = 50) -> str:\n",
        "    \"\"\"Load a research paper and split it into manageable chunks.\"\"\"\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load()\n",
        "    full_text = \" \".join([page.page_content for page in pages])\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "    chunks = splitter.split_text(full_text)\n",
        "    return \" \".join(chunks)  # Combine chunks for processing\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_model = ChatGroq(groq_api_key=userdata.get(\"GROQ_API_KEY\").strip(), model=\"llama3-8b-8192\")\n",
        "\n",
        "    # Load and chunk the research paper\n",
        "    paper_path = \"/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf\"\n",
        "    paper_text = load_and_chunk_paper(paper_path)\n",
        "\n",
        "    # Publishability Evaluation\n",
        "    publishability_evaluator = PublishabilityEvaluator(chat_model=chat_model)\n",
        "    publishability_result = publishability_evaluator.evaluate_publishability(paper_text)\n",
        "\n",
        "    if publishability_result.publishable:\n",
        "        # Conference Selection\n",
        "        agents = [\n",
        "            ConferenceAgent(name=\"Agent CVPR\", chat_model=chat_model, target_conference=\"CVPR\", conference_themes = \"\"\"\n",
        "                        CVPR focuses on the field of computer vision, image processing, and pattern recognition. Key themes include:\n",
        "                          Object detection and recognition.\n",
        "                          Image segmentation and scene understanding.\n",
        "                          Visual tracking and motion analysis.\n",
        "                          3D vision, stereo vision, and depth estimation.\n",
        "                          Deep learning for vision tasks (CNNs, Vision Transformers, etc.).\n",
        "                          Applications in medical imaging, autonomous vehicles, and robotics.\n",
        "                          Video processing and understanding.\n",
        "                          Computational photography and imaging.\n",
        "                          Low-level vision (denoising, super-resolution).\n",
        "                          Vision-based augmented reality and virtual reality.\n",
        "                        \"\"\",\n",
        "                        conference_context = \"\"\"\n",
        "                        Examples of accepted papers:\n",
        "                          - YOLO: Real-Time Object Detection\n",
        "                              Summary: Proposes an efficient deep learning model for real-time object detection with state-of-the-art accuracy.\n",
        "                          - SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\n",
        "                              Summary: Focuses on pixel-wise image segmentation using deep neural networks, with applications in autonomous systems.\n",
        "                          - 3D Object Detection and Localization Using RGB and Depth Data\n",
        "                              Summary: Combines RGB and depth information for accurate 3D object detection in indoor environments.\n",
        "                        Evaluation Criteria:\n",
        "                         - Relevance to core computer vision topics.\n",
        "                         - Advances in methodology or application.\n",
        "                         - Practical implications for industries like healthcare, robotics, and transportation.\n",
        "                        \"\"\"),\n",
        "            ConferenceAgent(name=\"Agent NeurIPS\", chat_model=chat_model, target_conference=\"NeurIPS\"  ,conference_themes =\"\"\"\n",
        "                        NeurIPS emphasizes machine learning, computational neuroscience, and AI-related topics. Key themes include:\n",
        "                            Deep learning (e.g., architectures, generative models, optimization for deep networks, foundation models, LLMs)\n",
        "                            Reinforcement learning and decision-making.\n",
        "                            Probabilistic models and Bayesian learning.\n",
        "                            Optimization techniques for machine learning.\n",
        "                            Representation learning and embeddings.\n",
        "                            AI for healthcare, climate science, and sustainability.\n",
        "                            Neuroscience-inspired algorithms and theories.\n",
        "                            Ethical AI, fairness, and explainability.\n",
        "                        \"\"\",\n",
        "                        conference_context = \"\"\"\n",
        "                        Examples of accepted papers:\n",
        "                          - Attention Is All You Need\n",
        "                             Summary: Introduces the transformer architecture, which revolutionized NLP and other domains by using self-attention mechanisms.\n",
        "                          - Generative Adversarial Networks (GANs)\n",
        "                             Summary: Proposes a novel framework for training generative models using adversarial networks.\n",
        "                          - Adam: A Method for Stochastic Optimization\n",
        "                             Summary: Proposes a new optimization algorithm that balances convergence speed and stability in deep learning.\n",
        "                        Evaluation Criteria:\n",
        "                          - Novelty and theoretical contributions.\n",
        "                          - Experimental rigor and reproducibility.\n",
        "                          - Broader implications for machine learning and interdisciplinary applications.\n",
        "                        \"\"\"),\n",
        "            ConferenceAgent(name=\"Agent EMNLP\", chat_model=chat_model, target_conference=\"EMNLP\" ,conference_themes = \"\"\"\n",
        "                        EMNLP specializes in NLP and computational linguistics. Key themes include:\n",
        "                          Machine translation and cross-lingual NLP.\n",
        "                          Large language models and foundational models.\n",
        "                          Sentiment analysis and opinion mining.\n",
        "                          Dialogue systems and conversational AI.\n",
        "                          Question answering and information retrieval.\n",
        "                          Text summarization and abstraction.\n",
        "                          Morphology, syntax, and semantics.\n",
        "                          Multimodal NLP (text + image/audio fusion).\n",
        "                          Ethical concerns in NLP (bias, toxicity detection).\n",
        "                          Low-resource and multilingual NLP.\n",
        "                        \"\"\",\n",
        "                        conference_context = \"\"\"\n",
        "                        Examples of accepted papers:\n",
        "\n",
        "Paper Title: Pre-trained Language Models for Text-to-Text Generation\n",
        "Summary: This paper explores pre-trained models (e.g., T5) for a variety of text-to-text NLP tasks, showcasing improvements across summarization, question answering, and translation.\n",
        "\n",
        "Paper Title: Adversarial Training for Robust Text Classification\n",
        "Summary: Proposes an adversarial training framework that enhances the robustness of text classification models against noisy and adversarial inputs.\n",
        "\n",
        "Paper Title: Knowledge-Enhanced Contextual Representations for Entity Linking\n",
        "Summary: Combines external knowledge sources with contextual embeddings to improve entity linking in complex domains.\n",
        "\n",
        "Paper Title: Multimodal Sentiment Analysis with Attention Mechanisms\n",
        "Summary: Introduces a framework for combining visual and textual inputs to improve sentiment detection in videos.\n",
        "\n",
        "Evaluation Criteria:\n",
        "Relevance to NLP:\n",
        "\n",
        "Papers should directly address core NLP tasks, methodologies, or applications.\n",
        "Emphasis on empirical evaluation and innovation in language processing methods.\n",
        "Quality of Methodology:\n",
        "\n",
        "Rigorous experiments, proper baselines, and thorough ablation studies.\n",
        "Use of diverse and large-scale datasets to validate results.\n",
        "Novelty of Contribution:\n",
        "\n",
        "New architectures, algorithms, or findings that advance the field.\n",
        "Extensions of existing methods to novel tasks or domains.\n",
        "Broader Impacts:\n",
        "\n",
        "Ethical implications of the research (e.g., bias in models, data privacy).\n",
        "Potential for cross-disciplinary applications (e.g., in healthcare, education, or social media).\n",
        "Themes Breakdown with Examples:\n",
        "Natural Language Understanding:\n",
        "\n",
        "Semantic role labeling, coreference resolution, and discourse parsing.\n",
        "Example: A New Framework for Semantic Parsing Using Pre-Trained Transformers.\n",
        "Language Generation:\n",
        "\n",
        "Dialogue systems, machine translation, and creative text generation.\n",
        "Example: Controlled Text Generation with Discrete and Continuous Latent Variables.\n",
        "Information Extraction:\n",
        "\n",
        "Named entity recognition, relation extraction, and knowledge graph construction.\n",
        "Example: Joint Entity and Relation Extraction with Transformer-Based Models.\n",
        "Multimodal NLP:\n",
        "\n",
        "Integrating vision, audio, and textual modalities.\n",
        "Example: Aligning Text and Vision for Multimodal Machine Translation.\n",
        "Social and Ethical Considerations:\n",
        "\n",
        "Bias, fairness, interpretability, and environmental impacts of NLP models.\n",
        "Example: Bias Mitigation in Pre-trained Models: A Data Augmentation Approach.\n",
        "\n",
        "                        \"\"\"),\n",
        "            ConferenceAgent(name=\"Agent KDD\", chat_model=chat_model, target_conference=\"KDD\"  ,conference_themes = \"\"\"\n",
        "                        KDD centers on data mining, big data, and applied AI. Key themes include:\n",
        "                            Scalable data mining algorithms.\n",
        "                            Graph data and network analysis.\n",
        "                            Temporal and sequential data mining.\n",
        "                            Anomaly detection and predictive modeling.\n",
        "                            Recommender systems and personalization.\n",
        "                            Causal inference and counterfactual reasoning.\n",
        "                            Applications in finance, e-commerce, and marketing.\n",
        "                            Data visualization and interpretability.\n",
        "                            Data ethics and privacy-preserving techniques.\n",
        "                            AI and data-driven solutions for social good.\n",
        "                        \"\"\",\n",
        "\n",
        "                        conference_context=\"\"\"\n",
        "                        Examples of accepted papers:\n",
        "\n",
        "Paper Title: Scalable Graph Neural Networks for Large-Scale Social Network Analysis\n",
        "Summary: Introduces a scalable GNN framework that handles billion-scale graphs with high efficiency, showcasing applications in social network analysis.\n",
        "\n",
        "Paper Title: Causal Discovery in High-Dimensional Data Using Deep Learning\n",
        "Summary: Proposes a deep learning-based approach for identifying causal relationships in high-dimensional data.\n",
        "\n",
        "Paper Title: Fair Representation Learning with Adversarial Networks\n",
        "Summary: Develops a framework for learning fair data representations while minimizing demographic bias in downstream tasks.\n",
        "\n",
        "Paper Title: Adaptive Online Learning for Real-Time Recommender Systems\n",
        "Summary: Presents an adaptive online learning algorithm that updates recommender system models in real time based on user interactions.\n",
        "\n",
        "Evaluation Criteria:\n",
        "Relevance to Knowledge Discovery and Data Mining:\n",
        "\n",
        "Papers must address core challenges in data mining, machine learning, or related applications.\n",
        "Emphasis on practical applications and scalability to real-world data.\n",
        "Methodological Rigor:\n",
        "\n",
        "Strong theoretical foundations or innovative empirical methodologies.\n",
        "Comprehensive experiments with real-world datasets and benchmarks.\n",
        "Scalability and Efficiency:\n",
        "\n",
        "Solutions must handle large-scale data effectively, both in computation and memory.\n",
        "Novelty and Impact:\n",
        "\n",
        "New techniques, algorithms, or applications that significantly advance the state-of-the-art.\n",
        "Real-world relevance and potential societal or economic impact.\n",
        "Broader Implications:\n",
        "\n",
        "Ethical considerations, such as fairness, transparency, and responsible use of data.\n",
        "Long-term applicability across industries and domains.\n",
        "                        \"\"\"),\n",
        "        ]\n",
        "\n",
        "        storm_system = STORMSystem(agents=agents)\n",
        "        decision = storm_system.discuss_and_decide(paper_text)\n",
        "\n",
        "        print(f\"Paper ID: {publishability_result.paper_id}\")\n",
        "        print(f\"Overall Publishability: {publishability_result.publishable}\")\n",
        "        print(f\"Best Conference: {decision['best_conference']}\")\n",
        "        print(f\"Conference Justification: {decision['justification']}\")\n",
        "    else:\n",
        "        print(f\"Paper ID: {publishability_result.paper_id}\")\n",
        "        print(\"The paper is not deemed publishable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_groq langgraph langchain_community pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbQSqpJxcvq5",
        "outputId": "ff0c4caf-d23b-4924-f104-8fc0e32b622b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.61-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain_groq)\n",
            "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.48-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.14 (from langchain_community)\n",
            "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.24.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain_community) (0.3.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (24.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
            "Downloading langgraph-0.2.61-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
            "Downloading langgraph_sdk-0.1.48-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.24.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, groq, dataclasses-json, langchain-core, langgraph-checkpoint, langchain_groq, langgraph, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxo43JLRc30g",
        "outputId": "f2475a3e-896c-4c25-baf6-a1666847059a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TijIGk4-df6X",
        "outputId": "3b22110b-17ef-400c-b55d-4dab08b86b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python==3.12 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for python==3.12\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "myWyQedCeGhk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}