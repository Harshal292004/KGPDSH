{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "569fda7db5344814af2a1eceaeba3128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_306bc8af4a5d4cf9b2fc37f0f5d7c0b6",
              "IPY_MODEL_ce9eea41471f481c8ad62421cda068a8",
              "IPY_MODEL_e9fa5e11f4b04d4b942d6aa081dfd2df"
            ],
            "layout": "IPY_MODEL_5aefff37480a463a83f03fe61ecba183"
          }
        },
        "306bc8af4a5d4cf9b2fc37f0f5d7c0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edaed9cfaaaf4524b03746d327fa39e0",
            "placeholder": "​",
            "style": "IPY_MODEL_7fc1c0edf94749118a301d2bc1181eab",
            "value": "modules.json: 100%"
          }
        },
        "ce9eea41471f481c8ad62421cda068a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002164072a6f4df38326401243166da5",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52666c944c5941218b04289a3dea79bb",
            "value": 349
          }
        },
        "e9fa5e11f4b04d4b942d6aa081dfd2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad8297c31dc4eb9b14fe2059b9a2348",
            "placeholder": "​",
            "style": "IPY_MODEL_9a709b6d3f0b49ebb9b7e302f3b17719",
            "value": " 349/349 [00:00&lt;00:00, 14.5kB/s]"
          }
        },
        "5aefff37480a463a83f03fe61ecba183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edaed9cfaaaf4524b03746d327fa39e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc1c0edf94749118a301d2bc1181eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "002164072a6f4df38326401243166da5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52666c944c5941218b04289a3dea79bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cad8297c31dc4eb9b14fe2059b9a2348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a709b6d3f0b49ebb9b7e302f3b17719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7190309e6fd4a2099892962c22651be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8452b355c144e7fac57ea7b07937d64",
              "IPY_MODEL_d690e5c8b80a4f81882181145a82e277",
              "IPY_MODEL_2fc4422ef4234548aa92c749eb88be08"
            ],
            "layout": "IPY_MODEL_a4dc6d6d30b248fb828e0158164c3278"
          }
        },
        "c8452b355c144e7fac57ea7b07937d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ac0690a369c464da3f00e2518f2d2dd",
            "placeholder": "​",
            "style": "IPY_MODEL_b247846bcd3342779058c620fe5f28cf",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "d690e5c8b80a4f81882181145a82e277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3bc6539a69d454ca23b3175b02899f5",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d338d73a51a4c07b070aadfcaa24a2d",
            "value": 116
          }
        },
        "2fc4422ef4234548aa92c749eb88be08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f5864e96a7410595262520daa82d43",
            "placeholder": "​",
            "style": "IPY_MODEL_9d2916fe0c884c2b82277586abe5872f",
            "value": " 116/116 [00:00&lt;00:00, 6.38kB/s]"
          }
        },
        "a4dc6d6d30b248fb828e0158164c3278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac0690a369c464da3f00e2518f2d2dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b247846bcd3342779058c620fe5f28cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3bc6539a69d454ca23b3175b02899f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d338d73a51a4c07b070aadfcaa24a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49f5864e96a7410595262520daa82d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d2916fe0c884c2b82277586abe5872f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1f26e64905045c490c024ffc82fbc3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_825b269911cc4dd9b3d0d50b103fdc1b",
              "IPY_MODEL_fb8577bb624c4a109eee888b0b912b3b",
              "IPY_MODEL_1dd660f7c1e54d9a8b22e71e549529d0"
            ],
            "layout": "IPY_MODEL_a6897d49da0a4d5484a28bcc2d776441"
          }
        },
        "825b269911cc4dd9b3d0d50b103fdc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38ca7b60944a40769ad940532133d778",
            "placeholder": "​",
            "style": "IPY_MODEL_abdecddf210c45f09c4c4fb1fa08e7a5",
            "value": "README.md: 100%"
          }
        },
        "fb8577bb624c4a109eee888b0b912b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e392cac47e0a4c1bb31b37266f3ff69d",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81cf8771c50349f3a891c23b51756afc",
            "value": 10659
          }
        },
        "1dd660f7c1e54d9a8b22e71e549529d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91ca4396be4c4889a968e5258efcdd8c",
            "placeholder": "​",
            "style": "IPY_MODEL_017458f850c3422b8bce0bfb479a35d8",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 646kB/s]"
          }
        },
        "a6897d49da0a4d5484a28bcc2d776441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ca7b60944a40769ad940532133d778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abdecddf210c45f09c4c4fb1fa08e7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e392cac47e0a4c1bb31b37266f3ff69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81cf8771c50349f3a891c23b51756afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91ca4396be4c4889a968e5258efcdd8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "017458f850c3422b8bce0bfb479a35d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bb2210a560b469fae1da2c063ac030b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0cab713f204747c4a0f9f73868da3916",
              "IPY_MODEL_7907661bc0654d99b94788f83c284bab",
              "IPY_MODEL_1eca0816e28540aeac586c68c9bdd29a"
            ],
            "layout": "IPY_MODEL_955364f45fd14b92867a1a06bfe08d77"
          }
        },
        "0cab713f204747c4a0f9f73868da3916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3d3b31d988c4be1a766fe3f62069453",
            "placeholder": "​",
            "style": "IPY_MODEL_2df935a7c4a14b39b845ad6c6e6a57c9",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "7907661bc0654d99b94788f83c284bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42c81adde5ce4933a456d66c47856f4e",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8c6b1ba80504779b341e6755c8abedb",
            "value": 53
          }
        },
        "1eca0816e28540aeac586c68c9bdd29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f02e6cff03e54b3bb07711d4924e4bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_d7bb742b1954493eaac42d1cec4596f5",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.16kB/s]"
          }
        },
        "955364f45fd14b92867a1a06bfe08d77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3d3b31d988c4be1a766fe3f62069453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df935a7c4a14b39b845ad6c6e6a57c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42c81adde5ce4933a456d66c47856f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c6b1ba80504779b341e6755c8abedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f02e6cff03e54b3bb07711d4924e4bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7bb742b1954493eaac42d1cec4596f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4defa9a2a244ee1892cacfeba8ce0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffd02cc04be049e1afa5a9e3d797814d",
              "IPY_MODEL_46fceb66b96f44aba7a54ba574b0c29d",
              "IPY_MODEL_d43e45eb9115445ba514ca7450a1b5de"
            ],
            "layout": "IPY_MODEL_597d217d58d243509539fc6dcff5d033"
          }
        },
        "ffd02cc04be049e1afa5a9e3d797814d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e76db00ecbe54aeda02183acc590ed66",
            "placeholder": "​",
            "style": "IPY_MODEL_4d2f7394d05c4a88af1dfd519092d9a9",
            "value": "config.json: 100%"
          }
        },
        "46fceb66b96f44aba7a54ba574b0c29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f705415d84406dac00085b0fba381e",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eaa24e19df3a427cb730bdac69db014f",
            "value": 612
          }
        },
        "d43e45eb9115445ba514ca7450a1b5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2247abc69733433684ed195fad595415",
            "placeholder": "​",
            "style": "IPY_MODEL_eaa54249dd6a4dd8bf12a8f1e239e55c",
            "value": " 612/612 [00:00&lt;00:00, 37.8kB/s]"
          }
        },
        "597d217d58d243509539fc6dcff5d033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e76db00ecbe54aeda02183acc590ed66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d2f7394d05c4a88af1dfd519092d9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7f705415d84406dac00085b0fba381e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa24e19df3a427cb730bdac69db014f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2247abc69733433684ed195fad595415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa54249dd6a4dd8bf12a8f1e239e55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "920ddc375a814854a3239db968af001b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13de84b9c5da454ebfce26d0ab3b155d",
              "IPY_MODEL_ac83c71942b544679e93c3df6bbfd8c6",
              "IPY_MODEL_f225fe2222c14a5887e955445b1c4391"
            ],
            "layout": "IPY_MODEL_be0e56cc1c1b403bb5f2d31a35a258bc"
          }
        },
        "13de84b9c5da454ebfce26d0ab3b155d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece383bb40cb44cbb498ebbca5d65190",
            "placeholder": "​",
            "style": "IPY_MODEL_cf4bba06f13f400baa31cce26091394a",
            "value": "model.safetensors: 100%"
          }
        },
        "ac83c71942b544679e93c3df6bbfd8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8be93917218449c18f19086702f74062",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ef47d87512d4a2b93bf75961058f384",
            "value": 90868376
          }
        },
        "f225fe2222c14a5887e955445b1c4391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3efd9273d6d1405a8959bc84733dfbc5",
            "placeholder": "​",
            "style": "IPY_MODEL_c59df1a4753c4f899cc905cb8032075f",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 194MB/s]"
          }
        },
        "be0e56cc1c1b403bb5f2d31a35a258bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece383bb40cb44cbb498ebbca5d65190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf4bba06f13f400baa31cce26091394a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8be93917218449c18f19086702f74062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ef47d87512d4a2b93bf75961058f384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3efd9273d6d1405a8959bc84733dfbc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59df1a4753c4f899cc905cb8032075f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "705e9f0a93374b7790f675ce11b5378c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30273d88e9e24b46881d58758f09863d",
              "IPY_MODEL_e14f2b58f0344bab9b558a28afad8de3",
              "IPY_MODEL_0cf92667e9444798af8b36df9c52a88c"
            ],
            "layout": "IPY_MODEL_c29fcc4eea9e4348a60b133142129238"
          }
        },
        "30273d88e9e24b46881d58758f09863d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd750e90dd074ed182fd8ce58aea4df1",
            "placeholder": "​",
            "style": "IPY_MODEL_09670c0725a746a3a4bfe14cf066b98d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e14f2b58f0344bab9b558a28afad8de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_646bbe720dcd43d6bb9d693a6eef6570",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bdf859445d7453cb8d9593a6793b2db",
            "value": 350
          }
        },
        "0cf92667e9444798af8b36df9c52a88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12583b3e493e4b58a40fcadca05ffcd9",
            "placeholder": "​",
            "style": "IPY_MODEL_9be039b11d3648bc9869a7bc920856c0",
            "value": " 350/350 [00:00&lt;00:00, 4.71kB/s]"
          }
        },
        "c29fcc4eea9e4348a60b133142129238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd750e90dd074ed182fd8ce58aea4df1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09670c0725a746a3a4bfe14cf066b98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "646bbe720dcd43d6bb9d693a6eef6570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bdf859445d7453cb8d9593a6793b2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12583b3e493e4b58a40fcadca05ffcd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be039b11d3648bc9869a7bc920856c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29402f16b56744f293f3f51f9e499694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f6b49db3aff4a5fb87653dbf2ea56b0",
              "IPY_MODEL_3154fa83295d4690bcf16a3b5857d685",
              "IPY_MODEL_b9eafa9d6b3347dfb94940f03cb06c6a"
            ],
            "layout": "IPY_MODEL_e65cf06eddfc42c393038fb273f5ddb4"
          }
        },
        "7f6b49db3aff4a5fb87653dbf2ea56b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69f306888c52451e9ff9e0d661c82ad2",
            "placeholder": "​",
            "style": "IPY_MODEL_eaabf3522d1c4a0f8c29b7faf2956021",
            "value": "vocab.txt: 100%"
          }
        },
        "3154fa83295d4690bcf16a3b5857d685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae24c06dc8564dab8cd533761c26a668",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b4027fcccb44ee0b935a3a8698783b0",
            "value": 231508
          }
        },
        "b9eafa9d6b3347dfb94940f03cb06c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b747fbc6e57d4af499582f20f1aa1171",
            "placeholder": "​",
            "style": "IPY_MODEL_10e73b3d37c64d46970f9d0a6c157848",
            "value": " 232k/232k [00:00&lt;00:00, 3.50MB/s]"
          }
        },
        "e65cf06eddfc42c393038fb273f5ddb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69f306888c52451e9ff9e0d661c82ad2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaabf3522d1c4a0f8c29b7faf2956021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae24c06dc8564dab8cd533761c26a668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4027fcccb44ee0b935a3a8698783b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b747fbc6e57d4af499582f20f1aa1171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10e73b3d37c64d46970f9d0a6c157848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cb88c66923548db8b3b7afa76ae49be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ceddc3f1b22426aaa96ece97b6b929c",
              "IPY_MODEL_10e26108b01a45cea03a36f035370012",
              "IPY_MODEL_e46e13c0cdf140839624945effdbf69d"
            ],
            "layout": "IPY_MODEL_d2c3f2125be843019d0e5abd61ef957d"
          }
        },
        "3ceddc3f1b22426aaa96ece97b6b929c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e777a7426598411a902a14bcd235e04e",
            "placeholder": "​",
            "style": "IPY_MODEL_4734db1a7f4849268fa23fd1cad3d6d0",
            "value": "tokenizer.json: 100%"
          }
        },
        "10e26108b01a45cea03a36f035370012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5a049c12ad749c9ae6b44c00d426610",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7dc10f2bb7240fea154adfec54af70e",
            "value": 466247
          }
        },
        "e46e13c0cdf140839624945effdbf69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18961f6d8cd0401989216fe5eac6e4ce",
            "placeholder": "​",
            "style": "IPY_MODEL_9ecd11dcf21a47958d04515ba72791bd",
            "value": " 466k/466k [00:00&lt;00:00, 10.5MB/s]"
          }
        },
        "d2c3f2125be843019d0e5abd61ef957d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e777a7426598411a902a14bcd235e04e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4734db1a7f4849268fa23fd1cad3d6d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5a049c12ad749c9ae6b44c00d426610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7dc10f2bb7240fea154adfec54af70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18961f6d8cd0401989216fe5eac6e4ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ecd11dcf21a47958d04515ba72791bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83aff04d816f4d6a897892a79cb847de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22b673e0879946bca4ee0d8e634732d7",
              "IPY_MODEL_8040eb7863f641d3ad7086c0ae581fd9",
              "IPY_MODEL_e0333433064949278c035e2e52ce60ab"
            ],
            "layout": "IPY_MODEL_49dff49e40fb44aaa34710825ddfae74"
          }
        },
        "22b673e0879946bca4ee0d8e634732d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee1a463fc8cd499c94628e98ec52f006",
            "placeholder": "​",
            "style": "IPY_MODEL_bde0bde94bb84e96bdc7716b2a622387",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "8040eb7863f641d3ad7086c0ae581fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_228d2550de404a018f52c2a76ca9cf54",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bffade15395c40019d401bb0c1803c5d",
            "value": 112
          }
        },
        "e0333433064949278c035e2e52ce60ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33635f155ef44d9380b7fd3aadd7460e",
            "placeholder": "​",
            "style": "IPY_MODEL_5f46e184031d490eb6b0bbad62ed906b",
            "value": " 112/112 [00:00&lt;00:00, 4.96kB/s]"
          }
        },
        "49dff49e40fb44aaa34710825ddfae74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee1a463fc8cd499c94628e98ec52f006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde0bde94bb84e96bdc7716b2a622387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "228d2550de404a018f52c2a76ca9cf54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bffade15395c40019d401bb0c1803c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33635f155ef44d9380b7fd3aadd7460e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f46e184031d490eb6b0bbad62ed906b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7658516b4c0b47f78905e4eb56594992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_403b1765fe4841a4b64eed282cb0f40d",
              "IPY_MODEL_1e5da9483ff14fa19347b9944413e976",
              "IPY_MODEL_3ca1c509c411484f83658f33a4bbae36"
            ],
            "layout": "IPY_MODEL_02d4ea757d444e0f905195d97af0755d"
          }
        },
        "403b1765fe4841a4b64eed282cb0f40d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95441b82db094152979b9303115eca9d",
            "placeholder": "​",
            "style": "IPY_MODEL_37edfea6176247fd91c4193610ccb29f",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "1e5da9483ff14fa19347b9944413e976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce33b218157a49f89f7715c39c9c15f5",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d948ea8167543a1995bea6b9b0571ef",
            "value": 190
          }
        },
        "3ca1c509c411484f83658f33a4bbae36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e262467279d443f9485e068a5d552de",
            "placeholder": "​",
            "style": "IPY_MODEL_a1c665cee85a4cb79526897fbea60d01",
            "value": " 190/190 [00:00&lt;00:00, 3.76kB/s]"
          }
        },
        "02d4ea757d444e0f905195d97af0755d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95441b82db094152979b9303115eca9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37edfea6176247fd91c4193610ccb29f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce33b218157a49f89f7715c39c9c15f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d948ea8167543a1995bea6b9b0571ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e262467279d443f9485e068a5d552de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c665cee85a4cb79526897fbea60d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshal292004/KGPDSH/blob/master/Copy_of_CreateDataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFVM61IrX4qT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193f45df-0e80-418b-ff5d-22ef2bcec593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.10/dist-packages (0.2.60)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from langgraph) (2.0.9)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.1.48)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.2.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community tiktoken langgraph faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "toLP9w14Of6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd33da5e-960f-48e8-a3a4-6d62918bc5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "ZwaIMqa1UvQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "print(f\"Current Working Directory: {current_directory}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYMMF7jdQW_Z",
        "outputId": "c72c475f-2533-4446-a5bb-ef2de5667669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5p8_IYJQflz",
        "outputId": "f41b8931-e5a6-4aa8-97e2-27aa48589a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'drive', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uFMZF4a4RGGY",
        "outputId": "a42de3bd-4633-49f3-cf70-3c23e26310db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/KDSH_2025_Dataset/Reference')"
      ],
      "metadata": {
        "id": "OR_sCFWoQ3B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recursively_get_pdf_files(directory):\n",
        "    pdf_files = []\n",
        "    try:\n",
        "        if os.path.isfile(directory) and directory.endswith('.pdf'):\n",
        "            pdf_files.append(directory)\n",
        "        elif os.path.isdir(directory):\n",
        "            for file in os.listdir(directory):\n",
        "                full_path = os.path.join(directory, file)\n",
        "                pdf_files.extend(recursively_get_pdf_files(full_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing {directory}: {e}\")\n",
        "    return pdf_files\n"
      ],
      "metadata": {
        "id": "z5duPna2TJim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_files1=recursively_get_pdf_files('/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable')\n",
        "list_of_files2=recursively_get_pdf_files('/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable')"
      ],
      "metadata": {
        "id": "M-Mf-4NHT6Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_files1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYDJQj9iUAsZ",
        "outputId": "08ed2f04-35d6-4aae-a372-bd97f2ad0d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_files2"
      ],
      "metadata": {
        "id": "RsQkDLvXEwwM",
        "outputId": "5d6d9825-d038-49e1-8fd7-d365259742f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R002.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R003.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R004.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_docs(list_of_pdf_paths):\n",
        "  document_list=[]\n",
        "  for pdf_path in list_of_pdf_paths:\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    document_list.append(loader.load())\n",
        "  return document_list"
      ],
      "metadata": {
        "id": "o2qXS2kFUHZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPeAjjYqEv7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_list1= load_docs(list_of_files1)\n",
        "document_list2 = load_docs(list_of_files2)"
      ],
      "metadata": {
        "id": "mHwzhlAxUcfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrDAOudYUj5m",
        "outputId": "6bbf1003-92e2-466d-a6e5-4f0520ebc4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/298.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in document_list1:\n",
        "  print(\"-\"*20)\n",
        "  print(doc)\n",
        "  print(\"-\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS-Tv7JdVAIe",
        "outputId": "658acdaf-62f8-44ad-b426-2ea9299c6d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 0}, page_content='Detailed Action Identification in Baseball Game\\nRecordings\\nAbstract\\nThis research introduces MLB-YouTube, a new and complex dataset created for\\nnuanced activity recognition in baseball videos. This dataset is structured to\\nsupport two types of analysis: one for classifying activities in segmented videos\\nand another for detecting activities in unsegmented, continuous video streams. This\\nstudy evaluates several methods for recognizing activities, focusing on how they\\ncapture the temporal organization of activities in videos. This evaluation starts\\nwith categorizing segmented videos and progresses to applying these methods\\nto continuous video feeds. Additionally, this paper assesses the effectiveness of\\ndifferent models in the challenging task of forecasting pitch velocity and type\\nusing baseball broadcast videos. The findings indicate that incorporating temporal\\ndynamics into models is beneficial for detailed activity recognition.\\n1 Introduction\\nAction recognition, a significant problem in computer vision, finds extensive use in sports. Profes-\\nsional sporting events are extensively recorded for entertainment, and these recordings are invaluable\\nfor subsequent analysis by coaches, scouts, and media analysts. While numerous game statistics\\nare currently gathered manually, the potential exists for these to be replaced by computer vision\\nsystems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)\\nto automatically record pitch speed and movement, utilizing a network of high-speed cameras and\\nradar to collect detailed data on each player. Access to much of this data is restricted from the public\\ndomain.\\nThis paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-\\nties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or\\ndetection, our dataset emphasizes fine-grained activity recognition. The differences between activities\\nare often minimal, primarily involving the movement of a single individual, with a consistent scene\\nstructure across activities. The determination of activity is based on a single camera perspective. This\\nstudy compares various methods for temporal feature aggregation, both for classifying activities in\\nsegmented videos and for detecting them in continuous video streams.\\n2 Related Work\\nThe field of activity recognition has garnered substantial attention in computer vision research. Initial\\nsuccesses were achieved with hand-engineered features such as dense trajectories. The focus of more\\nrecent studies has shifted towards the application of Convolutional Neural Networks (CNNs) for\\nactivity recognition. Two-stream CNN architectures utilize both spatial RGB frames and optical\\nflow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have been\\ndeveloped. The development of these advanced CNN models has been supported by large datasets\\nsuch as Kinetics, THUMOS, and ActivityNet.\\nSeveral studies have investigated the aggregation of temporal features for the purpose of activity\\nrecognition. Research has compared several pooling techniques and determined that both Long Short-\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 1}, page_content='Term Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. It\\nhas been discovered that pooling intervals from varying locations and durations is advantageous for\\nactivity recognition. It was demonstrated that identifying and classifying key sub-event intervals can\\nlead to better performance.\\nRecently, segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently\\nfor activity detection. These methods depend on the 3D CNN to capture temporal dynamics, which\\ntypically span only 16 frames. Although longer-term temporal structures have been explored, this was\\nusually accomplished with temporal pooling of localized features or (spatio-)temporal convolutions\\nwith extended fixed intervals. Recurrent Neural Networks (RNNs) have also been applied to represent\\ntransitions in activity between frames.\\n3 MLB-YouTube Dataset\\nWe have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\\non YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos\\nintended for activity recognition and continuous videos designed for activity classification. The\\ndataset’s complexity is amplified by the fact that it originates from televised baseball games, where a\\nsingle camera perspective is shared among various activities. Additionally, there is minimal variance\\nin motion and appearance among different activities, such as swinging a bat versus bunting. In\\ncontrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities\\nwith diverse settings, scales, and camera angles, our dataset features activities where a single frame\\nmight not be adequate to determine the activity.\\nThe minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between\\nthese actions requires identifying whether the batter swings or not, detecting the umpire’s signal\\n(Figure 4) for a strike, or noting the absence of a signal for a ball. This is further complicated because\\nthe batter or catcher can obstruct the umpire, and each umpire has their unique style of signaling a\\nstrike.\\nOur dataset for segmented video analysis comprises 4,290 clips. Each clip is annotated for multiple\\nbaseball actions, including swing, hit, ball, strike, and foul. Given that a single clip may contain\\nseveral activities, this is considered a multi-label classification task. Table 1 presents the complete\\nlist of activities and their respective counts within the dataset. Additionally, clips featuring a pitch\\nwere annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. Furthermore, a\\ncollection of 2,983 hard negative examples, where no action is present, was gathered. These instances\\ninclude views of the crowd, the field, or players standing idly before or after a pitch. Examples of\\nactivities and hard negatives are depicted in Figure 2.\\nOur continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Every\\nframe in these videos is annotated with the baseball activities that occur. On average, each continuous\\nclip contains 7.2 activities, amounting to over 15,000 activity instances in total.\\nTable 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.\\nActivity Count\\nNo Activity 2983\\nBall 1434\\nStrike 1799\\nSwing 2506\\nHit 1391\\nFoul 718\\nIn Play 679\\nBunt 24\\nHit by Pitch 14\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 2}, page_content='4 Segmented Video Recognition Approach\\nWe investigate different techniques for aggregating temporal features in segmented video activity\\nrecognition. In segmented videos, the classification task is simpler because each frame corresponds to\\nan activity, eliminating the need for the model to identify the start and end of activities. Our methods\\nare based on a CNN that generates a per-frame or per-segment representation, derived from standard\\ntwo-stream CNNs using deep CNNs like I3D or InceptionV3.\\nGiven video features v of dimensions T × D, where T represents the video’s temporal length and D\\nis the feature’s dimensionality, the usual approach for feature pooling involves max- or mean-pooling\\nacross the temporal dimension, followed by a fully-connected layer for video clip classification, as\\ndepicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,\\nlosing temporal information. An alternative is to employ a fixed temporal pyramid with various\\nlengths, as shown in Fig 5(b), dividing the video into intervals of lengths 1/2, 1/4, and 1/8, and\\nmax-pooling each. The pooled features are concatenated, creating a K × D representation, where K\\nis the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip.\\nWe also explore learning temporal convolution filters to aggregate local temporal structures. A kernel\\nof size L×1 is applied to each frame, enabling each timestep representation to incorporate information\\nfrom adjacent frames. After applying max-pooling to the output of the temporal convolution, a fully-\\nconnected layer is used for classification, as illustrated in Fig. 5(c).\\nWhile temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.\\nPrevious studies have shown that learning the sub-interval to pool is beneficial for activity recognition.\\nThese learned intervals are defined by three parameters: a center g, a width σ, and a stride δ,\\nparameterizing N Gaussians. Given the video length T, the positions of the strided Gaussians are\\nfirst calculated as:\\ngn = 0.5 − T − (gn + 1)\\nN − 1 forn = 0, 1, . . . , N− 1\\npt,n = gn + (t − 0.5T + 0.5)1\\nδ fort = 0, 1, . . . , T− 1\\nThe filters are then generated as:\\nFm[i, t] = 1\\nZm\\nexp\\n\\x12\\n−(t − µi,m)2\\n2σ2m\\n\\x13\\ni ∈ {0, 1, . . . , N− 1}, t∈ {0, 1, . . . , T− 1}\\nwhere Zm is a normalization constant.\\nWe apply these filters F to the T × D video representation through matrix multiplication, yielding an\\nN × D representation that serves as input to a fully-connected layer for classification. This method\\nis shown in Fig 5(d).\\nAdditionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state\\nas input to a fully-connected layer for classification. We frame our tasks as multi-label classification\\nand train these models to minimize binary cross-entropy:\\nL(v) =\\nX\\nc\\nzc log(p(c|G(v))) + (1− zc) log(1− p(c|G(v)))\\nwhere G(v) is the function that pools the temporal information, and zc is the ground truth label for\\nclass c.\\n5 Activity Detection in Continuous Videos\\nDetecting activities in continuous videos poses a greater challenge. The goal here is to classify each\\nframe according to the activities occurring. Unlike segmented videos, continuous videos feature\\nmultiple sequential activities, often interspersed with frames of inactivity. This necessitates that\\nthe model learn to identify the start and end points of activities. As a baseline, we train a single\\nfully-connected layer to serve as a per-frame classifier, which does not utilize temporal information\\nbeyond that contained in the features.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 3}, page_content='We adapt the methods developed for segmented video classification to continuous videos by imple-\\nmenting a temporal sliding window approach. We select a fixed window duration of L features, apply\\nmax-pooling to each window (similar to Fig. 5(a)), and classify each pooled segment. This approach\\nis extended to temporal pyramid pooling by dividing the window of lengthL into segments of lengths\\nL/2, L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,\\nand the pooled features are concatenated, yielding a 14 × D-dimensional representation for each\\nwindow, which is then used as input to the classifier.\\nFor temporal convolutional models in continuous videos, we modify the segmented video approach by\\nlearning a temporal convolutional kernel of length L and convolving it with the input video features.\\nThis operation transforms input of size T × D into output of size T × D, followed by a per-frame\\nclassifier. This enables the model to aggregate local temporal information.\\nTo extend the sub-event model to continuous videos, we follow a similar approach but setT = L in\\nEq. 1, resulting in filters of length L. The T ×D video representation is convolved with the sub-event\\nfilters F, producing an N × D × T-dimensional representation used as input to a fully-connected\\nlayer for frame classification.\\nThe model is trained to minimize per-frame binary classification:\\nL(v) =\\nX\\nt,c\\nzt,c log(p(c|H(vt))) + (1− zt,c) log(1− p(c|H(vt)))\\nwhere vt is the per-frame or per-segment feature at time t, H(vt) is the sliding window application of\\none of the feature pooling methods, and zt,c is the ground truth class at time t.\\nA method to learn ’super-events’ (i.e., global video context) has been introduced and shown to be\\neffective for activity detection in continuous videos. This approach involves learning a set of temporal\\nstructure filters modeled as N Cauchy distributions. Each distribution is defined by a center xn and a\\nwidth γn. Given the video length T, the filters are constructed by:\\nxn = (T − 1)(tanh(x′\\nn) + 1)\\n2\\nfn(t) = 1\\nZn\\nγn\\nπ((t − xn)2 + γ2n) exp(1 − 2|tanh(γ′\\nn)|)\\nwhere Zn is a normalization constant, t ∈ {1, 2, . . . , T}, and n ∈ {1, 2, . . . , N}.\\nThe filters are combined with learned per-class soft-attention weights A, and the super-event repre-\\nsentation is computed as:\\nSc =\\nX\\nn\\nAc,n\\nX\\nt\\nfn(t) · vt\\nwhere v is the T × D video representation. These filters enable the model to focus on relevant\\nintervals for temporal context. The super-event representation is concatenated to each timestep and\\nused for classification. We also experiment with combining the super- and sub-event representations\\nto form a three-level hierarchy for event representation.\\n6 Experiments\\n6.1 Implementation Details\\nFor our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics\\ndatasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable\\nfeature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet\\nand Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth\\ncompared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow\\nwas computed and clipped to [−20, 20]. For InceptionV3, features were computed every 3 frames\\n(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\\n3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam\\noptimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50\\nepochs.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 4}, page_content='6.2 Segmented Video Activity Recognition\\nWe initially conducted binary pitch/non-pitch classification for each video segment. This task is\\nrelatively straightforward due to the distinct differences between pitch and non-pitch frames. The\\nresults, detailed in Table 2, reveal minimal variation across different features or models.\\nTable 2: Performance on segmented videos for binary pitch/non-pitch classification.\\nModel RGB Flow Two-stream\\nInceptionV3 97.46 98.44 98.67\\nInceptionV3 + sub-events 98.67 98.73 99.36\\nI3D 98.64 98.88 98.70\\nI3D + sub-events 98.42 98.35 98.65\\n6.2.1 Multi-label Classification\\nWe assessed various temporal feature aggregation methods by calculating the mean average precision\\n(mAP) for each video clip, a standard metric for multi-label classification. Table 4 compares the\\nperformance of these methods. All methods surpass mean/max-pooling, highlighting the importance\\nof preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs\\nshow some improvement. Temporal convolution offers a more significant performance boost but\\nrequires substantially more parameters (see Table 3). Learning sub-events, as per previous research,\\nyields the best results. While LSTMs and temporal convolutions have been used before, they need\\nmore parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitate\\nsequential processing of video features, whereas other methods can be fully parallelized.\\nTable 3: Additional parameters required for models when added to the base model (e.g., I3D or\\nInception V3).\\nModel # Parameters\\nMax/Mean Pooling 16K\\nPyramid Pooling 115K\\nLSTM 10.5M\\nTemporal Conv 31.5M\\nSub-events 36K\\nTable 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.\\nLearning sub-intervals for pooling is found to be crucial for activity recognition.\\nMethod RGB Flow Two-stream\\nRandom 16.3 16.3 16.3\\nInceptionV3 + mean-pool 35.6 47.2 45.3\\nInceptionV3 + max-pool 47.9 48.6 54.4\\nInceptionV3 + pyramid 49.7 53.2 55.3\\nInceptionV3 + LSTM 47.6 55.6 57.7\\nInceptionV3 + temporal conv 47.2 55.2 56.1\\nInceptionV3 + sub-events 56.2 62.5 62.6\\nI3D + mean-pool 42.4 47.6 52.7\\nI3D + max-pool 48.3 53.4 57.2\\nI3D + pyramid 53.2 56.7 58.7\\nI3D + LSTM 48.2 53.1 53.1\\nI3D + temporal conv 52.8 57.1 58.4\\nI3D + sub-events 55.5 61.2 61.3\\nTable 5 shows the average precision for each activity class. Learning temporal structure is particularly\\nbeneficial for frame-based features (e.g., InceptionV3), which capture less temporal information\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 5}, page_content='compared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detecting\\nstrikes, hits, foul balls, and hit-by-pitch events, which exhibit changes in video features post-event.\\nFor instance, after a hit, the camera often tracks the ball’s trajectory, while after a hit-by-pitch, it\\nfollows the player to first base, as illustrated in Fig. 6 and Fig. 7.\\nTable 5: Per-class average precision for segmented videos using two-stream features in multi-\\nlabel activity classification. Utilizing sub-events to discern temporal intervals of interest proves\\nadvantageous for activity recognition.\\nMethod Ball Strike Swing Hit Foul In Play Bunt Hit by Pitch\\nRandom 21.8 28.6 37.4 20.9 11.4 10.3 1.1 4.5\\nInceptionV3 + max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7\\nInceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2\\nI3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2\\nI3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0\\n6.2.2 Pitch Speed Regression\\nEstimating pitch speed from video frames is an exceptionally difficult problem, as it requires the\\nnetwork to pinpoint the pitch’s start and end, and derive the speed from a minimal signal. The baseball,\\noften obscured by the pitcher, travels at speeds over 100mph and covers 60.5 feet in approximately 0.5\\nseconds. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,\\nproving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos, we\\nrecalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connected\\nlayer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\\nand actual speeds, we achieved an average error of 3.6mph. Table 6 compares different models, and\\nFig. 8 illustrates the sub-events learned for various speeds.\\nTable 6: Results for pitch speed regression on segmented videos, reporting root-mean-squared errors.\\nMethod Two-stream\\nI3D 4.3 mph\\nI3D + LSTM 4.1 mph\\nI3D + sub-events 3.9 mph\\nInceptionV3 5.3 mph\\nInceptionV3 + LSTM 4.5 mph\\nInceptionV3 + sub-events 3.6 mph\\n6.2.3 Pitch Type Classification\\nWe conducted experiments to determine the feasibility of predicting pitch types from video, a task\\nmade challenging by pitchers’ efforts to disguise their pitches from batters and the subtle differences\\nbetween pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\\nutilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.\\nPose features were considered due to variations in body mechanics between different pitches. Our\\ndataset includes six pitch types, with results presented in Table 7. LSTMs performed worse than the\\nbaseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the\\neasiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult\\n(12%).\\n6.3 Continuous Video Activity Detection\\nWe evaluate models extended for continuous videos using per-frame mean average precision (mAP),\\nwith results shown in Table 8. This setting is more challenging than segmented videos, requiring\\nthe model to identify activity start and end times and handle ambiguous negative examples. All\\nmodels improve upon the baseline per-frame classification, confirming the importance of temporal\\ninformation. Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 6}, page_content='Table 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose\\nheatmaps.\\nMethod Accuracy\\nRandom 17.0%\\nI3D 25.8%\\nI3D + LSTM 18.5%\\nI3D + sub-events 34.5%\\nPose 28.4%\\nPose + LSTM 27.6%\\nPose + sub-events 36.4%\\nconvolution appear to overfit. Convolutional sub-events, especially when combined with super-event\\nrepresentation, significantly enhance performance, particularly for frame-based features.\\nTable 8: Performance on continuous videos for multi-label activity classification (per-frame mAP).\\nMethod RGB Flow Two-stream\\nRandom 13.4 13.4 13.4\\nI3D 33.8 35.1 34.2\\nI3D + max-pooling 34.9 36.4 36.8\\nI3D + pyramid 36.8 37.5 39.7\\nI3D + LSTM 36.2 37.3 39.4\\nI3D + temporal conv 35.2 38.1 39.2\\nI3D + sub-events 35.5 37.5 38.5\\nI3D + super-events 38.7 38.6 39.1\\nI3D + sub+super-events 38.2 39.4 40.4\\nInceptionV3 31.2 31.8 31.9\\nInceptionV3 + max-pooling 31.8 34.1 35.2\\nInceptionV3 + pyramid 32.2 35.1 36.8\\nInceptionV3 + LSTM 32.1 33.5 34.1\\nInceptionV3 + temporal conv 28.4 34.4 33.4\\nInceptionV3 + sub-events 32.1 35.8 37.3\\nInceptionV3 + super-events 31.5 36.2 39.6\\nInceptionV3 + sub+super-events 34.2 40.2 40.9\\n7 Conclusion\\nThis paper introduces MLB-YouTube, a novel and challenging dataset designed for detailed activity\\nrecognition in videos. We conduct a comparative analysis of various recognition techniques that\\nemploy temporal feature pooling for both segmented and continuous videos. Our findings reveal that\\nlearning sub-events to pinpoint temporal regions of interest significantly enhances performance in\\nsegmented video classification. In the context of activity detection in continuous videos, we establish\\nthat incorporating convolutional sub-events with a super-event representation, creating a three-level\\nactivity hierarchy, yields the most favorable outcomes.\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 0}, page_content='Advancements in 3D Food Modeling: A Review of the\\nMetaFood Challenge Techniques and Outcomes\\nAbstract\\nThe growing focus on leveraging computer vision for dietary oversight and nutri-\\ntion tracking has spurred the creation of sophisticated 3D reconstruction methods\\nfor food. The lack of comprehensive, high-fidelity data, coupled with limited\\ncollaborative efforts between academic and industrial sectors, has significantly\\nhindered advancements in this domain. This study addresses these obstacles by\\nintroducing the MetaFood Challenge, aimed at generating precise, volumetrically\\naccurate 3D food models from 2D images, utilizing a checkerboard for size cal-\\nibration. The challenge was structured around 20 food items across three levels\\nof complexity: easy (200 images), medium (30 images), and hard (1 image). A\\ntotal of 16 teams participated in the final assessment phase. The methodologies\\ndeveloped during this challenge have yielded highly encouraging outcomes in\\n3D food reconstruction, showing great promise for refining portion estimation in\\ndietary evaluations and nutritional tracking. Further information on this workshop\\nchallenge and the dataset is accessible via the provided URL.\\n1 Introduction\\nThe convergence of computer vision technologies with culinary practices has pioneered innovative\\napproaches to dietary monitoring and nutritional assessment. The MetaFood Workshop Challenge\\nrepresents a landmark initiative in this emerging field, responding to the pressing demand for precise\\nand scalable techniques for estimating food portions and monitoring nutritional consumption. Such\\ntechnologies are vital for fostering healthier eating behaviors and addressing health issues linked to\\ndiet.\\nBy concentrating on the development of accurate 3D models of food derived from various visual\\ninputs, including multiple views and single perspectives, this challenge endeavors to bridge the\\ndisparity between current methodologies and practical needs. It promotes the creation of unique\\nsolutions capable of managing the intricacies of food morphology, texture, and illumination, while also\\nmeeting the real-world demands of dietary evaluation. This initiative gathers experts from computer\\nvision, machine learning, and nutrition science to propel 3D food reconstruction technologies forward.\\nThese advancements have the potential to substantially enhance the precision and utility of food\\nportion estimation across diverse applications, from individual health tracking to extensive nutritional\\ninvestigations.\\nConventional methods for assessing diet, like 24-Hour Recall or Food Frequency Questionnaires\\n(FFQs), are frequently reliant on manual data entry, which is prone to inaccuracies and can be\\nburdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression-\\nbased methods for estimating food portions directly from images of eating occasions. By enhancing\\n3D reconstruction for food, the aim is to provide more accurate and intuitive nutritional assessment\\ntools. This technology could revolutionize the sharing of culinary experiences and significantly\\nimpact nutrition science and public health.\\nParticipants were tasked with creating 3D models of 20 distinct food items from 2D images, mim-\\nicking scenarios where mobile devices equipped with depth-sensing cameras are used for dietary\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 1}, page_content='recording and nutritional tracking. The challenge was segmented into three tiers of difficulty based\\non the number of images provided: approximately 200 images for easy, 30 for medium, and a single\\ntop-view image for hard. This design aimed to rigorously test the adaptability and resilience of\\nproposed solutions under various realistic conditions. A notable feature of this challenge was the use\\nof a visible checkerboard for physical referencing and the provision of depth images for each frame,\\nensuring the 3D models maintained accurate real-world measurements for portion size estimation.\\nThis initiative not only expands the frontiers of 3D reconstruction technology but also sets the stage\\nfor more reliable and user-friendly real-world applications, including image-based dietary assessment.\\nThe resulting solutions hold the potential to profoundly influence nutritional intake monitoring and\\ncomprehension, supporting broader health and wellness objectives. As progress continues, innovative\\napplications are anticipated to transform personal health management, nutritional research, and the\\nwider food industry. The remainder of this report is structured as follows: Section 2 delves into the\\nexisting literature on food portion size estimation, Section 3 describes the dataset and evaluation\\nframework used in the challenge, and Sections 4, 5, and 6 discuss the methodologies and findings of\\nthe top three teams (V olETA, ININ-VIAUN, and FoodRiddle), respectively.\\n2 Related Work\\nEstimating food portions is a crucial part of image-based dietary assessment, aiming to determine the\\nvolume, energy content, or macronutrients directly from images of meals. Unlike the well-studied\\ntask of food recognition, estimating food portions is particularly challenging due to the lack of 3D\\ninformation and physical size references necessary for accurately judging the actual size of food\\nportions. Accurate portion size estimation requires understanding the volume and density of food,\\nelements that are hard to deduce from a 2D image, underscoring the need for sophisticated techniques\\nto tackle this problem. Current methods for estimating food portions are grouped into four categories.\\nStereo-Based Approaches use multiple images to reconstruct the 3D structure of food. Some methods\\nestimate food volume using multi-view stereo reconstruction based on epipolar geometry, while\\nothers perform two-view dense reconstruction. Simultaneous Localization and Mapping (SLAM) has\\nalso been used for continuous, real-time food volume estimation. However, these methods are limited\\nby their need for multiple images, which is not always practical.\\nModel-Based Approaches use predefined shapes and templates to estimate volume. For instance,\\ncertain templates are assigned to foods from a library and transformed based on physical references to\\nestimate the size and location of the food. Template matching approaches estimate food volume from\\na single image, but they struggle with variations in food shapes that differ from predefined templates.\\nRecent work has used 3D food meshes as templates to align camera and object poses for portion size\\nestimation.\\nDepth Camera-Based Approaches use depth cameras to create depth maps, capturing the distance from\\nthe camera to the food. These depth maps form a voxel representation used for volume estimation.\\nThe main drawback is the need for high-quality depth maps and the extra processing required for\\nconsumer-grade depth sensors.\\nDeep Learning Approaches utilize neural networks trained on large image datasets for portion\\nestimation. Regression networks estimate the energy value of food from single images or from an\\n\"Energy Distribution Map\" that maps input images to energy distributions. Some networks use both\\nimages and depth maps to estimate energy, mass, and macronutrient content. However, deep learning\\nmethods require extensive data for training and are not always interpretable, with performance\\ndegrading when test images significantly differ from training data.\\nWhile these methods have advanced food portion estimation, they face limitations that hinder their\\nwidespread use and accuracy. Stereo-based methods are impractical for single images, model-based\\napproaches struggle with diverse food shapes, depth camera methods need specialized hardware,\\nand deep learning approaches lack interpretability and struggle with out-of-distribution samples. 3D\\nreconstruction offers a promising solution by providing comprehensive spatial information, adapting\\nto various shapes, potentially working with single images, offering visually interpretable results,\\nand enabling a standardized approach to food portion estimation. These benefits motivated the\\norganization of the 3D Food Reconstruction challenge, aiming to overcome existing limitations and\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 2}, page_content='develop more accurate, user-friendly, and widely applicable food portion estimation techniques,\\nimpacting nutritional assessment and dietary monitoring.\\n3 Datasets and Evaluation Pipeline\\n3.1 Dataset Description\\nThe dataset for the MetaFood Challenge features 20 carefully chosen food items from the MetaFood3D\\ndataset, each scanned in 3D and accompanied by video recordings. To ensure precise size accuracy\\nin the reconstructed 3D models, each food item was captured alongside a checkerboard and pattern\\nmat, serving as physical scaling references. The challenge is divided into three levels of difficulty,\\ndetermined by the quantity of 2D images provided for reconstruction:\\n• Easy: Around 200 images taken from video.\\n• Medium: 30 images.\\n• Hard: A single image from a top-down perspective.\\nTable 1 details the food items included in the dataset.\\nTable 1: MetaFood Challenge Data Details\\nObject Index Food Item Difficulty Level Number of Frames\\n1 Strawberry Easy 199\\n2 Cinnamon bun Easy 200\\n3 Pork rib Easy 200\\n4 Corn Easy 200\\n5 French toast Easy 200\\n6 Sandwich Easy 200\\n7 Burger Easy 200\\n8 Cake Easy 200\\n9 Blueberry muffin Medium 30\\n10 Banana Medium 30\\n11 Salmon Medium 30\\n12 Steak Medium 30\\n13 Burrito Medium 30\\n14 Hotdog Medium 30\\n15 Chicken nugget Medium 30\\n16 Everything bagel Hard 1\\n17 Croissant Hard 1\\n18 Shrimp Hard 1\\n19 Waffle Hard 1\\n20 Pizza Hard 1\\n3.2 Evaluation Pipeline\\nThe evaluation process is split into two phases, focusing on the accuracy of the reconstructed 3D\\nmodels in terms of shape (3D structure) and portion size (volume).\\n3.2.1 Phase-I: Volume Accuracy\\nIn the first phase, the Mean Absolute Percentage Error (MAPE) is used to evaluate portion size\\naccuracy, calculated as follows:\\nMAPE = 1\\nn\\nnX\\ni=1\\n\\x0c\\x0c\\x0c\\x0c\\nAi − Fi\\nAi\\n\\x0c\\x0c\\x0c\\x0c × 100% (1)\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 3}, page_content='where Ai is the actual volume (in ml) of the i-th food item obtained from the scanned 3D food mesh,\\nand Fi is the volume calculated from the reconstructed 3D mesh.\\n3.2.2 Phase-II: Shape Accuracy\\nTeams that perform well in Phase-I are asked to submit complete 3D mesh files for each food item.\\nThis phase involves several steps to ensure precision and fairness:\\n• Model Verification: Submitted models are checked against the final Phase-I submissions for\\nconsistency, and visual inspections are conducted to prevent rule violations.\\n• Model Alignment: Participants receive ground truth 3D models and a script to compute the\\nfinal Chamfer distance. They must align their models with the ground truth and prepare a\\ntransformation matrix for each submitted object. The final Chamfer distance is calculated\\nusing these models and matrices.\\n• Chamfer Distance Calculation: Shape accuracy is assessed using the Chamfer distance\\nmetric. Given two point sets X and Y , the Chamfer distance is defined as:\\ndCD(X, Y) = 1\\n|X|\\nX\\nx∈X\\nmin\\ny∈Y\\n∥x − y∥2\\n2 + 1\\n|Y |\\nX\\ny∈Y\\nmin\\nx∈X\\n∥x − y∥2\\n2 (2)\\nThis metric offers a comprehensive measure of similarity between the reconstructed 3D models and\\nthe ground truth. The final ranking is determined by combining scores from both Phase-I (volume\\naccuracy) and Phase-II (shape accuracy). Note that after the Phase-I evaluation, quality issues were\\nfound with the data for object 12 (steak) and object 15 (chicken nugget), so these items were excluded\\nfrom the final overall evaluation.\\n4 First Place Team - VolETA\\n4.1 Methodology\\nThe team’s research employs multi-view reconstruction to generate detailed food meshes and calculate\\nprecise food volumes.\\n4.1.1 Overview\\nThe team’s method integrates computer vision and deep learning to accurately estimate food volume\\nfrom RGBD images and masks. Keyframe selection ensures data quality, supported by perceptual\\nhashing and blur detection. Camera pose estimation and object segmentation pave the way for neural\\nsurface reconstruction, creating detailed meshes for volume estimation. Refinement steps, including\\nisolated piece removal and scaling factor adjustments, enhance accuracy. This approach provides a\\nthorough solution for accurate food volume assessment, with potential uses in nutrition analysis.\\n4.1.2 The Team’s Proposal: VolETA\\nThe team starts by acquiring input data, specifically RGBD images and corresponding food object\\nmasks. The RGBD images, denoted as ID = {IDi}n\\ni=1, where n is the total number of frames,\\nprovide depth information alongside RGB images. The food object masks, {Mf\\ni }n\\ni=1, help identify\\nregions of interest within these images.\\nNext, the team selects keyframes. From the set {IDi}n\\ni=1, keyframes {IK\\nj }k\\nj=1 ⊆ {IDi}n\\ni=1 are\\nchosen. A method is implemented to detect and remove duplicate and blurry images, ensuring\\nhigh-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fourier\\ntransform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-\\ning to detect similar images and retain overlapping ones. Duplicates and blurry images are excluded\\nto maintain data integrity and accuracy.\\nUsing the selected keyframes {IK\\nj }k\\nj=1, the team estimates camera poses through a method called\\nPixSfM, which involves extracting features using SuperPoint, matching them with SuperGlue, and\\nrefining them. The outputs are the camera poses {Cj}k\\nj=1, crucial for understanding the scene’s\\nspatial layout.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 4}, page_content='In parallel, the team uses a tool called SAM for reference object segmentation. SAM segments\\nthe reference object with a user-provided prompt, producing a reference object mask MR for each\\nkeyframe. This mask helps track the reference object across all frames. The XMem++ method\\nextends the reference object mask MR to all frames, creating a comprehensive set of reference object\\nmasks {MR\\ni }n\\ni=1. This ensures consistent reference object identification throughout the dataset.\\nTo create RGBA images, the team combines RGB images, reference object masks {MR\\ni }n\\ni=1, and\\nfood object masks {MF\\ni }n\\ni=1. This step, denoted as {IR\\ni }n\\ni=1, integrates various data sources into a\\nunified format for further processing.\\nThe team converts the RGBA images {IR\\ni }n\\ni=1 and camera poses {Cj}k\\nj=1 into meaningful metadata\\nand modeled data Dm. This transformation facilitates accurate scene reconstruction.\\nThe modeled data Dm is input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes\\n{Rf , Rr} for the reference and food objects, providing detailed 3D representations. The team uses the\\n\"Remove Isolated Pieces\" technique to refine the meshes. Given that the scenes contain only one food\\nitem, the diameter threshold is set to 5% of the mesh size. This method deletes isolated connected\\ncomponents with diameters less than or equal to 5%, resulting in a cleaned mesh {RCf , RCr}. This\\nstep ensures that only significant parts of the mesh are retained.\\nThe team manually identifies an initial scaling factor S using the reference mesh via MeshLab. This\\nfactor is fine-tuned to Sf using depth information and food and reference masks, ensuring accurate\\nscaling relative to real-world dimensions. Finally, the fine-tuned scaling factor Sf is applied to the\\ncleaned food mesh RCf , producing the final scaled food mesh RFf . This step culminates in an\\naccurately scaled 3D representation of the food object, enabling precise volume estimation.\\n4.1.3 Detecting the scaling factor\\nGenerally, 3D reconstruction methods produce unitless meshes by default. To address this, the team\\nmanually determines the scaling factor by measuring the distance for each block of the reference\\nobject mesh. The average of all block lengths lavg is calculated, while the actual real-world length is\\nconstant at lreal = 0.012 meters. The scaling factor S = lreal/lavg is applied to the clean food mesh\\nRCf , resulting in the final scaled food mesh RFf in meters.\\nThe team uses depth information along with food and reference object masks to validate the scaling\\nfactors. The method for assessing food size involves using overhead RGB images for each scene.\\nInitially, the pixel-per-unit (PPU) ratio (in meters) is determined using the reference object. Subse-\\nquently, the food width (fw) and length (fl) are extracted using a food object mask. To determine the\\nfood height (fh), a two-step process is followed. First, binary image segmentation is performed using\\nthe overhead depth and reference images, yielding a segmented depth image for the reference object.\\nThe average depth is then calculated using the segmented reference object depth ( dr). Similarly,\\nemploying binary image segmentation with an overhead food object mask and depth image, the\\naverage depth for the segmented food depth image (df ) is computed. The estimated food height fh is\\nthe absolute difference between dr and df . To assess the accuracy of the scaling factor S, the food\\nbounding box volume (fw × fl × fh) × PPU is computed. The team evaluates if the scaling factor\\nS generates a food volume close to this potential volume, resulting in Sfine . Table 2 lists the scaling\\nfactors, PPU, 2D reference object dimensions, 3D food object dimensions, and potential volume.\\nFor one-shot 3D reconstruction, the team uses One-2-3-45 to reconstruct a 3D model from a single\\nRGBA view input after applying binary image segmentation to both food RGB and mask images.\\nIsolated pieces are removed from the generated mesh, and the scaling factor S, which is closer to the\\npotential volume of the clean mesh, is reused.\\n4.2 Experimental Results\\n4.2.1 Implementation settings\\nExperiments were conducted using two GPUs: GeForce GTX 1080 Ti/12G and RTX 3060/6G. The\\nHamming distance for near image similarity was set to 12. For Gaussian kernel radius, even numbers\\nin the range [0...30] were used for detecting blurry images. The diameter for removing isolated pieces\\nwas set to 5%. NeuS2 was run for 15,000 iterations with a mesh resolution of 512x512, a unit cube\\n\"aabb scale\" of 1, \"scale\" of 0.15, and \"offset\" of [0.5, 0.5, 0.5] for each food scene.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 5}, page_content='4.2.2 VolETA Results\\nThe team extensively validated their approach on the challenge dataset and compared their results\\nwith ground truth meshes using MAPE and Chamfer distance metrics. The team’s approach was\\napplied separately to each food scene. A one-shot food volume estimation approach was used if\\nthe number of keyframes k equaled 1; otherwise, a few-shot food volume estimation was applied.\\nNotably, the keyframe selection process chose 34.8% of the total frames for the rest of the pipeline,\\nshowing the minimum frames with the highest information.\\nTable 2: List of Extracted Information Using RGBD and Masks\\nLevel Id Label Sf PPU Rw × Rl (fw × fl × fh)\\n1 Strawberry 0.08955223881 0.01786 320 × 360 (238 × 257 × 2.353)\\n2 Cinnamon bun 0.1043478261 0.02347 236 × 274 (363 × 419 × 2.353)\\n3 Pork rib 0.1043478261 0.02381 246 × 270 (435 × 778 × 1.176)\\nEasy 4 Corn 0.08823529412 0.01897 291 × 339 (262 × 976 × 2.353)\\n5 French toast 0.1034482759 0.02202 266 × 292 (530 × 581 × 2.53)\\n6 Sandwich 0.1276595745 0.02426 230 × 265 (294 × 431 × 2.353)\\n7 Burger 0.1043478261 0.02435 208 × 264 (378 × 400 × 2.353)\\n8 Cake 0.1276595745 0.02143 256 × 300 (298 × 310 × 4.706)\\n9 Blueberry muffin 0.08759124088 0.01801 291 × 357 (441 × 443 × 2.353)\\n10 Banana 0.08759124088 0.01705 315 × 377 (446 × 857 × 1.176)\\nMedium 11 Salmon 0.1043478261 0.02390 242 × 269 (201 × 303 × 1.176)\\n13 Burrito 0.1034482759 0.02372 244 × 271 (251 × 917 × 2.353)\\n14 Frankfurt sandwich 0.1034482759 0.02115 266 × 304 (400 × 1022 × 2.353)\\n16 Everything bagel 0.08759124088 0.01747 306 × 368 (458 × 134 × 1.176)\\nHard 17 Croissant 0.1276595745 0.01751 319 × 367 (395 × 695 × 2.176)\\n18 Shrimp 0.08759124088 0.02021 249 × 318 (186 × 95 × 0.987)\\n19 Waffle 0.01034482759 0.01902 294 × 338 (465 × 537 × 0.8)\\n20 Pizza 0.01034482759 0.01913 292 × 336 (442 × 651 × 1.176)\\nAfter finding keyframes, PixSfM estimated the poses and point cloud. After generating scaled meshes,\\nthe team calculated volumes and Chamfer distance with and without transformation metrics. Meshes\\nwere registered with ground truth meshes using ICP to obtain transformation metrics.\\nTable 3 presents quantitative comparisons of the team’s volumes and Chamfer distance with and\\nwithout estimated transformation metrics from ICP. For overall method performance, Table 4 shows\\nthe MAPE and Chamfer distance with and without transformation metrics.\\nAdditionally, qualitative results on one- and few-shot 3D reconstruction from the challenge dataset\\nare shown. The model excels in texture details, artifact correction, missing data handling, and color\\nadjustment across different scene parts.\\nLimitations: Despite promising results, several limitations need to be addressed in future work:\\n• Manual processes: The current pipeline includes manual steps like providing segmentation\\nprompts and identifying scaling factors, which should be automated to enhance efficiency.\\n• Input requirements: The method requires extensive input information, including food\\nmasks and depth data. Streamlining these inputs would simplify the process and increase\\napplicability.\\n• Complex backgrounds and objects: The method has not been tested in environments with\\ncomplex backgrounds or highly intricate food objects.\\n• Capturing complexities: The method has not been evaluated under different capturing\\ncomplexities, such as varying distances and camera speeds.\\n• Pipeline complexity: For one-shot neural rendering, the team currently uses One-2-3-45.\\nThey aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve\\nefficiency.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 6}, page_content='Table 3: Quantitative Comparison with Ground Truth Using Chamfer Distance\\nL Id Team’s V ol. GT V ol. Ch. w/ t.m Ch. w/o t.m\\n1 40.06 38.53 1.63 85.40\\n2 216.9 280.36 7.12 111.47\\n3 278.86 249.67 13.69 172.88\\nE 4 279.02 295.13 2.03 61.30\\n5 395.76 392.58 13.67 102.14\\n6 205.17 218.44 6.68 150.78\\n7 372.93 368.77 4.70 66.91\\n8 186.62 173.13 2.98 152.34\\n9 224.08 232.74 3.91 160.07\\n10 153.76 163.09 2.67 138.45\\nM 11 80.4 85.18 3.37 151.14\\n13 363.99 308.28 5.18 147.53\\n14 535.44 589.83 4.31 89.66\\n16 163.13 262.15 18.06 28.33\\nH 17 224.08 181.36 9.44 28.94\\n18 25.4 20.58 4.28 12.84\\n19 110.05 108.35 11.34 23.98\\n20 130.96 119.83 15.59 31.05\\nTable 4: Quantitative Comparison with Ground Truth Using MAPE and Chamfer Distance\\nMAPE Ch. w/ t.m Ch. w/o t.m\\n(%) sum mean sum mean\\n10.973 0.130 0.007 1.715 0.095\\n5 Second Place Team - ININ-VIAUN\\n5.1 Methodology\\nThis section details the team’s proposed network, illustrating the step-by-step process from original\\nimages to final mesh models.\\n5.1.1 Scale factor estimation\\nThe procedure for estimating the scale factor at the coordinate level is illustrated in Figure 9. The\\nteam adheres to a method involving corner projection matching. Specifically, utilizing the COLMAP\\ndense model, the team acquires the pose of each image along with dense point cloud data. For any\\ngiven image imgk and its extrinsic parameters [R|t]k, the team initially performs threshold-based\\ncorner detection, setting the threshold at 240. This step allows them to obtain the pixel coordinates\\nof all detected corners. Subsequently, using the intrinsic parameters k and the extrinsic parameters\\n[R|t]k, the point cloud is projected onto the image plane. Based on the pixel coordinates of the\\ncorners, the team can identify the closest point coordinates Pk\\ni for each corner, where i represents the\\nindex of the corner. Thus, they can calculate the distance between any two corners as follows:\\nDk\\nij = (Pk\\ni − Pk\\nj )2 ∀i ̸= j (3)\\nTo determine the final computed length of each checkerboard square in image k, the team takes the\\nminimum value of each row of the matrix Dk (excluding the diagonal) to form the vector dk. The\\nmedian of this vector is then used. The final scale calculation formula is given by Equation 4, where\\n0.012 represents the known length of each square (1.2 cm):\\nscale = 0.012Pn\\ni=1 med(dk) (4)\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 7}, page_content='5.1.2 3D Reconstruction\\nThe 3D reconstruction process, depicted in Figure 10, involves two different pipelines to accommodate\\nvariations in input viewpoints. The first fifteen objects are processed using one pipeline, while the\\nlast five single-view objects are processed using another.\\nFor the initial fifteen objects, the team uses COLMAP to estimate poses and segment the food using\\nthe provided segment masks. Advanced multi-view 3D reconstruction methods are then applied to\\nreconstruct the segmented food. The team employs three different reconstruction methods: COLMAP,\\nDiffusioNeRF, and NeRF2Mesh. They select the best reconstruction results from these methods and\\nextract the mesh. The extracted mesh is scaled using the estimated scale factor, and optimization\\ntechniques are applied to obtain a refined mesh.\\nFor the last five single-view objects, the team experiments with several single-view reconstruction\\nmethods, including Zero123, Zero123++, One2345, ZeroNVS, and DreamGaussian. They choose\\nZeroNVS to obtain a 3D food model consistent with the distribution of the input image. The\\nintrinsic camera parameters from the fifteenth object are used, and an optimization method based\\non reprojection error refines the extrinsic parameters of the single camera. Due to limitations in\\nsingle-view reconstruction, depth information from the dataset and the checkerboard in the monocular\\nimage are used to determine the size of the extracted mesh. Finally, optimization techniques are\\napplied to obtain a refined mesh.\\n5.1.3 Mesh refinement\\nDuring the 3D Reconstruction phase, it was observed that the model’s results often suffered from low\\nquality due to holes on the object’s surface and substantial noise, as shown in Figure 11.\\nTo address the holes, MeshFix, an optimization method based on computational geometry, is em-\\nployed. For surface noise, Laplacian Smoothing is used for mesh smoothing operations. The\\nLaplacian Smoothing method adjusts the position of each vertex to the average of its neighboring\\nvertices:\\nV (new)\\ni = V (old)\\ni + λ\\n\\uf8eb\\n\\uf8ed 1\\n|N(i)|\\nX\\nj∈N(i)\\nV (old)\\nj − V (old)\\ni\\n\\uf8f6\\n\\uf8f8 (5)\\nIn their implementation, the smoothing factor λ is set to 0.2, and 10 iterations are performed.\\n5.2 Experimental Results\\n5.2.1 Estimated scale factor\\nThe scale factors estimated using the described method are shown in Table 5. Each image and the\\ncorresponding reconstructed 3D model yield a scale factor, and the table presents the average scale\\nfactor for each object.\\n5.2.2 Reconstructed meshes\\nThe refined meshes obtained using the described methods are shown in Figure 12. The predicted\\nmodel volumes, ground truth model volumes, and the percentage errors between them are presented\\nin Table 6.\\n5.2.3 Alignment\\nThe team designs a multi-stage alignment method for evaluating reconstruction quality. Figure 13\\nillustrates the alignment process for Object 14. First, the central points of both the predicted and\\nground truth models are calculated, and the predicted model is moved to align with the central point\\nof the ground truth model. Next, ICP registration is performed for further alignment, significantly\\nreducing the Chamfer distance. Finally, gradient descent is used for additional fine-tuning to obtain\\nthe final transformation matrix.\\nThe total Chamfer distance between all 18 predicted models and the ground truths is 0.069441169.\\n8'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 8}, page_content='Table 5: Estimated Scale Factors\\nObject Index Food Item Scale Factor\\n1 Strawberry 0.060058\\n2 Cinnamon bun 0.081829\\n3 Pork rib 0.073861\\n4 Corn 0.083594\\n5 French toast 0.078632\\n6 Sandwich 0.088368\\n7 Burger 0.103124\\n8 Cake 0.068496\\n9 Blueberry muffin 0.059292\\n10 Banana 0.058236\\n11 Salmon 0.083821\\n13 Burrito 0.069663\\n14 Hotdog 0.073766\\nTable 6: Metric of V olume\\nObject Index Predicted V olume Ground Truth Error Percentage\\n1 44.51 38.53 15.52\\n2 321.26 280.36 14.59\\n3 336.11 249.67 34.62\\n4 347.54 295.13 17.76\\n5 389.28 392.58 0.84\\n6 197.82 218.44 9.44\\n7 412.52 368.77 11.86\\n8 181.21 173.13 4.67\\n9 233.79 232.74 0.45\\n10 160.06 163.09 1.86\\n11 86.0 85.18 0.96\\n13 334.7 308.28 8.57\\n14 517.75 589.83 12.22\\n16 176.24 262.15 32.77\\n17 180.68 181.36 0.37\\n18 13.58 20.58 34.01\\n19 117.72 108.35 8.64\\n20 117.43 119.83 20.03\\n6 Best 3D Mesh Reconstruction Team - FoodRiddle\\n6.1 Methodology\\nTo achieve high-fidelity food mesh reconstruction, the team developed two procedural pipelines as\\ndepicted in Figure 14. For simple and medium complexity cases, they employed a structure-from-\\nmotion strategy to ascertain the pose of each image, followed by mesh reconstruction. Subsequently,\\na sequence of post-processing steps was implemented to recalibrate the scale and improve mesh\\nquality. For cases involving only a single image, the team utilized image generation techniques to\\nfacilitate model generation.\\n6.1.1 Multi-View Reconstruction\\nFor Structure from Motion (SfM), the team enhanced the advanced COLMAP method by integrating\\nSuperPoint and SuperGlue techniques. This integration significantly addressed the issue of limited\\nkeypoints in scenes with minimal texture, as illustrated in Figure 15.\\nIn the mesh reconstruction phase, the team’s approach builds upon 2D Gaussian Splatting, which\\nemploys a differentiable 2D Gaussian renderer and includes regularization terms for depth distortion\\n9'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 9}, page_content='and normal consistency. The Truncated Signed Distance Function (TSDF) results are utilized to\\nproduce a dense point cloud.\\nDuring post-processing, the team applied filtering and outlier removal methods, identified the outline\\nof the supporting surface, and projected the lower mesh vertices onto this surface. They utilized\\nthe reconstructed checkerboard to correct the model’s scale and employed Poisson reconstruction to\\ncreate a complete, watertight mesh of the subject.\\n6.1.2 Single-View Reconstruction\\nFor 3D reconstruction from a single image, the team utilized advanced methods such as LGM, Instant\\nMesh, and One-2-3-45 to generate an initial mesh. This initial mesh was then refined in conjunction\\nwith depth structure information.\\nTo adjust the scale, the team estimated the object’s length using the checkerboard as a reference,\\nassuming that the object and the checkerboard are on the same plane. They then projected the 3D\\nobject back onto the original 2D image to obtain a more precise scale for the object.\\n6.2 Experimental Results\\nThrough a process of nonlinear optimization, the team sought to identify a transformation that\\nminimizes the Chamfer distance between their mesh and the ground truth mesh. This optimization\\naimed to align the two meshes as closely as possible in three-dimensional space. Upon completion\\nof this process, the average Chamfer dis- tance across the final reconstructions of the 20 objects\\namounted to 0.0032175 meters. As shown in Table 7, Team FoodRiddle achieved the best scores for\\nboth multi- view and single-view reconstructions, outperforming other teams in the competition.\\nTable 7: Total Errors for Different Teams on Multi-view and Single-view Data\\nTeam Multi-view (1-14) Single-view (16-20)\\nFoodRiddle 0.036362 0.019232\\nININ-VIAUN 0.041552 0.027889\\nV olETA 0.071921 0.058726\\n7 Conclusion\\nThis report examines and compiles the techniques and findings from the MetaFood Workshop\\nchallenge on 3D Food Reconstruction. The challenge sought to enhance 3D reconstruction methods\\nby concentrating on food items, tackling the distinct difficulties presented by varied textures, reflective\\nsurfaces, and intricate geometries common in culinary subjects.\\nThe competition involved 20 diverse food items, captured under various conditions and with differing\\nnumbers of input images, specifically designed to challenge participants in creating robust reconstruc-\\ntion models. The evaluation was based on a two-phase process, assessing both portion size accuracy\\nthrough Mean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distance\\nmetric.\\nOf all participating teams, three reached the final submission stage, presenting a range of innovative\\nsolutions. Team V olETA secured first place with the best overall performance in both Phase-I and\\nPhase-II, followed by team ININ-VIAUN in second place. Additionally, the FoodRiddle team\\nexhibited superior performance in Phase-II, highlighting a competitive and high-caliber field of\\nentries for 3D mesh reconstruction. The challenge has successfully advanced the field of 3D food\\nreconstruction, demonstrating the potential for accurate volume estimation and shape reconstruction\\nin nutritional analysis and food presentation applications. The novel methods developed by the\\nparticipating teams establish a strong foundation for future research in this area, potentially leading\\nto more precise and user-friendly approaches for dietary assessment and monitoring.\\n10')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 0}, page_content='Examining the Convergence of Denoising Diffusion Probabilistic\\nModels: A Quantitative Analysis\\nAbstract\\nDeep generative models, particularly diffusion models, are a significant family within deep learning. This study\\nprovides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion model\\nand the target distribution. In contrast to earlier research, this analysis does not rely on presumptions regarding\\nthe learned score function. Furthermore, the findings are applicable to any data-generating distributions within\\nrestricted instance spaces, even those lacking a density relative to the Lebesgue measure, and the upper limit is not\\nexponentially dependent on the ambient space dimension. The primary finding expands upon recent research by\\nMbacke et al. (2023), and the proofs presented are fundamental.\\n1 Introduction\\nDiffusion models, alongside generative adversarial networks and variational autoencoders (V AEs), are among the most influential\\nfamilies of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,\\nas well as in various other applications.\\nTwo primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative\\nmodels (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while\\nsimultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Conversely, SGMs\\nemploy score-matching methods to approximate the score function of the data-generating distribution, subsequently generating new\\nsamples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function, adding varying\\nnoise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the score\\nfunction for all noise levels has been proposed.\\nAlthough DDPMs and SGMs may initially seem distinct, it has been demonstrated that DDPMs implicitly approximate the score\\nfunction, with the sampling process resembling Langevin dynamics. Moreover, a unified perspective of both methods using stochastic\\ndifferential equations (SDEs) has been derived. The SGM can be viewed as a discretization of Brownian motion, and the DDPM as a\\ndiscretization of an Ornstein-Uhlenbeck process. Consequently, both DDPMs and SGMs are commonly referred to as SGMs in the\\nliterature. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-based\\nframework, necessitating assumptions about the effectiveness of the learned score function.\\nIn this research, a different strategy is employed, applying methods created for V AEs to DDPMs, which can be viewed as hierarchical\\nV AEs with fixed encoders. This method enables the derivation of quantitative, Wasserstein-based upper bounds without making\\nassumptions about the data distribution or the learned score function, and with simple proofs that do not need the SDE toolkit.\\nFurthermore, the bounds presented here do not involve any complex discretization steps, as the forward and backward processes are\\nconsidered discrete-time from the beginning, rather than being viewed as discretizations of continuous-time processes.\\n1.1 Related Works\\nThere has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. However,\\nthese studies frequently depend on restrictive assumptions regarding the data-generating distribution, produce non-quantitative upper\\nbounds, or exhibit exponential dependencies on certain parameters. This work successfully circumvents all three of these limitations.\\nSome bounds are based on very restrictive assumptions about the data-generating distribution, such as log-Sobolev inequalities,\\nwhich are unrealistic for real-world data distributions. Furthermore, some studies establish upper bounds on the Kullback-Leibler\\n(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by the\\ndiffusion model; however, unless strong assumptions are made about the support of the data-generating distribution, KL and TV\\nreach their maximum values. Such assumptions arguably do not hold for real-world data-generating distributions, which are widely\\nbelieved to satisfy the manifold hypothesis. Other work establishes conditions under which the support of the input distribution\\nis equal to the support of the learned distribution, and generalizes the bound to all f-divergences. Assuming L2 accurate score'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 1}, page_content='estimation, some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, but\\ntheir Wasserstein-based bounds are not quantitative. Quantitative Wasserstein distance upper bounds under the manifold hypothesis\\nhave been derived, but these bounds exhibit exponential dependencies on some of the problem parameters.\\n1.2 Our contributions\\nIn this study, strong assumptions about the data-generating distribution are avoided, and a quantitative upper bound on the Wasserstein\\ndistance is established without exponential dependencies on problem parameters, including the ambient space dimension. Moreover,\\na common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According to\\nsome, providing precise guarantees for the estimation of the score function is challenging, as it necessitates an understanding of the\\nnon-convex training dynamics of neural network optimization, which is currently beyond reach. Therefore, upper bounds are derived\\nwithout making assumptions about the learned score function. Instead, the bound presented here is dependent on a reconstruction\\nloss calculated over a finite independent and identically distributed (i.i.d.) sample. Intuitively, a loss function is defined, which\\nquantifies the average Euclidean distance between a sample from the data-generating distribution and the reconstruction obtained by\\nsampling noise and passing it through the backward process (parameterized by ˘03b8). This method is inspired by previous work on\\nV AEs.\\nThis approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution, avoids\\nexponential dependencies on the dimension, and provides a quantitative upper bound based on the Wasserstein distance. Furthermore,\\nthis method benefits from utilizing very straightforward and basic proofs.\\n2 Preliminaries\\nThroughout this paper, lowercase letters are used to represent both probability measures and their densities with respect to the\\nLebesgue measure, and variables are added in parentheses to enhance readability (e.g., q(xt|xt−1) to denote a time-dependent\\nconditional distribution). An instance space X, which is a subset of RD with the Euclidean distance as the underlying metric, and\\na target data-generating distribution µ ∈ M+\\n1 (X) are considered. Note that it is not assumed that µ has a density with respect to\\nthe Lebesgue measure. Additionally, || · ||represents the Euclidean (L2) norm, and Ep(x) is used as shorthand for Ex∼p(x). Given\\nprobability measures p, q∈ M+\\n1 (X) and a real number k >1, the Wasserstein distance of order k is defined as (Villani, 2009):\\nWk(p, q) = inf\\nγ∈Γ(p,q)\\n\\x12Z\\nX×X\\n||x − y||kdγ(x, y)\\n\\x131/k\\n,\\nwhere Γ(p, q) denotes the set of couplings of p and q, meaning the set of joint distributions on X × X with respective marginals p\\nand q. The product measure p ⊗ q is referred to as the trivial coupling, and the Wasserstein distance of order 1 is simply referred to\\nas the Wasserstein distance.\\n2.1 Denoising Diffusion Models\\nInstead of employing the SDE framework, diffusion models are presented using the DDPM formulation with discrete-time processes.\\nA diffusion model consists of two discrete-time stochastic processes: a forward process and a backward process. Both processes are\\nindexed by time 0 ≤ t ≤ T, where the number of time steps T is a predetermined choice.\\n**The forward process.** The forward process transforms a data point x0 ∼ µ into a noise distribution q(xT |x0) through a sequence\\nof conditional distributions q(xt|xt−1) for 1 ≤ t ≤ T. It is assumed that the forward process is defined such that for sufficiently\\nlarge T, the distribution q(xT |x0) is close to a simple noise distribution p(xT ), which is referred to as the prior distribution. For\\ninstance, p(xT ) = N(xT ; 0, I), the standard multivariate normal distribution, has been chosen in previous work.\\n**The backward process.** The backward process is a Markov process with parametric transition kernels. The objective of the\\nbackward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samples\\nfrom the distribution µ. Following previous work, it is assumed that the backward process is defined by Gaussian distributions\\npθ(xt−1|xt) for 2 ≤ t ≤ T as\\npθ(xt−1|xt) = N(xt−1; gθ\\nt (xt), σ2\\nt I),\\nand\\npθ(x0|x1) = gθ\\n1(x1),\\nwhere the variance parameters σ2\\nt ∈ R≥0 are defined by a fixed schedule, the mean functions gθ\\nt : RD → RD are learned using a\\nneural network (with parameters θ) for 2 ≤ t ≤ T, and gθ\\n1 : RD → X is a separate function dependent on σ1. In practice, the same\\nnetwork has been used for the functions gθ\\nt for 2 ≤ t ≤ T, and a separate discrete decoder for gθ\\n1.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 2}, page_content='Generating new samples from a trained diffusion model is accomplished by sampling xt−1 ∼ pθ(xt−1|xt) for 1 ≤ t ≤ T, starting\\nfrom a noise vector xT ∼ p(xT ) sampled from the prior p(xT ).\\nThe following assumption is made regarding the backward process.\\n**Assumption 1.** It is assumed that for each 1 ≤ t ≤ T, there exists a constant Kθ\\nt > 0 such that for every x1, x2 ∈ X,\\n||gθ\\nt (x1) − gθ\\nt (x2)|| ≤Kθ\\nt ||x1 − x2||.\\nIn other words, gθ\\nt is Kθ\\nt -Lipschitz continuous. This assumption is discussed in Remark 3.2.\\n2.2 Additional Definitions\\nThe distribution πθ(·|x0) is defined as\\nπθ(·|x0) = q(xT |x0)pθ(xT−1|xT )pθ(xT−2|xT−1) . . . pθ(x1|x2)pθ(·|x1).\\nIntuitively, for each x0 ∈ X, πθ(·|x0) represents the distribution on X obtained by reconstructing samples from q(xT |x0) through\\nthe backward process. Another way to interpret this distribution is that for any function f : X → R, the following equation holds:\\nEπθ(ˆx0|x0)[f(ˆx0)] = Eq(xT |x0)Epθ(xT−1|xT ) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].\\nGiven a finite set S = {x1\\n0, . . . , xn\\n0 } i.i.d. ∼ µ, the regenerated distribution is defined as the following mixture:\\nµθ\\nn = 1\\nn\\nnX\\ni=1\\nπθ(·|xi\\n0).\\nThis definition is analogous to the empirical regenerated distribution defined for V AEs. The distribution on X learned by the\\ndiffusion model is denoted as πθ(·) and defined as\\nπθ(·) = p(xT )pθ(xT−1|xT )pθ(xT−2|xT−1) . . . pθ(x1|x2)pθ(·|x1).\\nIn other words, for any function f : X → R, the expectation of f with respect to πθ(·) is\\nEπθ(ˆx0)[f(ˆx0)] = Ep(xT )Epθ(xT−1|xT ) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].\\nHence, both πθ(·) and πθ(·|x0) are defined using the backward process, with the difference that πθ(·) starts with the prior\\np(xT ) = N(xT ; 0, I), while πθ(·|x0) starts with the noise distribution q(xT |x0).\\nFinally, the loss function lθ : X × X → R is defined as\\nlθ(xT , x0) = Epθ(xT−1|xT )Epθ(xT−2|xT−1) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[||x0 − ˆx0||].\\nHence, given a noise vector xT and a sample x0, the loss lθ(xT , x0) represents the average Euclidean distance between x0 and any\\nsample obtained by passing xT through the backward process.\\n2.3 Our Approach\\nThe goal is to upper-bound the distance W1(µ, πθ(·)). Since the triangle inequality implies\\nW1(µ, πθ(·)) ≤ W1(µ, µθ\\nn) + W1(µθ\\nn, πθ(·)),\\nthe distance W1(µ, πθ(·)) can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The\\nupper bound on W1(µ, µθ\\nn) is obtained using a straightforward adaptation of a proof. First, W1(µ, µθ\\nn) is upper-bounded using the\\nexpectation of the loss function lθ, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependent\\non the empirical risk and the prior-matching term.\\nThe upper bound on the second term W1(µθ\\nn, πθ(·)) uses the definition of µθ\\nn. Intuitively, the difference between πθ(·|xi\\n0) and πθ(·)\\nis determined by the corresponding initial distributions: q(xT |xi\\n0) and p(xT ) for πθ(·). Hence, if the two initial distributions are\\nclose, and if the steps of the backward process are smooth (see Assumption 1), then πθ(·|xi\\n0) and πθ(·) are close to each other.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 3}, page_content='3 Main Result\\n3.1 Theorem Statement\\nWe are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generating\\ndistribution µ and the learned distribution πθ(·).\\n**Theorem 3.1.** Assume the instance space X has finite diameter ∆ = supx,x′∈X ||x − x′|| < ∞, and let λ >0 and δ ∈ (0, 1) be\\nreal numbers. Using the definitions and assumptions of the previous section, the following inequality holds with probability at least\\n1 − δ over the random draw of S = {x1\\n0, . . . , xn\\n0 } i.i.d. ∼ µ:\\nW1(µ, πθ(·)) ≤1\\nn\\nnX\\ni=1\\nEq(xT |xi\\n0)[lθ(xT , xi\\n0)] + 1\\nλn\\nnX\\ni=1\\nKL(q(xT |xi\\n0)||p(xT )) + 1\\nλn log n\\nδ + λ∆2\\n8n\\n+\\n TY\\nt=1\\nKθ\\nt\\n!\\nEq(xT |xi\\n0)Ep(yT )[||xT − yT ||]\\n+\\nTX\\nt=2\\n t−1Y\\ni=1\\nKθ\\ni\\n!\\nσtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I) are standard Gaussian vectors.\\n**Remark 3.1.** Before presenting the proof, let us discuss Theorem 3.1.\\n* Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. sample S, the bound holds with\\nhigh probability with respect to the randomness of S. This is the price we pay for having a quantitative upper bound with no\\nexponential dependencies on problem parameters and no assumptions on the data-generating distribution µ. * The first term of the\\nright-hand side is the average reconstruction loss computed over the sample S = {x1\\n0, . . . , xn\\n0 }. Note that for each 1 ≤ i ≤ n, the\\nexpectation of lθ(xT |xi\\n0) is only computed with respect to the noise distribution q(xT |xi\\n0) defined by xi\\n0 itself. Hence, this term\\nmeasures how well a noise vector xT ∼ q(xT |xi\\n0) recovers the original sample xi\\n0 using the backward process, and averages over\\nthe set S = {x1\\n0, . . . , xn\\n0 }. * If the Lipschitz constants satisfy Kθ\\nt < 1 for all 1 ≤ t ≤ T, then the larger T is, the smaller the upper\\nbound gets. This is because the product of Kθ\\nt ’s then converges to 0. In Remark 3.2 below, we show that the assumption thatKθ\\nt < 1\\nfor all t is a quite reasonable one. * The hyperparameter λ controls the trade-off between the prior-matching (KL) term and the\\ndiameter term ∆2. If Kθ\\nt < 1 for all 1 ≤ t ≤ T and T → ∞, then the convergence of the bound largely depends on the choice of λ.\\nIn that case, λ ∝ n1/2 leads to faster convergence, while λ ∝ n leads to slower convergence to a smaller quantity. This is because\\nthe bound stems from PAC-Bayesian theory, where this trade-off is common. * The last term of the equation does not depend on the\\nsample size n. Hence, the upper bound given by Theorem 3.1 does not converge to 0 as n → ∞. However, if the Lipschitz factors\\n(Kθ\\nt )1≤t≤T are all less than 1, then this term can be very small, especially in low-dimensional spaces.\\n3.2 Proof of the main theorem\\nThe following result is an adaptation of a previous result.\\n**Lemma 3.2.** Let λ >0 and δ ∈ (0, 1) be real numbers. With probability at least 1 − δ over the randomness of the sample\\nS = {x1\\n0, . . . , xn\\n0 } i.i.d. ∼ µ, the following holds:\\nW1(µ, µθ\\nn) ≤ 1\\nn\\nnX\\ni=1\\nEq(xT |xi\\n0)[lθ(xT , xi\\n0)] + 1\\nλn\\nnX\\ni=1\\nKL(q(xT |xi\\n0)||p(xT )) + 1\\nλn log n\\nδ + λ∆2\\n8n .\\nThe proof of this result is a straightforward adaptation of a previous proof.\\nNow, let us focus our attention on the second term of the right-hand side of the equation, namely W1(µθ\\nn, πθ(·)). This part is trickier\\nthan for V AEs, for which the generative model’s distribution is simply a pushforward measure. Here, we have a non-deterministic\\nsampling process with T steps.\\nAssumption 1 leads to the following lemma on the backward process.\\n**Lemma 3.3.** For any given x1, y1 ∈ X, we have\\nEpθ(x0|x1)Epθ(y0|y1)[||x0 − y0||] ≤ Kθ\\n1 ||x1 − y1||.\\nMoreover, if 2 ≤ t ≤ T, then for any given xt, yt ∈ X, we have\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 4}, page_content='Epθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1 − yt−1||] ≤ Kθ\\nt ||xt − yt|| + σtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I), meaning Eϵ,ϵ′ is a shorthand for Eϵ,ϵ′∼N(0,I).\\n**Proof.** For the first part, let x1, y1 ∈ X. Since according to the equation pθ(x0|x1) = δgθ\\n1 (x1)(x0) and pθ(y0|y1) = δgθ\\n1 (y1)(y0),\\nthen\\nEpθ(x0|x1)Epθ(y0|y1)[||x0 − y0||] = ||gθ\\n1(x1) − gθ\\n1(y1)|| ≤Kθ\\n1 ||x1 − y1||.\\nFor the second part, let 2 ≤ t ≤ T and xt, yt ∈ X. Since pθ(xt−1|xt) = N(xt−1; gθ\\nt (xt), σ2\\nt I), the reparameterization trick implies\\nthat sampling xt−1 ∼ pθ(xt−1|xt) is equivalent to setting\\nxt−1 = gθ\\nt (xt) + σtϵt, with ϵt ∼ N(0, I).\\nUsing the above equation, the triangle inequality, and Assumption 1, we obtain\\nEpθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1 − yt−1||]\\n= Eϵt,ϵ′\\nt∼N(0,I)[||gθ\\nt (xt) + σtϵt − gθ\\nt (yt) − σtϵ′\\nt||]\\n≤ Eϵt,ϵ′\\nt∼N(0,I)[||gθ\\nt (xt) − gθ\\nt (yt)||] + σtEϵt,ϵ′\\nt∼N(0,I)[||ϵt − ϵ′\\nt||]\\n≤ Kθ\\nt ||xt − yt|| + σtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I).\\nNext, we can use the inequalities of Lemma 3.3 to prove the following result.\\n**Lemma 3.4.** Let T ≥ 1. The following inequality holds:\\nEpθ(xT−1|xT )Epθ(yT−1|yT )Epθ(xT−2|xT−1)Epθ(yT−2|yT−1) . . . Epθ(x0|x1)Epθ(y0|y1)[||x0 − y0||]\\n≤\\n TY\\nt=1\\nKθ\\nt\\n!\\n||xT − yT || +\\nTX\\nt=2\\n t−1Y\\ni=1\\nKθ\\ni\\n!\\nσtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I).\\n**Proof Idea.** Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step.\\nUsing the two previous lemmas, we obtain the following upper bound on W1(µθ\\nn, πθ(·)).\\n**Lemma 3.5.** The following inequality holds:\\nW1(µθ\\nn, πθ(·)) ≤ 1\\nn\\nnX\\ni=1\\n TY\\nt=1\\nKθ\\nt\\n!\\nEq(xT |xi\\n0)Ep(yT )[||xT − yT ||] +\\nTX\\nt=2\\n t−1Y\\ni=1\\nKθ\\ni\\n!\\nσtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I).\\n**Proof.** Using the definition of W1, the trivial coupling, the definitions of µθ\\nn and πθ(·), and Lemma 3.4, we get the desired result.\\nCombining Lemmas 3.2 and 3.5 with the triangle inequality yields Theorem 3.1.\\n3.3 Special case using the forward process of Ho et al. (2020)\\nTheorem 3.1 establishes a general upper bound that holds for any forward process, as long as the backward process satisfies\\nAssumption 1. In this section, we specialize the statement of the theorem to the particular case of the forward process defined in\\nprevious work.\\nLet X ⊆ RD. The forward process is a Gauss-Markov process with transition densities defined as\\nq(xt|xt−1) = N(xt; √αtxt−1, (1 − αt)I),\\nwhere α1, . . . , αT is a fixed noise schedule such that 0 < αt < 1 for all t. This definition implies that at each time step 1 ≤ t ≤ T,\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 5}, page_content='q(xt|x0) = N(xt; √¯αtx0, (1 − ¯αt)I), with ¯αt =\\ntY\\ni=1\\nαi.\\nThe optimization objective to train the backward process ensures that for each time step t, the distribution pθ(xt−1|xt) remains close\\nto the ground-truth distribution q(xt−1|xt, x0) given by\\nq(xt−1|xt, x0) = N(xt−1; ˜µq\\nt (xt, x0), ˜σ2\\nt I),\\nwhere\\n˜µq\\nt (xt, x0) =\\n√αt(1 − ¯αt−1)\\n1 − ¯αt\\nxt +\\n√¯αt−1(1 − αt)\\n1 − ¯αt\\nx0.\\nNow, we discuss Assumption 1 under these definitions.\\n**Remark 3.2.** We can get a glimpse at the range of Kθ\\nt for a trained DDPM by looking at the distribution q(xt−1|xt, x0), since\\npθ(xt−1|xt) is optimized to be as close as possible to q(xt−1|xt, x0).\\nFor a given x0 ∼ µ, let us take a look at the Lipschitz norm of x 7→ ˜µq\\nt (x, x0). Using the above equation, we have\\n˜µq\\nt (xt, x0) − ˜µq\\nt (yt, x0) =\\n√αt(1 − ¯αt−1)\\n1 − ¯αt\\n(xt − yt).\\nHence, x 7→ ˜µq\\nt (x, x0) is K′\\nt-Lipschitz continuous with\\nK′\\nt =\\n√αt(1 − ¯αt−1)\\n1 − ¯αt\\n.\\nNow, if αt < 1 for all 1 ≤ t ≤ T, then we have 1 − ¯αt > 1 − ¯αt−1, which implies K′\\nt < 1 for all 1 ≤ t ≤ T.\\nRemark 3.2 shows that the Lipschitz norm of the mean function ˜µq\\nt (·, x0) does not depend on x0. Indeed, looking at the previous\\nequation, we can see that for any initial x0, the Lipschitz norm K′\\nt =\\n√αt(1−¯αt−1)\\n1−¯αt\\nonly depends on the noise schedule, not x0 itself.\\nSince gθ\\nt (·, x0) is optimized to match ˜µq\\nt (·, x0) for each x0 in the training set, and all the functions ˜µq\\nt (·, x0) have the same Lipschitz\\nnorm K′\\nt, we believe it is reasonable to assume gθ\\nt is Lipschitz continuous as well. This is the intuition behind Assumption 1.\\n**The prior-matching term.** With the definitions of this section, the prior matching term KL(q(xT |x0)||p(xT )) has the following\\nclosed form:\\nKL(q(xT |x0)||p(xT )) = 1\\n2\\n\\x02\\n−D log(1 − ¯αT ) − D¯αT + ¯αT ||x0||2\\x03\\n.\\n**Upper-bounds on the average distance between Gaussian vectors.** If ϵ, ϵ′ are D-dimensional vectors sampled from N(0, I), then\\nEϵ,ϵ′[||ϵ − ϵ′||] ≤\\n√\\n2D.\\nMoreover, since q(xT |x0) = N(xT ; √¯αT x0, (1 − ¯αT )I) and the prior p(yT ) = N(yT ; 0, I),\\nEq(xT |x0)Ep(yT )[||xT − yT ||] ≤\\np\\n¯αT ||x0||2 + (2 − ¯αT )D.\\n**Special case of the main theorem.** With the definitions of this section, the inequality of Theorem 3.1 implies that with probability\\nat least 1 − δ over the randomness of {x1\\n0, . . . , x\\n6')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 0}, page_content='Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems\\nwith Solutions Exhibiting Weak Minty Properties\\nAbstract\\nThis research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-\\nstrate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has\\nalready demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.\\nWe establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within\\nthis framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared\\noperator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified\\nversion of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem’s\\nspecific parameters.\\n1 Introduction\\nThe recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,\\nhave generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,\\nadversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed\\nthat generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization\\ncomponent and nonconcave in the maximization component remains limited, with some research even suggesting intractability in\\ncertain cases.\\nA specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)\\nexhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of\\nthe recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified\\n(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing\\nliterature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of\\nweak Minty solutions was quickly investigated.\\nAssumption 1 (Weak Minty solution). For a given operator F : Rd → Rd, there is a point u∗ ∈ Rd and a parameter ρ >0 such that:\\n⟨F(u), u− u∗⟩ ≥ −ρ\\n2∥F(u)∥2 ∀u ∈ Rd. (1)\\nMoreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving\\na complexity of O(ϵ−1) for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step\\nfollowed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant\\nof EG.\\nIn a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-\\nBackward (FoRB). We address the following question with an affirmative answer:\\nCan OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?\\nSpecifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0 and a parameter 0 < γ≤ 1\\nas follows:\\nuk = ¯uk − aF(¯uk),\\n¯uk+1 = ¯uk − γaF (uk), ∀k ≥ 0,\\ncan achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.\\nIt is worth noting that OGDA is most frequently expressed in a form where γ = 1. However, two recent studies have examined\\na more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of γ becomes'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 1}, page_content='apparent only when dealing with weak Minty solutions. In this context, we find that γ must be greater than 1 to ensure convergence,\\na phenomenon that is not observed in monotone problems.\\nWhen examining a general smooth min-max problem:\\nmin\\nx\\nmax\\ny\\nf(x, y)\\nthe operator F mentioned in Assumption 1 naturally emerges as F(u) := [∇xf(x, y), −∇yf(x, y)] with u = (x, y). However,\\nby examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can\\nconcurrently address more scenarios, such as certain equilibrium problems.\\nThe parameter ρ in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it\\nis essential that the step size exceeds a value proportional to ρ. Simultaneously, as is typical, the step size is limited from above\\nby the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than 1\\n4L , their\\nconvergence claim is valid only if ρ < 1\\n4L . This condition was later improved to ρ < 1\\n2L for the choice γ = 1 and to ρ < 1\\nL for\\neven smaller values of γ. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different\\nanalysis, we are able to match the most general condition on the weak Minty parameter ρ <1\\nL for appropriate γ and a.\\n1.1 Contribution\\nOur contributions are summarized as follows:\\n1. We establish a new convergence rate ofO(1/k), measured by the squared operator norm, for a modified version of OGDA,\\nwhich we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to\\nthe Minty variational inequality.\\n2. Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible\\nstep sizes for OGDA+ and obtain the most favorable result known for the standard method (γ = 1).\\n3. We demonstrate a complexity bound of O(ϵ−2) for a stochastic variant of the OGDA+ method.\\n4. We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without\\nrequiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in\\nareas with low curvature, enabling convergence where a fixed step size strategy might fail.\\n1.2 Related literature\\nWe will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap\\nfunction or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,\\nor under the Polyak-Łojasiewicz assumption.\\nWeak Minty.It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,\\ntermed \"weak Minty,\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.\\nConvergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as\\nthe update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the\\nlength of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is\\nalso proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step\\nsize we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps\\nper descent step, achieving the same O(1/k) rate as EG.\\nMinty solutions.Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.\\nIt was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing\\nthe resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a\\nspecific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While\\nthe assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone\\nproblems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point\\nby a factor proportional to the squared operator norm.\\nNegative comonotonicity.Although previously studied under the term \"cohypomonotonicity,\" the concept of negative comono-\\ntonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty\\nsolutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and\\nan improved convergence rate of O(1/k2) (in terms of the squared operator norm) was shown. Similarly, an accelerated version of\\nthe reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty\\nsolutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty\\nsolution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient\\nnorm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate\\nthis class from problems with weak Minty solutions.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 2}, page_content='Interaction dominance.The concept of α-interaction dominance for nonconvex-nonconcave min-max problems was investigated,\\nand it was shown that the proximal-point method converges sublinearly if this condition is met in y and linearly if it is met in both\\ncomponents. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively\\ncomonotone.\\nOptimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the\\nattention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has\\nalso been studied in the mathematical programming community.\\n2 Preliminaries\\n2.1 Notions of solution\\nWe outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These\\nconcepts are typically defined with respect to a constraint set C ⊆ Rd. A Stampacchia solution of the VI given by F : Rd → Rd is a\\npoint u∗ such that:\\n⟨F(u∗), u− u∗⟩ ≥0 ∀u ∈ C. (SVI)\\nIn this work, we only consider the unconstrained case where C = Rd, and the above condition simplifies to F(u∗) = 0. Closely\\nrelated is the following concept: A Minty solution is a point u∗ ∈ C such that:\\n⟨F(u), u− u∗⟩ ≥0 ∀u ∈ C. (MVI)\\nFor a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but\\nholds, for example, if the operator F is monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but\\nwithout any Minty solutions.\\n2.2 Notions of monotonicity\\nThis section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.\\nAn operator F is considered monotone if:\\n⟨F(u) − F(v), u− v⟩ ≥0.\\nSuch operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium\\nproblems.\\nTwo frequently studied notions that fall into this category are strongly monotone operators, which satisfy:\\n⟨F(u) − F(v), u− v⟩ ≥µ∥u − v∥2,\\nand cocoercive operators, which fulfill:\\n⟨F(u) − F(v), u− v⟩ ≥β∥F(u) − F(v)∥2. (2)\\nStrongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max\\nproblems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with β equal\\nto the inverse of the gradient’s Lipschitz constant.\\nDeparting from monotonicity.Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring\\nthe non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and\\nspurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and\\nforemost is the extensively studied setting of ν-weak monotonicity:\\n⟨F(u) − F(v), u− v⟩ ≥ −ν∥u − v∥2.\\nSuch operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it\\nincludes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this\\nproperty. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,\\nhas received much less attention and is given by:\\n⟨F(u) − F(v), u− v⟩ ≥ −γ∥F(u) − F(v)∥2.\\nClearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.\\nBehavior with respect to the solution.While the above properties are standard assumptions in the literature, it is usually sufficient\\nto require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of\\nmonotonicity, it is enough to ask for the operator F to be star-monotone, i.e.,\\n⟨F(u), u− u∗⟩ ≥0,\\nor star-cocoercive,\\n⟨F(u), u− u∗⟩ ≥γ∥F(u)∥2.\\nIn this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the\\noperator F to be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the\\nabove star notions are sometimes required to hold for all solutions u∗, in the following we only require it to hold for a single solution.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 3}, page_content='3 OGDA for problems with weak Minty solutions\\nThe generalized version of OGDA, which we denote with a \"+\" to emphasize the presence of the additional parameter γ, is given by:\\nAlgorithm 1OGDA+\\nRequire: Starting point u0 = u−1 ∈ Rd, step size a >0 and parameter 0 < γ <1.\\nfor k = 0, 1, ...do\\nuk+1 = uk − a((1 + γ)F(uk) − F(uk−1))\\nend for\\nTheorem 3.1.Let F : Rd → Rd be L-Lipschitz continuous satisfying Assumption 1 with 1\\nL > ρ, and let (uk)k≥0 be the iterates\\ngenerated by Algorithm 1 with step size a satisfying a > ρand\\naL ≤ 1 − γ\\n1 + γ . (3)\\nThen, for all k ≥ 0,\\nmin\\ni=0,...,k−1\\n∥F(ui)∥2 ≤ 1\\nkaγ(a − ρ)∥u0 + aF(u0) − u∗∥2.\\nIn particular, as long as ρ <1\\nL , we can find a γ small enough such that the above bound holds.\\nThe first observation is that we would like to choose a as large as possible, as this allows us to treat the largest class of problems\\nwith ρ < a. To be able to choose a large step size a, we must decrease γ, as evident from (3). However, this degrades the algorithm’s\\nspeed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could\\nderive an optimal γ (i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on\\nρ. In practice, the strategy of decreasing γ until convergence is achieved, but not further, yields reasonable results.\\nFurthermore, we want to point out that the condition ρ <1\\nL is precisely the best possible bound for EG+.\\n3.1 Improved bounds under monotonicity\\nWhile the above theorem also holds if the operator F is monotone, we can modify the proof slightly to obtain a better dependence on\\nthe parameters:\\nTheorem 3.2.Let F : Rd → Rd be monotone and L-Lipschitz. If aL = 2−γ\\n2+γ − ϵ for ϵ >0, then the iterates generated by OGDA+\\nfulfill\\nmin\\ni=0,...,k−1\\n∥F(ui)∥2 ≤ 2\\nka2γ2ϵ∥u0 + aF(u0) − u∗∥2.\\nIn particular, we can choose γ = 1 and a < 1\\n2L .\\nThere are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a < 1\\n2L . However, we\\nwant to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as\\nours for OGDA is shown, but requires the conservative step size bounda ≤ 1\\n16L . This was later improved to a ≤ 1\\n3L . All of these\\nonly deal with the case γ = 1. The only other reference that deals with a generalized (i.e., not necessarily γ = 1) version of OGDA\\nis another work, where the resulting step size condition is a ≤ 2−γ\\n4L , which is strictly worse than ours for any γ. To summarize, not\\nonly do we show for the first time that the step size of a generalization of OGDA can go above 1\\n2L , but we also provide the least\\nrestrictive bound for any value of γ.\\n3.2 OGDA+ stochastic\\nIn this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent\\nestimators F(·, ξi) at every iteration. We assume here that the estimator F is unbiased, i.e., E[F(uk, ξ)|uk−1] = F(uk), and has\\nbounded variance E[∥F(uk, ξ) − F(uk)∥2] ≤ σ2. We show that we can still guarantee convergence by using batch sizes B of order\\nO(ϵ−1).\\nAlgorithm 2stochastic OGDA+\\nRequire: Starting point u0 = u−1 ∈ Rd, step size a >0, parameter 0 < γ≤ 1 and batch size B.\\nfor k = 0, 1, ...do\\nSample i.i.d. (ξi)B\\ni=1 and compute estimator ˜gk = 1\\nB\\nPB\\ni=1 F(uk, ξk\\ni )\\nuk+1 = uk − a((1 + γ)˜gk − ˜gk−1)\\nend for\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 4}, page_content='Theorem 3.3. Let F : Rd → Rd be L-Lipschitz satisfying Assumption 1 with 1\\nL > ρ, and let (uk)k≥0 be the sequence of\\niterates generated by stochastic OGDA+, with a and γ satisfying ρ < a <1−γ\\n1+γ\\n1\\nL . Then, to visit an ϵ-stationary point such that\\nmini=0,...,k−1 E[∥F(ui)∥2] < ϵ, we require\\n1\\nkaγ(a − ρ)∥u0 + a˜g0 − u∗∥2 max\\n\\x1a\\n1, 4σ2\\naLϵ\\n\\x1b\\ncalls to the stochastic oracle ˜F, with large batch sizes of order O(ϵ−1).\\nIn practice, large batch sizes of order O(ϵ−1) are typically not desirable; instead, a small or decreasing step size is preferred. In the\\nweak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,\\nthe current analysis does not allow for variable γ.\\n4 EG+ with adaptive step sizes\\nIn this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the\\nLipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to\\nsmall step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in\\nchoosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the\\nstep size is chosen larger than a multiple of the weak Minty parameter ρ to guarantee convergence at all. For these reasons, we want\\nto outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried\\nout.\\nSince the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive\\nstep size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,\\nespecially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially\\ninteresting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in\\nmultiple gradient computations per iteration.\\nWe use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone\\ndecreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox\\nmethod, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.\\nA version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the\\noptimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and\\nare in terms of the gap function.\\nOne of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which\\nresults in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our\\nknowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch\\nis the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of F at\\nall. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative\\nconvergence result is still open.\\nAlgorithm 3EG+ with adaptive step size\\nRequire: Starting points u0, ¯u0 ∈ Rd, initial step size a0 and parameters τ ∈ (0, 1) and 0 < γ≤ 1.\\nfor k = 0, 1, ...do\\nFind the step size:\\nak = min\\n\\x1a\\nak−1, τ∥¯uk − ¯uk−1∥\\n∥F(¯uk) − F(¯uk−1)∥\\n\\x1b\\n(4)\\nCompute next iterate:\\nuk = ¯uk − akF(¯uk)\\n¯uk+1 = ¯uk − akγF (uk).\\nend for\\nClearly, ak is monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that\\nak ≥ min{a0, τ/L} > 0. The sequence therefore converges to a positive number, which we denote by a∞ := limk ak.\\nTheorem 4.1. Let F : Rd → Rd be L-Lipschitz that satisfies Assumption 1, where u∗ denotes any weak Minty solution, with\\na∞ > 2ρ, and let (uk)k≥0 be the iterates generated by Algorithm 3 with γ = 1\\n2 and τ ∈ (0, 1). Then, there exists a k0 ∈ N such that\\nmin\\ni=k0,...,k\\n∥F(uk)∥2 ≤ 1\\nk − k0\\nL\\nτ(a∞/2 − ρ)∥¯uk0 − u∗∥2.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 5}, page_content='Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the\\nLipschitz constant of the operator F does not need to be known. Moreover, the step size choice presented in (4) might allow us\\nto take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later\\niterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage\\nthat they allow us to solve a richer class of problems, as we are able to relax the condition ρ < 1\\n4L in the case of EG+ to ρ < a∞/2,\\nwhere a∞ = limk ak ≥ τ/L.\\nOn the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when\\nak/ak+1 ≤ 1\\nτ is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in\\nthe starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient\\nak/ak+1 being too large. This drawback could be mitigated by choosing τ smaller. However, this will result in poor performance\\ndue to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be\\ncircumvented, and authors instead focused on the convergence of the iterates without any rate.\\n5 Numerical experiments\\nIn the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see\\nAlgorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification\\nof EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with\\nan initial guess made by second-order information, whose extra cost we ignore in the experiments.\\n5.1 Von Neumann’s ratio game\\nWe consider von Neumann’s ratio game, which is given by:\\nmin\\nx∈∆m\\nmax\\ny∈∆n\\nV (x, y) = ⟨x, Ry⟩\\n⟨x, Sy⟩, (5)\\nwhere R ∈ Rm×n and S ∈ Rm×n with ⟨x, Sy⟩ > 0 for all x ∈ ∆m, y∈ ∆n, with ∆ := {z ∈ Rd : zi > 0, Pd\\ni=1 zi = 1} denoting\\nthe unit simplex. Expression (5) can be interpreted as the value V (x, y) for a stochastic game with a single state and mixed strategies.\\nWe see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although\\nan estimated ρ is more than ten times larger than the estimated Lipschitz constant.\\n5.2 Forsaken\\nA particularly difficult min-max toy example with a \"Forsaken\" solution was proposed and is given by:\\nmin\\nx∈R\\nmax\\ny∈R\\nx(y − 0.45) + ϕ(x) − ϕ(y), (6)\\nwhere ϕ(z) = 1\\n6 z6 − 2\\n4 z4 + 1\\n4 z2 − 1\\n2 z. This problem exhibits a Stampacchia solution at (x∗, y∗) ≈ (0.08, 0.4), but also two limit\\ncycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to\\nthe solution repels possible trajectories of iterates, thus \"shielding\" the solution. Later, it was noticed that, restricted to the box\\n∥(x, y)∥∞ < 3, the above-mentioned solution is weak Minty with ρ ≥ 2 · 0.477761, which is much larger than 1\\n2L ≈ 0.08. In line\\nwith these observations, we can see that none of the fixed step size methods with a step size bounded by 1\\nL converge. In light of this\\nobservation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz\\nconstant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling\\nlimit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in\\nthe backtracking procedure.\\n5.3 Lower bound example\\nThe following min-max problem was introduced as a lower bound on the dependence between ρ and L for EG+:\\nmin\\nx∈R\\nmax\\ny∈R\\nµxy + ζ\\n2(x2 − y2). (7)\\nIn particular, it was stated that EG+ (with any γ) and constant step size a = 1\\nL converges for this problem if and only if (0, 0) is a\\nweak Minty solution with ρ <1−γ\\nL , where ρ and L can be computed explicitly in the above example and are given by:\\nL =\\np\\nµ2 + ζ2 and ρ = µ2 − ζ2\\n2µ .\\nBy choosing µ = 3 and ζ = −1, we get exactly ρ = 1\\nL , therefore predicting divergence of EG+ for any γ, which is exactly what is\\nempirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case ρ < 1\\nL , we\\nobserve rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 6}, page_content='6 Conclusion\\nMany intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave\\nframework. Very recently, it was demonstrated that theO(1/k) bounds on the squared operator norm for EG and OGDA for the\\nlast iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the\\npresence of merely weak Minty solutions remains an open question.\\nIn general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the\\nmajority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem\\n(7), which is not covered by theory, and OGDA+ is the only method capable of converging.\\nFinally, we note that the previous paradigm in pure minimization of \"smaller step size ensures convergence\" but \"larger step size\\ngets there faster,\" where the latter is typically constrained by the reciprocal of the gradient’s Lipschitz constant, does not appear\\nto hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates\\nthat convergence can be lost if the step size is excessively small and sometimes needs to be larger than 1\\nL , which one can typically\\nonly hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a\\nbacktracking linesearch.article graphicx\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 0}, page_content='Addressing Popularity Bias with Popularity-Conscious Alignment and\\nContrastive Learning\\nAbstract\\nCollaborative Filtering (CF) often encounters substantial difficulties with popularity bias because of the skewed\\ndistribution of items in real-world datasets. This tendency creates a notable difference in accuracy between items\\nthat are popular and those that are not. This discrepancy impedes the accurate comprehension of user preferences\\nand intensifies the Matthew effect within recommendation systems. To counter popularity bias, current methods\\nconcentrate on highlighting less popular items or on differentiating the correlation between item representations\\nand their popularity. Despite their effectiveness, current approaches continue to grapple with two significant\\nissues: firstly, the extraction of shared supervisory signals from popular items to enhance the representations of\\nless popular items, and secondly, the reduction of representation separation caused by popularity bias. In this\\nstudy, we present an empirical examination of popularity bias and introduce a method called Popularity-Aware\\nAlignment and Contrast (PAAC) to tackle these two problems. Specifically, we utilize the common supervisory\\nsignals found in popular item representations and introduce an innovative popularity-aware supervised alignment\\nmodule to improve the learning of representations for unpopular items. Furthermore, we propose adjusting the\\nweights in the contrastive learning loss to decrease the separation of representations by focusing on popularity.\\nWe confirm the efficacy and logic of PAAC in reducing popularity bias through thorough experiments on three\\nreal-world datasets.\\n1 Introduction\\nContemporary recommender systems are essential in reducing information overload. Personalized recommendations frequently\\nemploy collaborative filtering (CF) to assist users in discovering items that may interest them. CF-based techniques primarily\\nlearn user preferences and item attributes by matching the representations of users with the items they engage with. Despite their\\nachievements, CF-based methods frequently encounter the issue of popularity bias, which leads to considerable disparities in\\naccuracy between items that are popular and those that are not. Popularity bias occurs because there are limited supervisory signals\\nfor items that are not popular, which results in overfitting during the training phase and decreased effectiveness on the test set. This\\nhinders the precise comprehension of user preferences, thereby diminishing the variety of recommendations. Furthermore, popularity\\nbias can worsen the Matthew effect, where items that are already popular gain even more popularity because they are recommended\\nmore frequently.\\nTwo significant challenges are presented when mitigating popularity bias in recommendation systems. The first challenge is the\\ninadequate representation of unpopular items during training, which results in overfitting and limited generalization ability. The\\nsecond challenge, known as representation separation, happens when popular and unpopular items are categorized into distinct\\nsemantic spaces, thereby intensifying the bias and diminishing the precision of recommendations.\\n2 Methodology\\nTo overcome the current difficulties in reducing popularity bias, we introduce the Popularity-Aware Alignment and Contrast (PAAC)\\nmethod. We utilize the common supervisory signals present in popular item representations to direct the learning of unpopular\\nrepresentations, and we present a popularity-aware supervised alignment module. Moreover, we incorporate a re-weighting system\\nin the contrastive learning module to deal with representation separation by considering popularity.\\n2.1 Supervised Alignment Module\\nDuring the training process, the alignment of representations usually emphasizes users and items that have interacted, often causing\\nitems to be closer to interacted users than non-interacted ones in the representation space. However, because unpopular items have\\nlimited interactions, they are usually modeled based on a small group of users. This limited focus can result in overfitting, as the\\nrepresentations of unpopular items might not fully capture their features.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 1}, page_content='The disparity in the quantity of supervisory signals is essential for learning representations of both popular and unpopular items.\\nSpecifically, popular items gain from a wealth of supervisory signals during the alignment process, which helps in effectively\\nlearning their representations. On the other hand, unpopular items, which have a limited number of users providing supervision, are\\nmore susceptible to overfitting. This is because there is insufficient representation learning for unpopular items, emphasizing the\\neffect of supervisory signal distribution on the quality of representation. Intuitively, items interacted with by the same user have\\nsome similar characteristics. In this section, we utilize common supervisory signals in popular item representations and suggest a\\npopularity-aware supervised alignment method to improve the representations of unpopular items.\\nWe initially filter items with similar characteristics based on the user’s interests. For any user, we define the set of items they interact\\nwith. We count the frequency of each item appearing in the training dataset as its popularity. Subsequently, we group items based on\\ntheir relative popularity. We divide items into two groups: the popular item group and the unpopular item group. The popularity of\\neach item in the popular group is higher than that of any item in the unpopular group. This indicates that popular items receive more\\nsupervisory information than unpopular items, resulting in poorer recommendation performance for unpopular items.\\nTo tackle the issue of insufficient representation learning for unpopular items, we utilize the concept that items interacted with by the\\nsame user share some similar characteristics. Specifically, we use similar supervisory signals in popular item representations to\\nimprove the representations of unpopular items. We align the representations of items to provide more supervisory information to\\nunpopular items and improve their representation, as follows:\\nLSA =\\nX\\nu∈U\\n1\\n|Iu|\\nX\\ni∈Iupop,j∈Iuunpop\\n||f(i) − f(j)||2, (1)\\nwhere f(·) is a recommendation encoder and hi = f(i). By efficiently using the inherent information in the data, we provide more\\nsupervisory signals for unpopular items without introducing additional side information. This module enhances the representation of\\nunpopular items, mitigating the overfitting issue.\\n2.2 Re-weighting Contrast Module\\nRecent research has indicated that popularity bias frequently leads to a noticeable separation in the representation of item embeddings.\\nAlthough methods based on contrastive learning aim to enhance overall uniformity by distancing negative samples, their current\\nsampling methods might unintentionally worsen this separation. When negative samples follow the popularity distribution, which\\nis dominated by popular items, prioritizing unpopular items as positive samples widens the gap between popular and unpopular\\nitems in the representation space. Conversely, when negative samples follow a uniform distribution, focusing on popular items\\nseparates them from most unpopular ones, thus worsening the representation gap. Existing studies use the same weights for positive\\nand negative samples in the contrastive loss function, without considering differences in item popularity. However, in real-world\\nrecommendation datasets, the impact of items varies due to dataset characteristics and interaction distributions. Neglecting this\\naspect could lead to suboptimal results and exacerbate representation separation.\\nWe propose to identify different influences by re-weighting different popularity items. To this end, we introduce re-weighting\\ndifferent positive and negative samples to mitigate representation separation from a popularity-centric perspective. We incorporate\\nthis approach into contrastive learning to better optimize the consistency of representations. Specifically, we aim to reduce the risk\\nof pushing items with varying popularity further apart. For example, when using a popular item as a positive sample, our goal is\\nto avoid pushing unpopular items too far away. Thus, we introduce two hyperparameters to control the weights when items are\\nconsidered positive and negative samples.\\nTo ensure balanced and equitable representations of items within our model, we first propose a dynamic strategy to categorize items\\ninto popular and unpopular groups for each mini-batch. Instead of relying on a fixed global threshold, which often leads to the\\noverrepresentation of popular items across various batches, we implement a hyperparameter x. This hyperparameter readjusts the\\nclassification of items within the current batch. By adjusting the hyperparameter x, we maintain a balance between different item\\npopularity levels. This enhances the model’s ability to generalize across diverse item sets by accurately reflecting the popularity\\ndistribution in the current training context. Specifically, we denote the set of items within each batch as IB. And then we divide IB\\ninto a popular group Ipop and an unpopular group Iunpop based on their respective popularity levels, classifying the top x% of items\\nas Ipop:\\nIB = Ipop ∪ Iunpop, ∀i ∈ Ipop ∧ j ∈ Iunpop, p(i) > p(j), (2)\\nwhere Ipop ∈ IB and Iunpop ∈ IB are disjoint, with Ipop consisting of the top x% of items in the batch. In this work, we dynamically\\ndivided items into popular and unpopular groups within each mini-batch based on their popularity, assigning the top 50% as popular\\nitems and the bottom 50% as unpopular items. This radio not only ensures equal representation of both groups in our contrastive\\nlearning but also allows items to be classified adaptively based on the batch’s current composition.\\nAfter that, we use InfoNCE to optimize the uniformity of item representations. Unlike traditional CL-based methods, we calculate\\nthe loss for different item groups. Specifically, we introduce the hyperparameter α to control the positive sample weights between\\npopular and unpopular items, adapting to varying item distributions in different datasets:\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 2}, page_content='LCL\\nitem = α × LCL\\npop + (1 − α) × LCL\\nunpop, (3)\\nwhere LCL\\npop represents the contrastive loss when popular items are considered as positive samples, and LCL\\nunpop represents the\\ncontrastive loss when unpopular items are considered as positive samples. The value of α ranges from 0 to 1, where α = 0 means\\nexclusive emphasis on the loss of unpopular items LCL\\nunpop, and α = 1 means exclusive emphasis on the loss of popular items\\nLCL\\npop. By adjusting α, we can effectively balance the impact of positive samples from both popular and unpopular items, allowing\\nadaptability to varying item distributions in different datasets.\\nFollowing this, we fine-tune the weighting of negative samples in the contrastive learning framework using the hyperparameterβ.\\nThis parameter controls how samples from different popularity groups contribute as negative samples. Specifically, we prioritize\\nre-weighting items with popularity opposite to the positive samples, mitigating the risk of excessively pushing negative samples\\naway and reducing representation separation. Simultaneously, this approach ensures the optimization of intra-group consistency. For\\ninstance, when dealing with popular items as positive samples, we separately calculate the impact of popular and unpopular items\\nas negative samples. The hyperparameter β is then used to control the degree to which unpopular items are pushed away. This is\\nformalized as follows:\\nL\\n′\\npop =\\nX\\ni∈Ipop\\nlog exp(h\\n′\\nihi/τ)P\\nj∈Ipop\\nexp(h\\n′\\nihj/τ) + β P\\nj∈Iunpop\\nexp(h\\n′\\nihj/τ), (4)\\nsimilarly, the contrastive loss for unpopular items is defined as:\\nL\\n′\\nunpop =\\nX\\ni∈Iunpop\\nlog exp(h\\n′\\nihi/τ)P\\nj∈Iunpop exp(h\\n′\\nihj/τ) + β P\\nj∈Ipop exp(h\\n′\\nihj/τ), (5)\\nwhere the parameter β ranges from 0 to 1, controlling the negative sample weighting in the contrastive loss. When β = 0, it means\\nthat only intra-group uniformity optimization is performed. Conversely, when β = 1, it means equal treatment of both popular and\\nunpopular items in terms of their impact on positive samples. The setting of β allows for a flexible adjustment between prioritizing\\nintra-group uniformity and considering the impact of different popularity levels in the training. We prefer to push away items\\nwithin the same group to optimize uniformity. This setup helps prevent over-optimizing the uniformity of different groups, thereby\\nmitigating representation separation.\\nThe final re-weighting contrastive objective is the weighted sum of the user objective and the item objective:\\nLCL = 1\\n2 × (LCL\\nitem + LCL\\nuser). (6)\\nIn this way, we not only achieved consistency in representation but also reduced the risk of further separating items with similar\\ncharacteristics into different representation spaces, thereby alleviating the issue of representation separation caused by popularity\\nbias.\\n2.3 Model Optimization\\nTo reduce popularity bias in collaborative filtering tasks, we employ a multi-task training strategy to jointly optimize the classic\\nrecommendation loss (LREC ), supervised alignment loss (LSA), and re-weighting contrast loss (LCL).\\nL = LREC + λ1LSA + λ2LCL + λ3||Θ||2, (7)\\nwhere Θ is the set of model parameters in LREC as we do not introduce additional parameters, λ1 and λ2 are hyperparameters that\\ncontrol the strengths of the popularity-aware supervised alignment loss and the re-weighting contrastive learning loss respectively,\\nand λ3 is the L2 regularization coefficient. After completing the model training process, we use the dot product to predict unknown\\npreferences for recommendations.\\n3 Experiments\\nIn this section, we assess the efficacy of PAAC through comprehensive experiments, aiming to address the following research\\nquestions:\\n• How does PAAC compare to existing debiasing methods?\\n• How do different designed components play roles in our proposed PAAC?\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 3}, page_content='• How does PAAC alleviate the popularity bias?\\n• How do different hyper-parameters affect the PAAC recommendation performance?\\n3.1 Experiments Settings\\n3.1.1 Datasets\\nIn our experiments, we use three widely public datasets: Amazon-book, Yelp2018, and Gowalla. We retained users and items with a\\nminimum of 10 interactions.\\n3.1.2 Baselines and Evaluation Metrics\\nWe implement the state-of-the-art LightGCN to instantiate PAAC, aiming to investigate how it alleviates popularity bias. We\\ncompare PAAC with several debiased baselines, including re-weighting-based models, decorrelation-based models, and contrastive\\nlearning-based models.\\nWe utilize three widely used metrics, namely Recall@K, HR@K, and NDCG@K, to evaluate the performance of Top-K recommen-\\ndation. Recall@K and HR@K assess the number of target items retrieved in the recommendation results, emphasizing coverage. In\\ncontrast, NDCG@K evaluates the positions of target items in the ranking list, with a focus on their positions in the list. We use\\nthe full ranking strategy, considering all non-interacted items as candidate items to avoid selection bias during the test stage. We\\nrepeated each experiment five times with different random seeds and reported the average scores.\\n3.2 Overall Performance\\nAs shown in Table 1, we compare our model with several baselines across three datasets. The best performance for each metric\\nis highlighted in bold, while the second best is underlined. Our model consistently outperforms all compared methods across all\\nmetrics in every dataset.\\n• Our proposed model PAAC consistently outperforms all baselines and significantly mitigates the popularity bias. Specif-\\nically, PAAC enhances LightGCN, achieving improvements of 282.65%, 180.79%, and 82.89% in NDCG@20 on the\\nYelp2018, Gowalla, and Amazon-Book datasets, respectively. Compared to the strongest baselines, PAAC delivers better\\nperformance. The most significant improvements are observed on Yelp2018, where our model achieves an 8.70% increase\\nin Recall@20, a 10.81% increase in HR@20, and a 30.2% increase in NDCG@20. This improvement can be attributed\\nto our use of popularity-aware supervised alignment to enhance the representation of less popular items and re-weighted\\ncontrastive learning to address representation separation from a popularity-centric perspective.\\n• The performance improvements of PAAC are smaller on sparser datasets. For example, on the Gowalla dataset, the\\nimprovements in Recall@20, HR@20, and NDCG@20 are 3.18%, 5.85%, and 5.47%, respectively. This may be because,\\nin sparser datasets like Gowalla, even popular items are not well-represented due to lower data density. Aligning unpopular\\nitems with these poorly represented popular items can introduce noise into the model. Therefore, the benefits of using\\nsupervisory signals for unpopular items may be reduced in very sparse environments, leading to smaller performance\\nimprovements.\\n• Regarding the baselines for mitigating popularity bias, the improvement of some is relatively limited compared to the\\nbackbone model (LightGCN) and even performs worse in some cases. This may be because some are specifically designed\\nfor traditional data-splitting scenarios, where the test set still follows a long-tail distribution, leading to poor generalization.\\nSome mitigate popularity bias by excluding item popularity information. Others use invariant learning to remove popularity\\ninformation at the representation level, generally performing better than the formers. This shows the importance of\\naddressing popularity bias at the representation level. Some outperform the other baselines, emphasizing the necessary to\\nimprove item representation consistency for mitigating popularity bias.\\n• Different metrics across various datasets show varying improvements in model performance. This suggests that different\\ndebiasing methods may need distinct optimization strategies for models. Additionally, we observe varying effects of PAAC\\nacross different datasets. This difference could be due to the sparser nature of the Gowalla dataset. Conversely, our model\\ncan directly provide supervisory signals for unpopular items and conduct intra-group optimization, consistently maintaining\\noptimal performance across all metrics on the three datasets.\\n3.3 Ablation Study\\nTo better understand the effectiveness of each component in PAAC, we conduct ablation studies on three datasets. Table 2 presents a\\ncomparison between PAAC and its variants on recommendation performance. Specifically, PAAC-w/o P refers to the variant where\\nthe re-weighting contrastive loss of popular items is removed, focusing instead on optimizing the consistency of representations for\\nunpopular items. Similarly, PAAC-w/o U denotes the removal of the re-weighting contrastive loss for unpopular items. PAAC-w/o\\nA refers to the variant without the popularity-aware supervised alignment loss. It’s worth noting that PAAC-w/o A differs from\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 4}, page_content='Table 1: Performance comparison on three public datasets with K = 20. The best performance is indicated in bold, while the\\nsecond-best performance is underlined. The superscripts * indicate p ≤ 0.05 for the paired t-test of PAAC vs. the best baseline (the\\nrelative improvements are denoted as Imp.).\\n!\\nModel Yelp2018 Gowalla Amazon-book\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\nMF 0.0050 0.0109 0.0093 0.0343 0.0422 0.0280 0.0370 0.0388 0.0270\\nLightGCN 0.0048 0.0111 0.0098 0.0380 0.0468 0.0302 0.0421 0.0439 0.0304\\nIPS 0.0104 0.0183 0.0158 0.0562 0.0670 0.0444 0.0488 0.0510 0.0365\\nMACR 0.0402 0.0312 0.0265 0.0908 0.1086 0.0600 0.0515 0.0609 0.0487\\nα-Adjnorm 0.0053 0.0088 0.0080 0.0328 0.0409 0.0267 0.0422 0.0450 0.0264\\nInvCF 0.0444 0.0344 0.0291 0.1001 0.1202 0.0662 0.0562 0.0665 0.0515\\nAdap-τ 0.0450 0.0497 0.0341 0.1182 0.1248 0.0794 0.0641 0.0678 0.0511\\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\\nImp. +9.78 % +10.81% +8.70% +3.18% +5.85% +5.47% +9.36% +6.78% 5.90%\\nSimGCL in that we split the contrastive loss on the item side, LCL\\nitem, into two distinct losses: LCL\\npop and LCL\\nunpop. This approach\\nallows us to separately address the consistency of popular and unpopular item representations, thereby providing a more detailed\\nanalysis of the impact of each component on the overall performance.\\nFrom Table 2, we observe that PAAC-w/o A outperforms SimGCL in most cases. This validates that re-weighting the importance of\\npopular and unpopular items can effectively improve the model’s performance in alleviating popularity bias. It also demonstrates the\\neffectiveness of using supervision signals from popular items to enhance the representations of unpopular items, providing more\\nopportunities for future research on mitigating popularity bias. Moreover, compared with PAAC-w/o U, PAAC-w/o P results in much\\nworse performance. This confirms the importance of re-weighting popular items in contrastive learning for mitigating popularity\\nbias. Finally, PAAC consistently outperforms the three variants, demonstrating the effectiveness of combining supervised alignment\\nand re-weighting contrastive learning. Based on the above analysis, we conclude that leveraging supervisory signals from popular\\nitem representations can better optimize representations for unpopular items, and re-weighting contrastive learning allows the model\\nto focus on more informative or critical samples, thereby improving overall performance. All the proposed modules significantly\\ncontribute to alleviating popularity bias.\\nTable 2: Ablation study of PAAC, highlighting the best-performing model on each dataset and metrics in bold. Specifically,\\nPAAC-w/o P removes the re-weighting contrastive loss of popular items, PAAC-w/o U eliminates the re-weighting contrastive loss\\nof unpopular items, and PAAC-w/o A omits the popularity-aware supervised alignment loss.\\n!\\nModel Yelp2018 Gowalla Amazon-book\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\\nPAAC-w/o P 0.0443 0.0536 0.0340 0.1098 0.1191 0.0750 0.0616 0.0639 0.0458\\nPAAC-w/o U 0.0462 0.0545 0.0358 0.1120 0.1179 0.0752 0.0594 0.0617 0.0464\\nPAAC-w/o A 0.0466 0.0547 0.0360 0.1195 0.1260 0.0815 0.0687 0.0711 0.0536\\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\\n3.4 Debias Ability\\nTo further verify the effectiveness of PAAC in alleviating popularity bias, we conduct a comprehensive analysis focusing on the\\nrecommendation performance across different popularity item groups. Specifically, 20% of the most popular items are labeled\\n’Popular’, and the rest are labeled ’Unpopular’. We compare the performance of PAAC with LightGCN, IPS, MACR, and SimGCL\\nusing the NDCG@20 metric across different popularity groups. We use ∆ to denote the accuracy gap between the two groups. We\\ndraw the following conclusions:\\n• Improving the performance of unpopular items is crucial for enhancing overall model performance. Specially, on the\\nYelp2018 dataset, PAAC shows reduced accuracy in recommending popular items, with a notable decrease of 20.14%\\ncompared to SimGCL. However, despite this decrease, the overall recommendation accuracy surpasses that of SimGCL\\nby 11.94%, primarily due to a 6.81% improvement in recommending unpopular items. This improvement highlights the\\nimportance of better recommendations for unpopular items and emphasizes their crucial role in enhancing overall model\\nperformance.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 5}, page_content='• Our proposed PAAC significantly enhances the recommendation performance for unpopular items. Specifically, we observe\\nan improvement of 8.94% and 7.30% in NDCG@20 relative to SimGCL on the Gowalla and Yelp2018 datasets, respectively.\\nThis improvement is due to the popularity-aware alignment method, which uses supervisory signals from popular items to\\nimprove the representations of unpopular items.\\n• PAAC has successfully narrowed the accuracy gap between different item groups. Specifically, PAAC achieved the smallest\\ngap, reducing the NDCG@20 accuracy gap by 34.18% and 87.50% on the Gowalla and Yelp2018 datasets, respectively.\\nThis indicates that our method treats items from different groups fairly, effectively alleviating the impact of popularity\\nbias. This success can be attributed to our re-weighted contrast module, which addresses representation separation from a\\npopularity-centric perspective, resulting in more consistent recommendation results across different groups.\\n3.5 Hyperparameter Sensitivities\\nIn this section, we analyze the impact of hyperparameters in PAAC. Firstly, we investigate the influence of λ1 and λ2, which\\nrespectively control the impact of the popularity-aware supervised alignment and re-weighting contrast loss. Additionally, in the\\nre-weighting contrastive loss, we introduce two hyperparameters, α and β, to control the re-weighting of different popularity items\\nas positive and negative samples. Finally, we explore the impact of the grouping ratio x on the model’s performance.\\n3.5.1 Effect of λ1 and λ2\\nAs formulated in Eq. (11), λ1 controls the extent of providing additional supervisory signals for unpopular items, while λ2 controls\\nthe extent of optimizing representation consistency. Horizontally, with the increase inλ2, the performance initially increases and\\nthen decreases. This indicates that appropriate re-weighting contrastive loss effectively enhances the consistency of representation\\ndistributions, mitigating popularity bias. However, overly strong contrastive loss may lead the model to neglect recommendation\\naccuracy. Vertically, as λ1 increases, the performance also initially increases and then decreases. This suggests that suitable\\nalignment can provide beneficial supervisory signals for unpopular items, while too strong an alignment may introduce more noise\\nfrom popular items to unpopular ones, thereby impacting recommendation performance.\\n3.5.2 Effect of re-weighting coefficient α and β\\nTo mitigate representation separation due to imbalanced positive and negative sampling, we introduce two hyperparameters into the\\ncontrastive loss. Specifically, α controls the weight difference between positive samples from popular and unpopular items, while β\\ncontrols the influence of different popularity items as negative samples.\\nIn our experiments, while keeping other hyperparameters constant, we search α and β within the range {0, 0.2, 0.4, 0.6, 0.8, 1}. As\\nα and β increase, performance initially improves and then declines. The optimal hyperparameters for the Yelp2018 and Gowalla\\ndatasets are α = 0.8, β = 0.6 and α = 0.2, β = 0.2, respectively. This may be attributed to the characteristics of the datasets. The\\nYelp2018 dataset, with a higher average interaction frequency per item, benefits more from a higher weightα for popular items as\\npositive samples. Conversely, the Gowalla dataset, being relatively sparse, prefers a smaller α. This indicates the importance of\\nconsidering dataset characteristics when adjusting the contributions of popular and unpopular items to the model.\\nNotably, α and β are not highly sensitive within the range [0, 1], performing well across a broad spectrum. Performance exceeds the\\nbaseline regardless of β values when other parameters are optimal. Additionally, α values from [0.4, 1.0] on the Yelp2018 dataset\\nand [0.2, 0.8] on the Gowalla dataset surpass the baseline, indicating less need for precise tuning. Thus, α and β achieve optimal\\nperformance without meticulous adjustments, focusing on weight coefficients to maintain model efficacy.\\n3.5.3 Effect of grouping ratio x\\nTo investigate the impact of different grouping ratios on recommendation performance, we developed a flexible classification\\nmethod for items within each mini-batch based on their popularity. Instead of adopting a fixed global threshold, which tends to\\noverrepresent popular items in some mini-batches, our approach dynamically divides items in each mini-batch into popular and\\nunpopular categories. Specifically, the top x% of items are classified as popular and the remaining (100 - x)% as unpopular, with x\\nvarying. This strategy prevents the overrepresentation typical in fixed distribution models, which could skew the learning process\\nand degrade performance. To quantify the effects of these varying ratios, we examined various division ratios for popular items,\\nincluding 20%, 40%, 60%, and 80%, as shown in Table 3. The preliminary results indicate that both extremely low and high ratios\\nnegatively affect model performance, thereby underscoring the superiority of our dynamic data partitioning approach. Moreover,\\nwithin the 40%-60% range, our model’s performance remained consistently robust, further validating the effectiveness of PAAC.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 6}, page_content='Table 3: Performance comparison across varying popular item ratios x on metrics.\\n!\\nRatio Yelp2018 Gowalla\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\n20% 0.0467 0.0555 0.0361 0.1232 0.1319 0.0845\\n40% 0.0505 0.0581 0.0378 0.1239 0.1325 0.0848\\n50% 0.0494 0.0574 0.0375 0.1232 0.1321 0.0848\\n60% 0.0492 0.0569 0.0370 0.1225 0.1314 0.0843\\n80% 0.0467 0.0545 0.0350 0.1176 0.1270 0.0818\\n4 Related Work\\n4.1 Popularity Bias in Recommendation\\nPopularity bias is a prevalent problem in recommender systems, where unpopular items in the training dataset are seldom recom-\\nmended. Numerous techniques have been suggested to examine and decrease performance variations between popular and unpopular\\nitems. These techniques can be broadly divided into three categories.\\n• Re-weighting-based methods aim to increase the training weight or scores for unpopular items, redirecting focus away\\nfrom popular items during training or prediction. For instance, IPS adds compensation to unpopular items and adjusts\\nthe prediction of the user-item preference matrix, resulting in higher preference scores and improving rankings for\\nunpopular items. α-AdjNorm enhances the focus on unpopular items by controlling the normalization strength during the\\nneighborhood aggregation process in GCN-based models.\\n• Decorrelation-based methods aim to effectively remove the correlations between item representations (or prediction scores)\\nand popularity. For instance, MACR uses counterfactual reasoning to eliminate the direct impact of popularity on item\\noutcomes. In contrast, InvCF operates on the principle that item representations remain invariant to changes in popularity\\nsemantics, filtering out unstable or outdated popularity characteristics to learn unbiased representations.\\n• Contrastive-learning-based methods aim to achieve overall uniformity in item representations using InfoNCE, preserving\\nmore inherent characteristics of items to mitigate popularity bias. This approach has been demonstrated as a state-of-the-art\\nmethod for alleviating popularity bias. It employs data augmentation techniques such as graph augmentation or feature\\naugmentation to generate different views, maximizing positive pair consistency and minimizing negative pair consistency\\nto promote more uniform representations. Specifically, Adap- τ adjusts user/item embeddings to specific values, while\\nSimGCL integrates InfoNCE loss to enhance representation uniformity and alleviate popularity bias.\\n4.2 Representation Learning for CF\\nRepresentation learning is crucial in recommendation systems, especially in modern collaborative filtering (CF) techniques. It\\ncreates personalized embeddings that capture user preferences and item characteristics. The quality of these representations critically\\ndetermines a recommender system’s effectiveness by precisely capturing the interplay between user interests and item features.\\nRecent studies emphasize two fundamental principles in representation learning: alignment and uniformity. The alignment principle\\nensures that embeddings of similar or related items (or users) are closely clustered together, improving the system’s ability to\\nrecommend items that align with a user’s interests. This principle is crucial when accurately reflecting user preferences through\\ncorresponding item characteristics. Conversely, the uniformity principle ensures a balanced distribution of all embeddings across the\\nrepresentation space. This approach prevents the over-concentration of embeddings in specific areas, enhancing recommendation\\ndiversity and improving generalization to unseen data.\\nIn this work, we focus on aligning the representations of popular and unpopular items interacted with by the same user and re-\\nweighting uniformity to mitigate representation separation. Our model PAAC uniquely addresses popularity bias by combining group\\nalignment and contrastive learning, a first in the field. Unlike previous works that align positive user-item pairs or contrastive pairs,\\nPAAC directly aligns popular and unpopular items, leveraging the rich information of popular items to enhance the representations\\nof unpopular items and reduce overfitting. Additionally, we introduce targeted re-weighting from a popularity-centric perspective to\\nachieve a more balanced representation.\\n5 Conclusion\\nIn this study, we have examined popularity bias and put forward PAAC as a method to lessen its impact. We postulated that items\\nengaged with by the same user exhibit common traits, and we utilized this insight to coordinate the representations of both popular\\nand unpopular items via a popularity-conscious supervised alignment method. This strategy furnished additional supervisory data for\\nless popular items. It is important to note that our concept of aligning and categorizing items according to user-specific preferences\\nintroduces a fresh perspective on alignment. Moreover, we tackled the problem of representation separation seen in current CL-based\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 7}, page_content='models by incorporating two hyperparameters to regulate the influence of items with varying popularity levels when considered\\nas positive and negative samples. This method refined the uniformity of representations and successfully reduced separation. We\\nvalidated our method, PAAC, on three publicly available datasets, demonstrating its effectiveness and underlying rationale.\\nIn the future, we will explore deeper alignment and contrast adjustments tailored to specific tasks to further mitigate popularity\\nbias. We aim to investigate the synergies between alignment and contrast and extend our approach to address other biases in\\nrecommendation systems.\\nAcknowledgments\\nThis work was supported in part by grants from the National Key Research and Development Program of China, the National Natural\\nScience Foundation of China, the Fundamental Research Funds for the Central Universities, and Quan Cheng Laboratory.\\n8')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 0}, page_content='Detecting Medication Usage in Parkinson’s Disease Through\\nMulti-modal Indoor Positioning: A Pilot Study in a Naturalistic\\nEnvironment\\nAbstract\\nParkinson’s disease (PD) is a progressive neurodegenerative disorder that leads to motor symptoms, including gait\\nimpairment. The effectiveness of levodopa therapy, a common treatment for PD, can fluctuate, causing periods of\\nimproved mobility (\"on\" state) and periods where symptoms re-emerge (\"off\" state). These fluctuations impact\\ngait speed and increase in severity as the disease progresses. This paper proposes a transformer-based method that\\nuses both Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices to enhance\\nindoor localization accuracy. A secondary goal is to determine if indoor localization, particularly in-home gait\\nspeed features (like the time to walk between rooms), can be used to identify motor fluctuations by detecting if a\\nperson with PD is taking their levodopa medication or not. The method is evaluated using a real-world dataset\\ncollected in a free-living setting, where movements are varied and unstructured. Twenty-four participants, living\\nin pairs (one with PD and one control), resided in a sensor-equipped smart home for five days. The results show\\nthat the proposed network surpasses other methods for indoor localization. The evaluation of the secondary goal\\nreveals that accurate room-level localization, when converted into in-home gait speed features, can accurately\\npredict whether a PD participant is taking their medication or not.\\n1 Introduction\\nParkinson’s disease (PD) is a debilitating neurodegenerative condition that affects approximately 6 million individuals globally.\\nIt manifests through various motor symptoms, including bradykinesia (slowness of movement), rigidity, and gait impairment. A\\ncommon complication associated with levodopa, the primary medication for PD, is the emergence of motor fluctuations that are\\nlinked to medication timing. Initially, patients experience a consistent and extended therapeutic effect when starting levodopa.\\nHowever, as the disease advances, a significant portion of patients begin to experience \"wearing off\" of their medication before\\nthe next scheduled dose, resulting in the reappearance of parkinsonian symptoms, such as slowed gait. These fluctuations in\\nsymptoms negatively impact patients’ quality of life and often necessitate adjustments to their medication regimen. The severity\\nof motor symptoms can escalate to the point where they impede an individual’s ability to walk and move within their own home.\\nConsequently, individuals may be inclined to remain confined to a single room, and when they do move, they may require more time\\nto transition between rooms. These observations could potentially be used to identify periods when PD patients are experiencing\\nmotor fluctuations related to their medication being in an ON or OFF state, thereby providing valuable information to both clinicians\\nand patients.\\nA sensitive and accurate ecologically-validated biomarker for PD progression is currently unavailable, which has contributed to\\nfailures in clinical trials for neuroprotective therapies in PD. Gait parameters are sensitive to disease progression in unmedicated\\nearly-stage PD and show promise as markers of disease progression, making measuring gait parameters potentially useful in clinical\\ntrials of disease-modifying interventions. Clinical evaluations of PD are typically conducted in artificial clinic or laboratory settings,\\nwhich only capture a limited view of an individual’s motor function. Continuous monitoring could capture symptom progression,\\nincluding motor fluctuations, and sensitively quantify them over time.\\nWhile PD symptoms, including gait and balance parameters, can be measured continuously at home using wearable devices\\ncontaining inertial motor units (IMUs) or smartphones, this data does not show the context in which the measurements are taken.\\nDetermining a person’s location within a home (indoor localization) could provide valuable contextual information for interpreting\\nPD symptoms. For instance, symptoms like freezing of gait and turning in gait vary depending on the environment, so knowing a\\nperson’s location could help predict such symptoms or interpret their severity. Additionally, understanding how much time someone\\nspends alone or with others in a room is a step towards understanding their social participation, which impacts quality of life in\\nPD. Localization could also provide valuable information in the measurement of other behaviors such as non-motor symptoms like\\nurinary function (e.g., how many times someone visits the toilet room overnight).'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 1}, page_content='IoT-based platforms with sensors capturing various modalities of data, combined with machine learning, can be used for unobtrusive\\nand continuous indoor localization in home environments. Many of these techniques utilize radio-frequency signals, specifically the\\nReceived Signal Strength Indication (RSSI), emitted by wearables and measured at access points (AP) throughout a home. These\\nsignals estimate the user’s position based on perceived signal strength, creating radio-map features for each room. To improve\\nlocalization accuracy, accelerometer data from wearable devices, along with RSSI, can be used to distinguish different activities\\n(e.g., walking vs. standing). Since some activities are associated with specific rooms (e.g., stirring a pan on the stove is likely to\\noccur in a kitchen), accelerometer data can enhance RSSI’s ability to differentiate between adjacent rooms, an area where RSSI\\nalone may be insufficient.\\nThe heterogeneity of PD, where symptoms and their severity vary between patients, poses a challenge for generalizing accelerometer\\ndata across different individuals. Severe symptoms, such as tremors, can introduce bias and accumulated errors in accelerometer data,\\nparticularly when collected from wrist-worn devices, which are a common and well-accepted placement location. Naively combining\\naccelerometer data with RSSI may degrade indoor localization performance due to varying tremor levels in the acceleration signal.\\nThis work makes two primary contributions to address these challenges.\\n(1) We detail the use of RSSI, augmented by accelerometer data, to achieve room-level localization. Our proposed network\\nintelligently selects accelerometer features that can enhance RSSI performance in indoor localization. To rigorously assess our\\nmethod, we utilize a free-living dataset (where individuals live without external intervention) developed by our group, encompassing\\ndiverse and unstructured movements as expected in real-world scenarios. Evaluation on this dataset, including individuals with and\\nwithout PD, demonstrates that our network outperforms other methods across all cross-validation categories.\\n(2) We demonstrate how accurate room-level localization predictions can be transformed into in-home gait speed biomarkers (e.g.,\\nnumber of room-to-room transitions, room-to-room transition duration). These biomarkers can effectively classify the OFF or ON\\nmedication state of a PD patient from this pilot study data.\\n2 Related Work\\nExtensive research has utilized home-based passive sensing systems to evaluate how the activities and behavior of individuals with\\nneurological conditions, primarily cognitive dysfunction, change over time. However, there is limited work assessing room use in\\nthe home setting in people with Parkinson’s.\\nGait quantification using wearables or smartphones is an area where a significant amount of work has been done. Cameras can\\nalso detect parkinsonian gait and some gait features, including step length and average walking speed. Time-of-flight devices,\\nwhich measure distances between the subject and the camera, have been used to assess medication adherence through gait analysis.\\nFrom free-living data, one approach to gait and room use evaluation in home settings is by emitting and detecting radio waves to\\nnon-invasively track movement. Gait analysis using radio wave technology shows promise to track disease progression, severity, and\\nmedication response. However, this approach cannot identify who is doing the movement and also suffers from technical issues\\nwhen the radio waves are occluded by another object. Much of the work done so far using video to track PD symptoms has focused\\non the performance of structured clinical rating scales during telemedicine consultations as opposed to naturalistic behavior, and\\nthere have been some privacy concerns around the use of video data at home.\\nRSSI data from wearable devices is a type of data with fewer privacy concerns; it can be measured continuously and unobtrusively\\nover long periods to capture real-world function and behavior in a privacy-friendly way. In indoor localization, fingerprinting using\\nRSSI is the typical technique used to estimate the wearable (user) location by using signal strength data representing a coarse and\\nnoisy estimate of the distance from the wearable to the access point. RSSI signals are not stable; they fluctuate randomly due to\\nshadowing, fading, and multi-path effects. However, many techniques have been proposed in recent years to tackle these fluctuations\\nand indirectly improve localization accuracy. Some works utilize deep neural networks (DNN) to generate coarse positioning\\nestimates from RSSI signals, which are then refined by a hidden Markov model (HMM) to produce a final location estimate. Other\\nworks try to utilize a time series of RSSI data and exploit the temporal connections within each access point to estimate room-level\\nposition. A CNN is used to build localization models to further leverage the temporal dependencies across time-series readings.\\nIt has been suggested that we cannot rely on RSSI alone for indoor localization in home environments for PD subjects due to\\nshadowing rooms with tight separation. Some researchers combine RSSI signals and inertial measurement unit (IMU) data to test\\nthe viability of leveraging other sensors in aiding the positioning system to produce a more accurate location estimate. Classic\\nmachine learning approaches such as Random Forest (RF), Artificial Neural Network (ANN), and k-Nearest Neighbor (k-NN) are\\ntested, and the result shows that the RF outperforms other methods in tracking a person in indoor environments. Others combine\\nsmartphone IMU sensor data and Wi-Fi-received signal strength indication (RSSI) measurements to estimate the exact location (in\\nEuclidean position X, Y) of a person in indoor environments. The proposed sensor fusion framework uses location fingerprinting in\\ncombination with a pedestrian dead reckoning (PDR) algorithm to reduce positioning errors.\\nLooking at this multi-modality classification/regression problem from a time series perspective, there has been a lot of exploration\\nin tackling a problem where each modality can be categorized as multivariate time series data. LSTM and attention layers are\\noften used in parallel to directly transform raw multivariate time series data into a low-dimensional feature representation for each\\nmodality. Later, various processes are done to further extract correlations across modalities through the use of various layers (e.g.,\\nconcatenation, CNN layer, transformer, self-attention). Our work is inspired by prior research where we only utilize accelerometer\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 2}, page_content='data to enrich the RSSI, instead of utilizing all IMU sensors, in order to reduce battery consumption. In addition, unlike previous\\nwork that stops at predicting room locations, we go a step further and use room-to-room transition behaviors as features for a binary\\nclassifier predicting whether people with PD are taking their medications or withholding them.\\n3 Cohort and Dataset\\n**Dataset:** This dataset was collected using wristband wearable sensors, one on each wrist of all participants, containing tri-axial\\naccelerometers and 10 Access Points (APs) placed throughout the residential home, each measuring the RSSI. The wearable devices\\nwirelessly transmit data using the Bluetooth Low Energy (BLE) standard, which can be received by the 10 APs. Each AP records the\\ntransmitted packets from the wearable sensor, which contains the accelerometer readings sampled at 30Hz, with each AP recording\\nRSSI values sampled at 5 Hz.\\nThe dataset contains 12 spousal/parent-child/friend-friend pairs (24 participants in total) living freely in a smart home for five days.\\nEach pair consists of one person with PD and one healthy control volunteer (HC). This pairing was chosen to enable PD vs. HC\\ncomparison, for safety reasons, and also to increase the naturalistic social behavior (particularly amongst the spousal pairs who\\nalready lived together). From the 24 participants, five females and seven males have PD. The average age of the participants is 60.25\\n(PD 61.25, Control 59.25), and the average time since PD diagnosis for the person with PD is 11.3 years (range 0.5-19).\\nTo measure the accuracy of the machine learning models, wall-mounted cameras are installed on the ground floor of the house,\\nwhich capture red-green-blue (RGB) and depth data 2-3 hours daily (during daylight hours at times when participants were at home).\\nThe videos were then manually annotated to the nearest millisecond to provide localization labels. Multiple human labelers used\\nsoftware called ELAN to watch up to 4 simultaneously-captured video files at a time. The resulting labeled data recorded the kitchen,\\nhallway, dining room, living room, stairs, and porch. The duration of labeled data recorded by the cameras for PD and HC is 72.84\\nand 75.31 hours, respectively, which provides a relatively balanced label set for our room-level classification. Finally, to evaluate\\nthe ON/OFF medication state, participants with PD were asked to withhold their dopaminergic medications so that they were in\\nthe practically-defined OFF medications state for a temporary period of several hours during the study. Withholding medications\\nremoves their mitigation on symptoms, leading to mobility deterioration, which can include slowing of gait.\\n**Data pre-processing for indoor localization:** The data from the two wearable sensors worn by each participant were combined at\\neach time point, based on their modality, i.e., twenty RSSI values (corresponding to 10 APs for each of the two wearable sensors)\\nand accelerometry traces in six spatial directions (corresponding to the three spatial directions (x, y, z) for each wearable) were\\nrecorded at each time point. The accelerometer data is resampled to 5Hz to synchronize the data with RSSI values. With a 5-second\\ntime window and a 5Hz sampling rate, each RSSI data sample has an input of size (25 x 20), and accelerometer data has an input of\\nsize (25 x 6). Imputation for missing values, specifically for RSSI data, is applied by replacing the missing values with a value that is\\nnot possible normally (i.e., -120dB). Missing values exist in RSSI data whenever the wearable is out of range of an AP. Finally, all\\ntime-series measurements by the modalities are normalized.\\n**Data pre-processing for medication state:** Our main focus is for our neural network to continuously produce room predictions,\\nwhich are then transformed into in-home gait speed features, particularly for persons with PD. We hypothesize that during their\\nOFF medication state, the deterioration in mobility of a person with PD is exhibited by how they transition between rooms. These\\nfeatures include ’Room-to-room Transition Duration’ and the ’Number of Transitions’ between two rooms. ’Number of Transitions’\\nrepresents how active PD subjects are within a certain period of time, while ’Room-to-room Transition Duration’ may provide\\ninsight into how severe their disease is by the speed with which they navigate their home environment. With the layout of the house\\nwhere participants stayed, the hallway is used as a hub connecting all other rooms labeled, and ’Room-to-room Transition’ shows\\nthe transition duration (in seconds) between two rooms connected by the hallway. The transition between (1) kitchen and living\\nroom, (2) kitchen and dining room, and (3) dining room and living room are chosen as the features due to their commonality across\\nall participants. For these features, we limit the transition time duration (i.e., the time spent in the hallway) to 60 seconds to exclude\\ntransitions likely to be prolonged and thus may not be representative of the person’s mobility.\\nThese in-home gait speed features are produced by an indoor-localization model by feeding RSSI signals and accelerometer data\\nfrom 12 PD participants from 6 a.m. to 10 p.m. daily, which are aggregated into 4-hour windows. From this, each PD participant\\nwill have 20 data samples (four data samples for each of the five days), each of which contains six features (three for the mean of\\nroom-to-room transition duration and three for the number of room-to-room transitions). There is only one 4-hour window during\\nwhich the person with PD is OFF medications. These samples are then used to train a binary classifier determining whether a person\\nwith PD is ON or OFF their medications.\\nFor a baseline comparison to the in-home gait speed features, demographic features which include age, gender, years of PD, and\\nMDS-UPDRS III score (the gold-standard clinical rating scale score used in clinical trials to measure motor disease severity in\\nPD) are chosen. Two MDS-UPDRS III scores are assigned for each PD participant; one is assigned when a person with PD is ON\\nmedications, and the other one is assigned when a person with PD is OFF medications. For each in-home gait speed feature data\\nsample, there will be a corresponding demographic feature data sample that is used to train a different binary classifier to predict\\nwhether a person with PD is ON or OFF medications.\\n**Ethical approval:** Full approval from the NHS Wales Research Ethics Committee was granted on December 17, 2019, and\\nHealth Research Authority and Health and Care Research Wales approval was confirmed on January 14, 2020; the research was\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 3}, page_content='conducted in accord with the Helsinki Declaration of 1975; written informed consent was gained from all study participants. In\\norder to protect participant privacy, supporting data is not shared openly. It will be made available to bona fide researchers subject to\\na data access agreement.\\n4 Methodologies and Framework\\nWe introduce Multihead Dual Convolutional Self Attention (MDCSA), a deep neural network that utilizes dual modalities for indoor\\nlocalization in home environments. The network addresses two challenges that arise from multimodality and time-series data:\\n(1) Capturing multivariate features and filtering multimodal noises. RSSI signals, which are measured at multiple access points\\nwithin a home received from wearable communication, have been widely used for indoor localization, typically using a fingerprinting\\ntechnique that produces a ground truth radio map of a home. Naturally, the wearable also produces acceleration measurements which\\ncan be used to identify typical activities performed in a specific room, and thus we can explore if accelerometer data will enrich\\nthe RSSI signals, in particular to help distinguish adjacent rooms, which RSSI-only systems typically struggle with. If it will, how\\ncan we incorporate these extra features (and modalities) into the existing features for accurate room predictions, particularly in the\\ncontext of PD where the acceleration signal may be significantly impacted by the disease itself?\\n(2) Modeling local and global temporal dynamics. The true correlations between inputs both intra-modality (i.e., RSSI signal among\\naccess points) and inter-modality (i.e., RSSI signal against accelerometer fluctuation) are dynamic. These dynamics can affect one\\nanother within a local context (e.g., cyclical patterns) or across long-term relationships. Can we capture local and global relationships\\nacross different modalities?\\nThe MDCSA architecture addresses the aforementioned challenges through a series of neural network layers, which are described in\\nthe following sections.\\n4.1 Modality Positional Embedding\\nDue to different data dimensionality between RSSI and accelerometer, coupled with the missing temporal information, a linear\\nlayer with a positional encoding is added to transform both RSSI and accelerometer data into their respective embeddings. Suppose\\nwe have a collection of RSSI signals xr = [xr\\n1, xr\\n2, ..., xr\\nT ] ∈ RT×r and accelerometer data xa = [xa\\n1, xa\\n2, ..., xa\\nT ] ∈ RT×a within\\nT time units, where xr\\nt = [xr\\nt1, xr\\nt2, ..., xr\\ntr] represents RSSI signals from r access points, and xa\\nt = [xa\\nt1, xa\\nt2, ..., xa\\nta] represents\\naccelerometer data from a spatial directions at time t with t < T. Given feature vectors xt = [xr\\nt , xa\\nt ] with u ∈ {r, a} representing\\nRSSI or accelerometer data at time t, and t < Trepresenting the time index, a positional embedding hu\\nt for RSSI or accelerometer\\ncan be obtained by:\\nhu\\nt = (Wuxu\\nt + bu) + τt (1)\\nwhere Wu ∈ Ru×d and bu ∈ Rd are the weight and bias to learn, d is the embedding dimension, and τt ∈ Rd is the corresponding\\nposition encoding at time t.\\n4.2 Locality Enhancement with Self-Attention\\nSince it is time-series data, the importance of an RSSI or accelerometer value at each point in time can be identified in relation to its\\nsurrounding values - such as cyclical patterns, trends, or fluctuations. Utilizing historical context that can capture local patterns on\\ntop of point-wise values, performance improvements in attention-based architectures can be achieved. One straightforward option is\\nto utilize a recurrent neural network such as a long-short term memory (LSTM) approach. However, in LSTM layers, the local\\ncontext is summarized based on the previous context and the current input. Two similar patterns separated by a long period of time\\nmight have different contexts if they are processed by the LSTM layers. We utilize a combination of causal convolution layers and\\nself-attention layers, which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input ˆx1 ∈ RN×d\\nand a secondary input ˆx2 ∈ RN×d and yields:\\nDCSA (ˆx1, ˆx2) = GRN(Norm(ϕ(ˆx1) + ˆx1), Norm(ϕ(ˆx2) + ˆx2)) (2)\\nwith\\nϕ(ˆx) = SA(Φk(ˆx)WQ, Φk(ˆx)WK, Φk(ˆx)WV ) (3)\\nwhere GRN(.) is the Gated Residual Network to integrate dual inputs into one integrated embedding, Norm(.) is a standard layer\\nnormalization, SA(.) is a scaled dot-product self-attention, Φk(.) is a 1D-convolutional layer with a kernel size {1, k} and a stride\\nof 1, WK ∈ Rd×d, WQ ∈ Rd×d, WV ∈ Rd×d are weights for keys, queries, and values of the self-attention layer, and d is the\\nembedding dimension. Note that all weights for GRN are shared across each time step t.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 4}, page_content='4.3 Multihead Dual Convolutional Self-Attention\\nOur approach employs a self-attention mechanism to capture global dependencies across time steps. It is embedded as part of the\\nDCSA architecture. Inspired by utilizing multihead self-attention, we utilize our DCSA with various kernel lengths with the same\\naim: allowing asymmetric long-term learning. The multihead DCSA takes in two inputs ˆx1, ˆx2 ∈ RN×d and yields:\\nMDCSA k1,...,kn (ˆx1, ˆx2) = Ξn(ϕk1,...,kn (ˆx1, ˆx2)) (4)\\nwith\\nϕki (ˆx1, ˆx2) = SA(Φki (ˆx1)WQ, Φki (ˆx2)WK, Φki (ˆx1, ˆx2)WV ) (5)\\nwhere Φki (.) is a 1D-convolutional layer with a kernel size {1, ki} and a stride ki, WK ∈ Rd×d, WQ ∈ Rd×d, WV ∈ Rd×d are\\nweights for keys, queries, and values of the self-attention layer, and Ξn(.) concatenates the output of each DCSA ki (.) in temporal\\norder. For regularization, a normalization layer followed by a dropout layer is added after Equation 4.\\nFollowing the modality positional embedding layer in subsection 4.1, the positional embeddings of RSSI hr = [hr\\n1, ..., hr\\nT ] and\\naccelerometer ha = [ha\\n1, ..., ha\\nT ], produced by Eq. 1, are then fed to an MDCSA layer with various kernel sizes [k1, ..., kn]:\\nh = MDCSA k1,...,kn (hr, ha) (6)\\nto yield h = [h1, ..., hT ] with ht ∈ Rd and t < T.\\n4.4 Final Layer and Loss Calculation\\nWe apply two different layers to produce two different outputs during training. The room-level predictions are produced via a single\\nconditional random field (CRF) layer in combination with a linear layer applied to the output of Eq. 7 to produce the final predictions\\nas:\\nˆyt = CRF (ϕ(ht)) (7)\\nq′(ht) = Wpht + bp (8)\\nwhere Wp ∈ Rd×m and bp ∈ Rm are the weight and bias to learn, m is the number of room locations, and h = [h1, ..., hT ] ∈ RT×d\\nis the refined embedding produced by Eq. 7. Even though the transformer can take into account neighbor information before\\ngenerating the refined embedding at time step t, its decision is independent; it does not take into account the actual decision made by\\nother refined embeddings t. We use a CRF layer to cover just that, i.e., to maximize the probability of the refined embeddings of all\\ntime steps, so it can better model cases where refined embeddings closest to one another must be compatible (i.e., minimizing the\\npossibility for impossible room transitions). When finding the best sequence of room location ˆyt, the Viterbi Algorithm is used as a\\nstandard for the CRF layer.\\nFor the second layer, we choose a particular room as a reference and perform a binary classification at each time step t. The binary\\nclassification is produced via a linear layer applied to the refined embedding ht as:\\nˆft = Wf ht + bf (9)\\nwhere Wf ∈ Rd×1 and bf ∈ R are the weight and bias to learn, and ˆf = [ ˆf1, ...,ˆfT ] ∈ RT is the target probabilities for the\\nreferenced room within time window T. The reason to perform a binary classification against a particular room is because of our\\ninterest in improving the accuracy in predicting that room. In our application, the room of our choice is the hallway, where it will be\\nused as a hub connecting any other room.\\n**Loss Functions:** During the training process, the MDCSA network produces two kinds of outputs. Emission outputs (outputs\\nproduced by Equation 9 prior to prediction outputs) ˆe = [ϕ(h1), ..., ϕ(hT )] are trained to generate the likelihood estimate of room\\npredictions, while the binary classification output ˆf = [ ˆf1, ...,ˆfT ] is used to train the probability estimate of a particular room. The\\nfinal loss function can be formulated as a combination of both likelihood and binary cross-entropy loss functions described as:\\nL(ˆe, y,ˆf, f) = LLL(ˆe, y) +\\nTX\\nt=1\\nLBCE ( ˆft, ft) (10)\\nLLL(ˆe, y) =\\nTX\\ni=0\\nP(ϕ(hi))qT\\ni (yi|yi−1) −\\nTX\\ni=0\\nP(ϕ(hi))[qT\\ni (yi|yi−1)] (11)\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 5}, page_content='LBCE ( ˆf, f) = − 1\\nT\\nTX\\nt=0\\nft log( ˆft) + (1− ft) log(1− ˆft) (12)\\nwhere LLL(.) represents the negative log-likelihood and LBCE (.) denotes the binary cross-entropy, y = [y1, ..., yT ] ∈ RT is the\\nactual room locations, and f = [f1, ..., fT ] ∈ RT is the binary value whether at time t the room is the referenced room or not.\\nP(yi|yi−1) denotes the conditional probability, and P(yt|yt−1) denotes the transition matrix cost of having transitioned from yt−1\\nto yt.\\n5 Experiments and Results\\nWe compare our proposed network, MDCSA1,4,7 (MDCSA with 3 kernels of size 1, 4, and 7), with:\\n- Random Forest (RF) as a baseline technique, which has been shown to work well for indoor localization. - A modified transformer\\nencoder in combination with a CRF layer representing a model with the capability to capture global dependency and enforce\\ndependencies in temporal aspects. - A state-of-the-art model for multimodal and multivariate time series with a transformer encoder\\nto learn asymmetric correlations across modalities. - An alternative to the previous model, representing it with a GRN layer replacing\\nthe context aggregation layer and a CRF layer added as the last layer. - MDCSA1,4,7 4APS, as an ablation study, with our proposed\\nnetwork (i.e., MDCSA1,4,7) using 4 access points for the RSSI (instead of 10 access points) and accelerometer data (ACCL) as its\\ninput features. - MDCSA1,4,7 RSSI, as an ablation study, with our proposed network using only RSSI, without ACCL, as its input\\nfeatures. - MDCSA1,4,7 4APS RSSI, as an ablation study, with our proposed network using only 4 access points for the RSSI as its\\ninput features.\\nFor RF, all the time series features of RSSI and accelerometry are flattened and merged into one feature vector for room-level\\nlocalization. For the modified transformer encoder, at each time step t, RSSI xr\\nt and accelerometer xa\\nt features are combined via a\\nlinear layer before they are processed by the networks. A grid search on the parameters of each network is performed to find the best\\nparameter for each model. The parameters to tune are the embedding dimension d in 128, 256, the number of epochs in 200, 300,\\nand the learning rate in 0.01, 0.0001. The dropout rate is set to 0.15, and a specific optimizer in combination with a Look-Ahead\\nalgorithm is used for the training with early stopping using the validation performance. For the RF, we perform a cross-validated\\nparameter search for the number of trees (200, 250), the minimum number of samples in a leaf node (1, 5), and whether a warm start\\nis needed. The Gini impurity is used to measure splits.\\n**Evaluation Metrics:** We are interested in developing a system to monitor PD motor symptoms in home environments. For\\nexample, we will consider if there is any significant difference in the performance of the system when it is trained with PD data\\ncompared to being trained with healthy control (HC) data. We tailored our training procedure to test our hypothesis by performing\\nvariations of cross-validation. Apart from training our models on all HC subjects (ALL-HC), we also perform four different kinds of\\ncross-validation: 1) We train our models on one PD subject (LOO-PD), 2) We train our models on one HC subject (LOO-HC), 3) We\\ntake one HC subject and use only roughly four minutes’ worth of data to train our models (4m-HC), 4) We take one PD subject and\\nuse only roughly four minutes’ worth of data to train our models (4m-PD). For all of our experiments, we test our trained models on\\nall PD subjects (excluding the one used as training data for LOO-PD and 4m-PD). For room-level localization accuracy, we use\\nprecision and weighted F1-score, all averaged and standard deviated across the test folds.\\nTo showcase the importance of in-home gait speed features in differentiating the medication state of a person with PD, we first\\ncompare how accurate the ’Room-to-room Transition’ duration produced by each network is to the ground truth (i.e., annotated\\nlocation). We hypothesize that the more accurate the transition is compared to the ground truth, the better mobility features are for\\nmedication state classification. For the medication state classification, we then compare two different groups of features with two\\nsimple binary classifiers: 1) the baseline demographic features (see Section 3), and 2) the normalized in-home gait speed features.\\nThe metric we use for ON/OFF medication state evaluation is the weighted F1-Score and AUROC, which are averaged and standard\\ndeviated across the test folds.\\n5.1 Experimental Results\\n**Room-level Accuracy:** The first part of Table 1 compares the performance of the MDCSA network and other approaches for\\nroom-level classification. For room-level classification, the MDCSA network outperforms other networks and RF with a minimum\\nimprovement of 1.3% for the F1-score over the second-best network in each cross-validation type, with the exception of the ALL-HC\\nvalidation. The improvement is more significant in the 4m-HC and 4m-PD validations, when the training data are limited, with an\\naverage improvement of almost 9% for the F1-score over the alternative to the state-of-the-art model.\\nThe LOO-HC and LOO-PD validations show that a model that has the ability to capture the temporal dynamics across time steps will\\nperform better than a standard baseline technique such as a Random Forest. The modified transformer encoder and the state-of-the-art\\nmodel perform better in those two validations due to their ability to capture asynchronous relations across modalities. However,\\nwhen the training data becomes limited, as in 4m-HC and 4m-PD validations, having extra capabilities is necessary to further\\nextract temporal information and correlations. Due to being a vanilla transformer requiring a considerable amount of training\\ndata, the modified transformer encoder performs worst in these two validations. The state-of-the-art model performs quite well\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 6}, page_content='due to its ability to capture local context via LSTM for each modality. However, in general, its performance suffers in both the\\nLOO-PD and 4m-PD validations as the accelerometer data (and modality) may be erratic due to PD and should be excluded at\\ntimes from contributing to room classification. The MDCSA network has all the capabilities that the state-of-the-art model has,\\nwith an improvement in suppressing the accelerometer modality when needed via the GRN layer embedded in DCSA. Suppressing\\nthe noisy modality seems to have a strong impact on maintaining the performance of the network when the training data is limited.\\nThis is validated by how the alternative to the state-of-the-art model (i.e., the state-of-the-art model with added GRN and CRF\\nlayers) outperforms the standard state-of-the-art model by an average of 2.2% for the F1-score in the 4m-HC and 4m-PD validations.\\nIt is further confirmed by MDCSA1,4,7 4APS against MDCSA1,4,7 4APS RSSI, with the latter model, which does not include\\nthe accelerometer data, outperforming the former for the F1-score by an average of 1.6% in the last three cross-validations. It is\\nworth pointing out that the MDCSA1,4,7 4APS RSSI model performed the best in the 4m-PD validation. However, the omission of\\naccelerometer data affects the model’s ability to differentiate rooms that are more likely to have active movement (i.e., hall) than the\\nrooms that are not (i.e., living room). It can be seen from Table 2 that the MDCSA1,4,7 4APS RSSI model has low performance in\\npredicting the hallway compared to the full model of MDCSA1,4,7. As a consequence, the MDCSA1,4,7 4APS RSSI model cannot\\nproduce in-home gait speed features as\\naccurately, as shown in Table 3.\\n**Room-to-room Transition and Medication Accuracy:** We hypothesize that during their OFF medication state, the deterioration\\nin mobility of a person with PD is exhibited by how they transition between rooms. To test this hypothesis, a Wilcoxon signed-rank\\ntest was used on the annotated data from PD participants undertaking each of the three individual transitions between rooms whilst\\nON (taking) and OFF (withholding) medications to assess whether the mean transition duration ON medications was statistically\\nsignificantly shorter than the mean transition duration for the same transition OFF medications for all transitions studied (see Table\\n4). From this result, we argue that the mean transition duration obtained by each model from Table 1 that is close to the ground truth\\ncan capture what the ground truth captures. As mentioned in Section 3, this transition duration for each model is generated by the\\nmodel continuously performing room-level localization, focusing on the time a person is predicted to spend in a hallway between\\nrooms. We show, in Table 3, that the mean transition duration for all transitions studied produced by the MDCSA1,4,7 model is the\\nclosest to the ground truth, improving over the second best by around 1.25 seconds across all hall transitions and validations.\\nThe second part of Table 1 shows the performance of all our networks for medication state classification. The demographic\\nfeatures can be used as a baseline for each type of validation. The MDCSA network, with the exception of the ALL-HC validation,\\noutperforms any other network by a significant margin for the AUROC score. By using in-home gait speed features produced by\\nthe MDCSA network, a minimum of 15% improvement over the baseline demographic features can be obtained, with the biggest\\ngain obtained in the 4m-PD validation data. In the 4m-PD validation data, RF, TENER, and DTML could not manage to provide\\nany prediction due to their inability to capture (partly) hall transitions. Furthermore, TENER has shown its inability to provide any\\nmedication state prediction from the 4m-HC data validations. It can be validated by Table 3 when TENER failed to capture any\\ntransitions between the dining room and living room across all periods that have ground truths. MDCSA networks can provide\\nmedication state prediction and maintain their performance across all cross-validations thanks to the addition of Eq. 13 in the loss\\nfunction.\\n**Limitations and future research:** One limitation of this study is the relatively small sample size (which was planned as this is\\nan exploratory pilot study). We believe our sample size is ample to show proof of concept. This is also the first such work with\\nunobtrusive ground truth validation from embedded cameras. Future work should validate our approach further on a large cohort\\nof people with PD and consider stratifying for sub-groups within PD (e.g., akinetic-rigid or tremor-dominant phenotypes), which\\nwould also increase the generalizability of the results to the wider population. Future work in this matter could also include the\\nconstruction of a semi-synthetic dataset based on collected data to facilitate a parallel and large-scale evaluation.\\nThis smart home’s layout and parameters remain constant for all the participants, and we acknowledge that the transfer of this deep\\nlearning model to other varied home settings may introduce variations in localization accuracy. For future ecological validation and\\nbased on our current results, we anticipate the need for pre-training (e.g., a brief walkaround which is labeled) for each home, and\\nalso suggest that some small amount of ground-truth data will need to be collected (e.g., researcher prompting of study participants to\\nundertake scripted activities such as moving from room to room) to fully validate the performance of our approach in other settings.\\n6 Conclusion\\nWe have presented the MDCSA model, a new deep learning approach for indoor localization utilizing RSSI and wrist-worn\\naccelerometer data. The evaluation on our unique real-world free-living pilot dataset, which includes subjects with and without PD,\\nshows that MDCSA achieves state-of-the-art accuracy for indoor localization. The availability of accelerometer data does indeed\\nenrich the RSSI features, which, in turn, improves the accuracy of indoor localization.\\nAccurate room localization using these data modalities has a wide range of potential applications within healthcare. This could\\ninclude tracking of gait speed during rehabilitation from orthopedic surgery, monitoring wandering behavior in dementia, or\\ntriggering an alert for a possible fall (and long lie on the floor) if someone is in one room for an unusual length of time. Furthermore,\\naccurate room use and room-to-room transfer statistics could be used in occupational settings, e.g., to check factory worker location.\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 7}, page_content='Table 1: Room-level and medication state accuracy of all models. Standard deviation is shown in (.), the best performer is bold,\\nwhile the second best is italicized. Note that our proposed model is the one named MDCSA1,4,7\\n!\\nTraining Model Room-Level Localisation Medication State\\nPrecision F1-Score F1-Score AUROC\\nALL-HC\\nRF 95.00 95.20 56.67 (17.32) 84.55 (12.06)\\nTENER 94.60 94.80 47.08 (16.35) 67.74 (10.82)\\nDTML 94.80 94.90 50.33 (13.06) 75.97 (9.12)\\nAlt DTML 94.80 95.00 47.25 (5.50) 75.63 (4.49)\\nMDCSA1,4,7 4APS 92.22 92.22 53.47 (12.63) 73.48 (6.18)\\nMDCSA1,4,7 RSSI 94.70 94.90 51.14 (11.95) 68.33 (18.49)\\nMDCSA1,4,7 4APS RSSI 93.30 93.10 64.52 (11.44) 81.84 (6.30)\\nMDCSA1,4,7 94.90 95.10 64.13 (6.05) 80.95 (10.71)\\nDemographic Features 49.74 (15.60) 65.66 (18.54)\\nLOO-HC\\nRF 89.67 (1.85) 88.95 (2.61) 54.74 (11.46) 69.24 (17.77)\\nTENER 90.35 (1.87) 89.75 (2.24) 51.76 (14.37) 70.80 (9.78)\\nDTML 90.51 (1.95) 89.82 (2.60) 55.34 (13.67) 73.77 (9.84)\\nAlt DTML 90.52 (2.17) 89.71 (2.83) 49.56 (17.26) 73.26 (10.65)\\nMDCSA1,4,7 4APS 88.01 (6.92) 88.08 (5.73) 59.52 (20.62) 74.35 (16.78)\\nMDCSA1,4,7 RSSI 90.26 (2.43) 89.48 (3.47) 58.84 (23.08) 76.10 (10.84)\\nMDCSA1,4,7 4APS RSSI 88.55 (6.67) 88.75 (5.50) 42.34 (13.11) 72.58 (6.77)\\nMDCSA1,4,7 91.39 (2.13) 91.06 (2.62) 55.50 (15.78) 83.98 (13.45)\\nDemographic Features 51.79 (15.40) 68.33 (18.43)\\nLOO-PD\\nRF 86.89 (7.14) 84.71 (7.33) 43.28 (14.02) 62.63 (20.63)\\nTENER 86.91 (6.76) 86.18 (6.01) 36.04 (9.99) 60.03 (10.52)\\nDTML 87.13 (6.53) 86.31 (6.32) 43.98 (14.06) 66.93 (11.07)\\nAlt DTML 87.36 (6.30) 86.44 (6.63) 44.02 (16.89) 69.70 (12.04)\\nMDCSA1,4,7 4APS 86.44 (6.96) 85.93 (6.05) 47.26 (14.47) 72.62 (11.16)\\nMDCSA1,4,7 RSSI 87.61 (6.64) 87.21 (5.44) 45.71 (17.85) 67.76 (10.73)\\nMDCSA1,4,7 4APS RSSI 87.20 (7.17) 87.00 (6.12) 41.33 (17.72) 66.26 (12.11)\\nMDCSA1,4,7 88.04 (6.94) 87.82 (6.01) 49.99 (13.18) 81.08 (8.46)\\nDemographic Features 43.89 (14.43) 60.95 (25.16)\\n4m-HC\\nRF 74.27 (8.99) 69.87 (7.21) 50.47 (12.63) 59.55 (12.38)\\nTENER 69.86 (18.68) 60.71 (24.94) N/A N/A\\nDTML 77.10 (9.89) 70.12 (14.26) 43.89 (11.60) 64.67 (12.88)\\nAlt DTML 78.79 (3.95) 71.44 (9.82) 47.49 (14.64) 65.16 (12.56)\\nMDCSA1,4,7 4APS 81.42 (6.95) 78.65 (7.59) 42.87 (17.34) 67.09 (7.42)\\nMDCSA1,4,7 RSSI 81.69 (6.85) 77.12 (8.46) 49.95 (17.35) 69.71 (11.55)\\nMDCSA1,4,7 4APS RSSI 82.80 (7.82) 79.37 (8.98) 43.57 (23.87) 65.46 (15.78)\\nMDCSA1,4,7 83.32 (6.65) 80.24 (6.85) 55.43 (10.48) 78.24 (6.67)\\nDemographic Features 32.87 (13.81) 53.68 (13.86)\\n4m-PD\\nRF 71.00 (9.67) 65.89 (11.96) N/A N/A\\nTENER 65.30 (23.25) 58.57 (27.19) N/A N/A\\nDTML 70.35 (14.17) 64.00 (17.88) N/A N/A\\nAlt DTML 74.43 (9.59) 67.55 (14.50) N/A N/A\\nMDCSA1,4,7 4APS 81.02 (8.48) 76.85 (10.94) 49.97 (7.80) 69.10 (7.64)\\nMDCSA1,4,7 RSSI 77.47 (12.54) 73.99 (13.00) 41.79 (16.82) 67.37 (16.86)\\nMDCSA1,4,7 4APS RSSI 83.01 (6.42) 79.77 (7.05) 41.18 (12.43) 63.16 (11.06)\\nMDCSA1,4,7 83.30 (6.73) 76.77 (13.19) 48.61 (12.03) 76.39 (12.23)\\nDemographic Features 36.69 (18.15) 50.53 (15.60)\\nIn naturalistic settings, in-home mobility can be measured through the use of indoor localization models. We have shown, using\\nroom transition duration results, that our PD cohort takes longer on average to perform a room transition when they withhold\\nmedications. With accurate in-home gait speed features, a classifier model can then differentiate accurately if a person with PD is in\\nan ON or OFF medication state. Such changes show the promise of these localization outputs to detect the dopamine-related gait\\nfluctuations in PD that impact patients’ quality of life and are important in clinical decision-making. We have also demonstrated\\nthat our indoor localization system provides precise in-home gait speed features in PD with a minimal average offset to the ground\\n8'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 8}, page_content='Table 2: Hallway prediction on limited training data.\\nTraining Model Precision F1-Score\\n4m-HC\\nMDCSA 4APS RSSI 62.32 (19.72) 58.99 (23.87)\\nMDCSA 4APS 68.07 (23.22) 60.01 (26.24)\\nMDCSA 71.25 (21.92) 68.95 (17.89)\\n4m-PD\\nMDCSA 4APS RSSI 58.59 (23.60) 57.68 (24.27)\\nMDCSA 4APS 62.36 (18.98) 57.76 (20.07)\\nMDCSA 70.47 (14.10) 64.64 (21.38)\\nTable 3: Room-to-room transition accuracy (in seconds) of all models compared to the ground truth. Standard deviation is shown in\\n(.), the best performer is bold, while the second best is italicized. A model that fails to capture a transition between particular rooms\\nwithin a period that has the ground truth is assigned ’N/A’ score.\\n!\\nData Models Kitch-Livin Kitch-Dinin Dinin-Livin\\nGround Truth 18.71 (18.52) 14.65 (6.03) 10.64 (11.99)\\nALL-HC\\nRF 16.18 (12.08) 14.58 (10.22) 10.19 (9.46)\\nTENER 15.58 (8.75) 16.30 (12.94) 12.01 (13.01)\\nAlt DTML 15.27 (7.51) 13.40 (6.43) 10.84 (10.81)\\nMDCSA 17.70 (16.17) 14.94 (9.71) 10.76 (9.59)\\nLOO-HC\\nRF 17.52 (16.97) 11.93 (10.08) 9.23 (13.69)\\nTENER 14.62 (16.37) 9.58 (9.16) 7.21 (10.61)\\nAlt DTML 16.30 (17.78) 14.01 (8.08) 10.37 (12.44)\\nMDCSA 17.70 (17.42) 14.34 (9.48) 11.07 (13.60)\\nLOO-PD\\nRF 14.49 (15.28) 11.67 (11.68) 8.65 (13.06)\\nTENER 13.42 (14.88) 10.87 (10.37) 6.95 (10.28)\\nAlt DTML 16.98 (15.15) 15.26 (8.85) 9.99 (13.03)\\nMDCSA 16.42 (14.04) 14.48 (9.81) 10.77 (14.18)\\n4m-HC\\nRF 14.22 (18.03) 11.38 (15.46) 13.43 (18.87)\\nTENER 10.75 (15.67) 8.59 (14.39) N/A\\nAlt DTML 16.89 (18.07) 14.68 (13.57) 9.31 (15.70)\\nMDCSA 18.15 (19.12) 15.32 (14.93) 11.89 (17.55)\\n4m-PD\\nRF 11.52 (16.07) 8.73 (12.90) N/A\\nTENER 8.75 (14.89) N/A N/A\\nAlt DTML 14.75 (13.79) 13.47 (17.66) N/A\\nMDCSA 17.96 (19.17) 14.74 (10.83) 10.16 (14.03)\\ntruth. The network also outperforms other models in the production of in-home gait speed features, which is used to differentiate the\\nmedication state of a person with PD.\\nAcknowledgments\\nWe are very grateful to the study participants for giving so much time and effort to this research. We acknowledge the local\\nMovement Disorders Health Integration Team (Patient and Public Involvement Group) for their assistance at each study design step.\\nThis work was supported by various grants and institutions.\\nStatistical Significance Test\\nIt could be argued that all the localization models compared in Table 1 might not be statistically different due to the fairly high\\nstandard deviation across all types of cross-validations, which is caused by the relatively small number of participants. In order to\\ncompare multiple models over cross-validation sets and show the statistical significance of our proposed model, we perform the\\nFriedman test to first reject the null hypothesis. We then performed a pairwise statistical comparison: the Wilcoxon signed-rank test\\nwith Holm’s alpha correction.\\n9'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 9}, page_content='Table 4: PD participant room transition duration with ON and OFF medications comparison using Wilcoxon signed rank tests.\\nOFF transitions Mean transition duration ON transitions Mean transition duration W z\\nKitchen-Living OFF 17.2 sec Kitchen-Living ON 14.0 sec 75.0 2.824\\nDining-Kitchen OFF 12.9 sec Dining-Kitchen ON 9.2 sec 76.0 2.903\\nDining-Living OFF 10.4 sec Dining-Living ON 9.0 sec 64.0 1.961\\n10')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 0}, page_content='Safe Predictors for Input-Output Specification\\nEnforcement\\nAbstract\\nThis paper presents an approach for designing neural networks, along with other\\nmachine learning models, which adhere to a collection of input-output specifica-\\ntions. Our method involves the construction of a constrained predictor for each set\\nof compatible constraints, and combining these predictors in a safe manner using a\\nconvex combination of their predictions. We demonstrate the applicability of this\\nmethod with synthetic datasets and on an aircraft collision avoidance problem.\\n1 Introduction\\nThe increasing adoption of machine learning models, such as neural networks, in safety-critical\\napplications, such as autonomous vehicles and aircraft collision avoidance, highlights an urgent\\nneed for the development of guarantees on safety and robustness. These models may be required\\nto satisfy specific input-output specifications to ensure the algorithms comply with physical laws,\\ncan be executed safely, and are consistent with prior domain knowledge. Furthermore, these models\\nshould demonstrate adversarial robustness, meaning their outputs should not change abruptly within\\nsmall input regions – a property that neural networks often fail to satisfy.\\nRecent studies have shown the capacity to verify formally input-output specifications and adversarial\\nrobustness properties of neural networks. For instance, the Satisability Modulo Theory (SMT) solver\\nReluplex was employed to verify properties of networks being used in the Next-Generation Aircraft\\nCollision Avoidance System for Unmanned aircraft (ACAS Xu). Reluplex has also been used to\\nverify adversarial robustness. While Reluplex and other similar techniques can effectively determine\\nif a network satisfies a given specification, they do not offer a way to guarantee that the network will\\nmeet those specifications. Therefore, additional methods are needed to adjust networks if it is found\\nthat they are not meeting the desired properties.\\nThere has been an increase in techniques for designing networks with certified adversarial robustness,\\nbut enforcing more general safety properties in neural networks is still largely unexplored. One ap-\\nproach to achieving provably correct neural networks is through abstraction-refinement optimization.\\nThis approach has been applied to the ACAS-Xu dataset, but the network was not guaranteed to meet\\nthe specifications until after training. Our work seeks to design networks with enforced input-output\\nconstraints even before training has been completed. This will allow for online learning scenarios\\nwhere a system has to guarantee safety throughout its operation.\\nThis paper presents an approach for designing a safe predictor (a neural network or any other\\nmachine learning model) that will always meet a set of constraints on the input-output relationship.\\nThis assumes that the constrained output regions can be formulated to be convex. Our correct-\\nby-construction safe predictor is guaranteed to satisfy the constraints, even before training, and at\\nevery training step. We describe our approach in Section 2, and show its use in an aircraft collision\\navoidance problem in Section 3. Results on synthetic datasets can be found in Appendix B.\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 1}, page_content='2 Method\\nConsidering two normed vector spaces, an input space X and an output space Y , and a collection\\nof c different pairs of input-output constraints, (Ai, Bi), where Ai ⊆ X and Bi is a convex subset\\nof Y for each constraint i, the goal is to design a safe predictor, F : X → Y , that guarantees\\nx ∈ Ai ⇒ F(x) ∈ Bi.\\nLet b be a bit-string of length c. Define Ob as the set of points z such that, for all i, bi = 1 implies\\nz ∈ Ai, and bi = 0 implies z /∈ Ai. Ob thus represents the overlap regions for each combination of\\ninput constraints. For example, O101 is the set of points in A1 and A3, but not in A2, and O0...0 is\\nthe set where no input constraints apply. We also define O as the set of bit strings, b, such that Ob\\nis non-empty, and define k = |O|. The sets {Ob : b ∈ O} create a partition of X according to the\\ncombination of input constraints that apply.\\nGiven:\\n• c different input constraint proximity functions, σi : X → [0, 1], where σi is continuous and\\n∀x ∈ Ai, σi(x) = 0,\\n• k different constrained predictors, Gb : X → Bb, one for each b ∈ O, such that the domain\\nof each Gb is non-empty,\\nWe define:\\n• a set of weighting functions, wb(x) =\\nQ\\ni:bi=1(1−σi(x)) Q\\ni:bi=0 σi(x)P\\nb∈O\\nQ\\ni:bi=1(1−σi(x)) Q\\ni:bi=0 σi(x) , where\\nP\\nb∈O wb(x) = 1, and\\n• a safe predictor, F(x) = P\\nb∈O wb(x)Gb(x).\\nTheorem 2.1. For all i, if x ∈ Ai, then F(x) ∈ Bi.\\nA formal proof of Theorem 2.1 is presented in Appendix A and can be summarized as: if an input is\\nin Ai, then by construction of the proximity and weighting functions, all of the constrained predictors,\\nGb, that do not map to Bi will be given zero weight. Only the constrained predictors that map to\\nBi will be given non-zero weight, and because of the convexity ofBi, the weighted average of the\\npredictions will remain in Bi.\\nIf all Gb are continuous and if there are no two input sets, Ai and Aj, for which (Ai ∩ Aj) ⊂\\n(∂Ai ∪∂Aj), then F will be continuous. In the worst case, as the number of constraints grows linearly,\\nthe number of constrained predictors needed to describe our safe predictor grows exponentially. In\\npractice, however, we expect many of the constraint overlap sets,Ob, to be empty. Consequently, any\\npredictors corresponding to an empty set can be ignored. This significantly reduces the number of\\nconstrained predictors needed for many applications.\\nSee Figure 1 for an illustrative example of how to constructF(x) for a notional problem with two\\noverlapping input-output constraints.\\n2.1 Proximity Functions\\nThe proximity functions, σi, describe how close an input, x, is to a particular input constraint region,\\nAi. These functions are used to compute the weights of the constrained predictors. A desirable\\nproperty for σi is for σi(x) → 1 as d(x, Ai) → ∞, for some distance function. This ensures that\\nwhen an input is far from a constraint region, that constraint has little influence on the prediction for\\nthat input. A natural choice for such a function is:\\nσi(x; Σi) = 1 − exp\\n\\x12\\n−d(x, Ai)\\nσ1\\n\\x13σ2\\n.\\nHere, Σi is a set of parameters σ1 ∈ (0, ∞) and σ2 ∈ (1, ∞), which can be specified based on\\nengineering judgment, or learned using optimization over training data. In our experiments in\\nthis paper, we use proximity functions of this form and learn independent parameters for each\\ninput-constrained region. We plan to explore other choices for proximity functions in future work.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 2}, page_content='2.2 Learning\\nIf we have families of differentiable functions Gb(x; θb), continuously parameterized by θb, and\\nfamilies of σi(x; χi), differentiable and continuously parameterized by χi, then F(x; Θ, X), where\\nΘ = {θb : b ∈ O} and X = {χi : i = 1, ..., c}, is also continuously parameterized and differentiable.\\nWe can thus apply standard optimization techniques (e.g., gradient descent) to find parameters of F\\nthat minimize a loss function on some dataset, while also preserving the desired safety properties.\\nNote that the safety guarantee holds regardless of the parameters. To create each Gb(x; θb) we\\nconsider choosing:\\n• a latent space Rm,\\n• a map hb : Rm → Bb,\\n• a standard neural network architecture gb : X → Rm,\\nand then defining Gb(x; θb) = hb(gb(x; θb)).\\nThe framework proposed here does not require an entirely separate network for each b. In many\\napplications, it may be advantageous for the constrained predictors to share earlier layers, thus\\ncreating a shared representation of the input space. In addition, our definition of the safe predictor is\\ngeneral and is not limited to neural networks.\\nIn Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D\\nwith simple neural networks. These examples show that our safe predictor can enforce arbitrary\\ninput-output specifications using convex output constraints on neural networks, and that the learned\\nfunction is smooth.\\n3 Application to Aircraft Collision Avoidance\\nAircraft collision avoidance requires robust safety guarantees. The Next-Generation Collision\\nAvoidance System (ACAS X), which issues advisories to prevent near mid-air collisions, has both\\nmanned (ACAS Xa) and unmanned (ACAS Xu) variants. The system was originally designed to\\nchoose optimal advisories while minimizing disruptive alerts by solving a partially observable Markov\\ndecision process. The solution took the form of a large look-up table, mapping each possible input\\ncombination to scores for all possible advisories. The advisory with the highest score would then be\\nissued. By using a deep neural network (DNN) to compress the policy tables, it has been necessary to\\nverify that the DNNs meet certain safety specifications.\\nA desirable ˘201csafeability˘201d property for ACAS X was defined in a previous work. This property\\nspeci01ed that for any given input state within the ˘201csafeable region,˘201d an advisory would never\\nbe issued that could put the aircraft into a state where a safe advisory would no longer exist. This\\nconcept is similar to control invariance. A simplified model of the ACAS Xa system was created,\\nnamed VerticalCAS. DNNs were then generated to approximate the learned policy, and Reluplex was\\nused to verify whether the DNNs satisfied the safeability property. This work found thousands of\\ncounterexamples where the DNNs did not meet the criteria.\\nOur approach for designing a safe predictor ensures any collision avoidance system will meet the\\nsafeability property by construction. Appendix C describes in detail how we apply our approach to\\na subset of the VerticalCAS datasets using a conservative, convex approximation of the safeability\\nconstraints. These constraints are defined such that if an aircraft state is in the \"unsafeable region\",\\nAunsafeable,i, for the ith advisory, the score for that advisory must not be the highest, i.e., x ∈\\nAunsafeable,i ⇒ Fi(x) < maxj Fj(x), where Fj(x) is the output score for the jth advisory.\\nTable 1 shows the performance of a standard, unconstrained network and our safe predictor. For both\\nnetworks, we present the percentage accuracy (ACC) and violations (percentage of inputs for which\\nthe network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets,\\nbased on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500).\\nAs shown in the table, our safe predictor adheres to the required safeability property. Furthermore,\\nthe accuracy of our predictor remains the same as the unconstrained network, demonstrating we are\\nnot losing accuracy to achieve safety guarantees.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 3}, page_content='Table 1: Results of the best configurations of β-TCV AE on DCI, FactorV AE, SAP, MIG, and IRS\\nmetrics.\\nNETWORK ACC (COC) VIOLATIONS (COC) ACC (CL1500) VIOLATIONS (CL1500)\\nSTANDARD 96.87 0.22 93.89 0.20\\nSAFE 96.69 0.00 94.78 0.00\\n4 Discussion and Future Work\\nWe propose an approach for designing a safe predictor that adheres to input-output specifications for\\nuse in safety-critical machine learning systems, demonstrating it on an aircraft collision avoidance\\nproblem. The novelty of our approach is its simplicity and guaranteed enforcement of specifications\\nthrough combinations of convex output constraints during all stages of training. Future work includes\\nadapting and using techniques from optimization and control barrier functions, as well as incorporating\\nnotions of adversarial robustness into our design, such as extending the work to bound the Lipschitz\\nconstant of our networks.\\nAppendix A: Proof of Theorem 2.1\\nProof. Fix i and assume that x ∈ Ai. It follows that σi(x) = 0 , so for all b ∈ O where bi = 0,\\nwb(x) = 0. Thus,\\nF(x) =\\nX\\nb∈O,bi=1\\nwb(x)Gb(x).\\nIf bi = 1, Gb(x) ∈ Bi, and thus F(x) is also in Bi by the convexity of Bi.\\nAppendix B: Example on Synthetic Datasets\\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem. This\\nexample uses inputs and outputs in 1-D with one input-output constraint. The unconstrained network\\nconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.\\nThe safe predictor shares this structure with the unconstrained network but has its own fully connected\\nlayer for the constrained predictors, G0 and G1. Training uses a sampled subset of points from\\nthe input space. Figure 3 shows an example of applying our safe predictor to a notional regression\\nproblem with a 2-D input and 1-D output, using two overlapping constraints. The unconstrained\\nnetwork has two hidden layers of dimension 20 and ReLU activations, followed by a fully connected\\nlayer. The constrained predictors, G00, G10, G01, and G11, share the hidden layers but also have an\\nadditional hidden layer of size 20 with ReLU, followed by a fully connected layer. Training uses a\\nsampled subset of points from the input space.\\nAppendix C: Details of VerticalCAS Experiment\\nC.1 Safeability Constraints\\nThe \"safeability\" property, originally introduced and used to verify the safety of the VerticalCAS\\nneural networks can be encoded into a set of input-output constraints. The \"safeable region\" for\\na given advisory represents input locations where that advisory can be selected such that future\\nadvisories exist that will prevent an NMAC. If no future advisories exist, the advisory is \"unsafeable\"\\nand the corresponding input region is the \"unsafeable region\". Examples of these regions, and their\\nproximity functions are shown in Figure 5 for the CL1500 advisory.\\nThe constraints we enforce are that x ∈ Aunsafeable,i ⇒ Fi(x) < maxj Fj(x), ∀i, where Aunsafeable,i is\\nthe unsafeable region for the ith advisory, and Fj(x) is the output score for the jth advisory. Because\\nthe output regions of the safeable constraints are not convex, we make a conservative approximation,\\nenforcing Fi(x) = minj Fj(x), for all x ∈ Aunsafeable,i.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 4}, page_content='C.2 Proximity Functions\\nWe start by generating the unsafeable region bounds from the open source code. We then compute a\\n\"distance function\" between input space points (vO - vI, h, τ), and the unsafeable region for each\\nadvisory. These are not true distances but are 0 if and only if the data point is within the unsafeable\\nset. These are then used to produce proximity functions as given in Equation 1.\\nC.3 Structure of Predictors\\nThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden\\nlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for the\\nunconstrained network. For our constrained predictors, we use the same structure but have shared\\nfirst four layers for all predictors. This provides a common learned representation of the input space,\\nwhile allowing each predictor to adapt to its own constraints. After the shared layers, each constrained\\npredictor has an additional two hidden layers and their final outputs are projected onto our convex\\napproximation of the safe region of the output space, usingGb(x) = minj Gj(x). In our experiments,\\nwe set ϵ = 0.0001.\\nWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeability\\nconstraints. The number of nodes for the unconstrained and safe implementations were 270 and 2880,\\nrespectively. Our safe predictor is orders of magnitude smaller than the original look-up tables.\\nC.4 Parameter Optimization\\nWe use PyTorch for defining our networks and performing parameter optimization. We optimize both\\nthe unconstrained and safe predictors using the asymmetric loss function to select advisories while\\nalso accurately predicting scores. The data is split using an 80/20 train/test split with a random seed\\nof 0. The optimizer is ADAM with a learning rate of 0.0003 and batch size of 216, with training for\\n500 epochs.\\nAppendix A: Proof of Theorem 2.1\\nProof. Let x ∈ Ai. Then, σi(x) = 0, and for all b ∈ O where bi = 0, wb(x) = 0. Thus,\\nF(x) =\\nX\\nb∈O,bi=1\\nwb(x)Gb(x)\\nIf bi = 1, then Gb(x) ∈ Bi, and therefore F(x) is in Bi due to the convexity of Bi.\\nAppendix B: Example on Synthetic Datasets\\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem with 1-D\\ninput and outputs, and one input-output constraint. The unconstrained network has a single hidden\\nlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictor\\nshares this structure with constrained predictors, G0 and G1, but each predictor has its own fully\\nconnected layer. The training uses a sampled subset of points from the input space and the learned\\npredictors are shown for the continuous input space.\\nFigure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D\\ninput and 1-D output and two overlapping constraints. The unconstrained network has two hidden\\nlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrained\\npredictors G00, G10, G01 and G11 share the hidden layers and have an additional hidden layer of size\\n20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of points\\nfrom the input space and the learned predictors are shown for the continuous input space.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 5}, page_content='Appendix C: Details of VerticalCAS Experiment\\nC.1 Safeability Constraints\\nThe “safeability” property from prior work can be encoded into a set of input-output constraints. The\\n“safeable region” for a given advisory is the set of input space locations where that advisory can be\\nchosen, for which future advisories exist that will prevent an NMAC. If no future advisories exist for\\npreventing an NMAC, the advisory is deemed “unsafeable,” and the corresponding input region is the\\n“unsafeable region.” Figure 5 shows an example of these regions for the CL1500 advisory.\\nThe constraints we enforce in our safe predictor are: x ∈ Aunsafeable,i ⇒ Fi(x) < maxj Fj(x),\\n∀i. To make the output regions convex, we approximate by enforcingFi(x) = minj Fj(x), for all\\nx ∈ Aunsafeable,i.\\nC.2 Proximity Functions\\nWe start by generating the bounds on the unsafeable regions. Then, a distance function is computed\\nbetween points in the input space (vO − vI, h, τ), and the unsafeable region for each advisory. While\\nthese are not true distances, their values are 0 if and only if the data point is inside the unsafeable set.\\nWhen used to produce proximity functions as given in Equation 1, these values help ensure safety.\\nFigure 5 shows examples of the unsafeable region, distance function, and proximity function for the\\nCL1500 advisory.\\nC.3 Structure of Predictors\\nThe compressed versions of the policy tables from prior work are neural networks with six hidden\\nlayers, 45 dimensions in each layer, and ReLU activation functions. We use the same architecture\\nfor our standard, unconstrained network. For constrained predictors, we use a similar architecture.\\nHowever, the first four hidden layers are shared between all of the predictors. This learns a single,\\nshared input space representation, and also allows each predictor to adapt to its constraints. Each\\nconstrained predictor has two additional hidden layers and their outputs are projected onto our convex\\napproximation of the safe output region. We accomplish this by setting the score for any unsafeable\\nadvisory i to Gi(x) = minj Gj(x) − ϵ. In our experiments, we used ϵ = 0.0001.\\nTo enforce the VerticalCAS safeability constraints, we need 30 separate predictors. This increases\\nthe size of the network from 270 to 2880 nodes for the unconstrained and safe implementations\\nrespectively. However, our safe predictor remains smaller than the original look-up tables by several\\norders of magnitude.\\nC.4 Parameter Optimization\\nWe define our networks and perform parameter optimization using PyTorch. We optimize the\\nparameters of both the unconstrained network and our safe predictor using the asymmetric loss\\nfunction, guiding the network to select optimal advisories while accurately predicting scores from\\nthe look-up tables. Each dataset is split using an 80/20 train/test split, with a random seed of 0. The\\noptimizer is ADAM, with a learning rate of 0.0003, a batch size of 216, and the number of training\\nepochs is 500.\\n6')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 0}, page_content='Generalization in ReLU Networks via Restricted\\nIsometry and Norm Concentration\\nAbstract\\nRegression tasks, while aiming to model relationships across the entire input space,\\nare often constrained by limited training data. Nevertheless, if the hypothesis func-\\ntions can be represented effectively by the data, there is potential for identifying a\\nmodel that generalizes well. This paper introduces the Neural Restricted Isometry\\nProperty (NeuRIPs), which acts as a uniform concentration event that ensures all\\nshallow ReLU networks are sketched with comparable quality. To determine the\\nsample complexity necessary to achieve NeuRIPs, we bound the covering numbers\\nof the networks using the Sub-Gaussian metric and apply chaining techniques. As-\\nsuming the NeuRIPs event, we then provide bounds on the expected risk, applicable\\nto networks within any sublevel set of the empirical risk. Our results show that all\\nnetworks with sufficiently small empirical risk achieve uniform generalization.\\n1 Introduction\\nA fundamental requirement of any scientific model is a clear evaluation of its limitations. In recent\\nyears, supervised machine learning has seen the development of tools for automated model discovery\\nfrom training data. However, these methods often lack a robust theoretical framework to estimate\\nmodel limitations. Statistical learning theory quantifies the limitation of a trained model by the\\ngeneralization error. This theory uses concepts such as the VC-dimension and Rademacher complexity\\nto analyze generalization error bounds for classification problems. While these traditional complexity\\nnotions have been successful in classification problems, they do not apply to generic regression\\nproblems with unbounded risk functions, which are the focus of this study. Moreover, traditional\\ntools in statistical learning theory have not been able to provide a fully satisfying generalization\\ntheory for neural networks.\\nUnderstanding the risk surface during neural network training is crucial for establishing a strong\\ntheoretical foundation for neural network-based machine learning, particularly for understanding\\ngeneralization. Recent studies on neural networks suggest intriguing properties of the risk surface.\\nIn large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,\\nglobal minima exist in each connected component of the risk’s sublevel set and are path-connected.\\nIn this work, we contribute to a generalization theory for shallow ReLU networks, by giving uniform\\ngeneralization error bounds within the empirical risk’s sublevel set. We use methods from the analysis\\nof convex linear regression, where generalization bounds for empirical risk minimizers are derived\\nfrom recent advancements in stochastic processes’ chaining theory. Empirical risk minimization\\nfor non-convex hypothesis functions cannot generally be solved efficiently. However, under certain\\nassumptions, it is still possible to derive generalization error bounds, as we demonstrate in this paper\\nfor shallow ReLU networks. Existing works have applied methods from compressed sensing to\\nbound generalization errors for arbitrary hypothesis functions. However, they do not capture the\\nrisk’s stochastic nature through the more advanced chaining theory.\\nThis paper is organized as follows. We begin in Section II by outlining our assumptions about the\\nparameters of shallow ReLU networks and the data distribution to be interpolated. The expected and\\nempirical risk are introduced in Section III, where we define the Neural Restricted Isometry Property\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 1}, page_content='(NeuRIPs) as a uniform norm concentration event. We present a bound on the sample complexity for\\nachieving NeuRIPs in Theorem 1, which depends on both the network architecture and parameter\\nassumptions. We provide upper bounds on the generalization error that are uniformly applicable\\nacross the sublevel sets of the empirical risk in Section IV . We prove this property in a network\\nrecovery setting in Theorem 2, and also an agnostic learning setting in Theorem 3. These results\\nensure a small generalization error, when any optimization algorithm finds a network with a small\\nempirical risk. We develop the key proof techniques for deriving the sample complexity of achieving\\nNeuRIPs in Section V , by using the chaining theory of stochastic processes. The derived results are\\nsummarized in Section VI, where we also explore potential future research directions.\\n2 Notation and Assumptions\\nIn this section, we will define the key notations and assumptions for the neural networks examined\\nin this study. A Rectified Linear Unit (ReLU) function ϕ : R → R is given by ϕ(x) := max(x, 0).\\nGiven a weight vector w ∈ Rd, a bias b ∈ R, and a sign κ ∈ {±1}, a ReLU neuron is a function\\nϕ(w, b, κ) : Rd → R defined as\\nϕ(w, b, κ)(x) = κϕ(wT x + b).\\nShallow neural networks are constructed as weighted sums of neurons. Typically they are represented\\nby a graph with n neurons in a single hidden layer. When using the ReLU activation function, we can\\napply a symmetry procedure to represent these as sums:\\n¯ϕ¯p(x) =\\nnX\\ni=0\\nϕpi (x),\\nwhere ¯p is the tuple (p1, . . . , pn).\\nAssumption 1. The parameters ¯p, which index shallow ReLU networks, are drawn from a set\\n¯P ⊆ (Rd × R × {±1})n.\\nFor ¯P, we assume there exist constants cw ≥ 0 and cb ∈ [1, 3], such that for all parameter tuples\\n¯p = {(w1, b1, κ1), . . . ,(wn, bn, κn)} ∈¯P, we have\\n∥wi∥ ≤cw and |bi| ≤cb.\\nWe denote the set of shallow networks indexed by a parameter set ¯P by\\nΦ ¯P := {ϕ¯p : ¯p ∈ ¯P}.\\nWe now equip the input spaceRd of the networks with a probability distribution. This distribution\\nreflects the sampling process and makes each neural network a random variable. Additionally, a\\nrandom label y takes its values in the output space R, for which we assume the following.\\nAssumption 2. The random sample x ∈ Rd and label y ∈ R follow a joint distribution µ such that\\nthe marginal distribution µx of sample x is standard Gaussian with density\\n1\\n(2π)d/2 exp\\n\\x12\\n−∥x∥2\\n2\\n\\x13\\n.\\nAs available data, we assume independent copies {(xj, yj)}m\\nj=1 of the random pair (x, y), each\\ndistributed by µ.\\n3 Concentration of the Empirical Norm\\nSupervised learning algorithms interpolate labels y for samples x, both distributed jointly by µ on\\nX × Y. This task is often solved under limited data accessibility. The training data, respecting\\nAssumption 2, consists of m independent copies of the random pair (x, y). During training, the\\ninterpolation quality of a hypothesis function f : X → Ycan only be assessed at the given random\\nsamples {xj}m\\nj=1. Any algorithm therefore accesses each function f through its sketch samples\\nS[f] = (f(x1), . . . , f(xm)),\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 2}, page_content='where S is the sample operator. After training, the quality of a resulting model is often measured by\\nits generalization to new data not used during training. With Rd × R as the input and output space,\\nwe quantify a function f’s generalization error with its expected risk:\\nEµ[f] := Eµ|y − f(x)|2.\\nThe functional || · ||µ, also gives the norm of the space L2(Rd, µx), which consists of functions\\nf : Rd → R with\\n∥f∥2\\nµ := Eµx [|f(x)|2].\\nIf the label y depends deterministically on the associated sample x, we can treat y as an element of\\nL2(Rd, µx), and the expected risk of any function f is the function’s distance to y. By sketching any\\nhypothesis function f with the sample operator S, we perform a Monte-Carlo approximation of the\\nexpected risk, which is termed the empirical risk:\\n∥f∥2\\nm := 1\\nm\\nmX\\nj=1\\n(f(xj) − yj)2 =\\n\\r\\r\\r\\r\\n1√m(y1, . . . , ym)T − S[f]\\n\\r\\r\\r\\r\\n2\\n2\\n.\\nThe random functional || · ||m also defines a seminorm on L2(Rd, µx), referred to as the empirical\\nnorm. Under mild assumptions, || · ||m fails to be a norm.\\nIn order to obtain a well generalizing model, the goal is to identify a function f with a low expected\\nrisk. However, with limited data, we are restricted to optimizing the empirical risk. Our strategy for\\nderiving generalization guarantees is based on the stochastic relation between both risks. If {xj}m\\nj=1\\nare independently distributed by µx, the law of large numbers implies that for any f ∈ L2(Rd, µx)\\nthe convergence\\nlim\\nm→∞\\n∥f∥m = ∥f∥µ.\\nWhile this establishes the asymptotic convergence of the empirical norm to the function norm for a\\nsingle function f, we have to consider two issues to formulate our concept of norm concentration:\\nFirst, we need non-asymptotic results, that is bounds on the distance |∥f∥m − ∥f∥µ| for a fixed\\nnumber of samples m. Second, the bounds on the distance need to be uniformly valid for all functions\\nf in a given set.\\nSample operators which have uniform concentration properties have been studied as restricted\\nisometries in the area of compressed sensing. For shallow ReLU networks of the form (1), we define\\nthe restricted isometry property of the sampling operator S as follows.\\nDefinition 1. Let s ∈ (0, 1) be a constant and ¯P be a parameter set. We say that the Neural Restricted\\nIsometry Property (NeuRIPs( ¯P)) is satisfied if, for all ¯p ∈ ¯P it holds that\\n(1 − s)∥ϕ¯p∥µ ≤ ∥ϕ¯p∥m ≤ (1 + s)∥ϕ¯p∥µ.\\nIn the following Theorem, we provide a bound on the number m of samples, which is sufficient for\\nthe operator S to satisfy NeuRIPs( ¯P).\\nTheorem 1. There exist universal constants C1, C2 ∈ R such that the following holds: For\\nany sample operator S, constructed from random samples {xj}, respecting Assumption 2, let\\n¯P ⊂ (Rd × R × {±1})n be any parameter set satisfying Assumption 1 and ||ϕ¯p||µ > 1 for all\\n¯p ∈ ¯P. Then, for any u > 2 and s ∈ (0, 1), NeuRIPs( ¯P) is satisfied with probability at least\\n1 − 17 exp(−u/4) provided that\\nm ≥ n3c2\\nw\\n(1 − s)2 max\\n\\x12\\nC1\\n(8cb + d + ln(2))\\nu , C2\\nn2c2\\nw\\n(u/s)2\\n\\x13\\n.\\nOne should notice that, in Theorem 1, there is a tradeoff between the parameter s, which limits the\\ndeviation |∥ · ∥m − ∥ · ∥µ|, and the confidence parameter u. The lower bound on the corresponding\\nsample size m is split into two scaling regimes when understanding the quotientu of |∥·∥ m −∥·∥ µ|/s\\nas a precision parameter. While in the regime of low deviations and high probabilities the sample size\\nm must scale quadratically with u/s, in the regime of less precise statements one observes a linear\\nscaling.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 3}, page_content='4 Uniform Generalization of Sublevel Sets of the Empirical Risk\\nWhen the NeuRIPs event occurs, the function norm || · ||µ, which is related to the expected risk, is\\nclose to || · ||m, which corresponds to the empirical risk. Motivated by this property, we aim to find\\na shallow ReLU network ϕ¯p with small expected risk by solving the empirical risk minimization\\nproblem:\\nmin\\n¯p∈ ¯P\\n∥ϕ¯p − y∥2\\nm.\\nSince the set Φ ¯P of shallow ReLU networks is non-convex, this minimization cannot be solved\\nwith efficient convex optimizers. Therefore, instead of analyzing only the solution ϕ∗\\n¯p of the opti-\\nmization problem, we introduce a tolerance ϵ >0 for the empirical risk and provide bounds on the\\ngeneralization error, which hold uniformly on the sublevel set\\n¯Qy,ϵ :=\\n\\x08\\n¯p ∈ ¯P : ∥ϕ¯p − y∥2\\nm ≤ ϵ\\n\\t\\n.\\nBefore considering generic regression problems, we will initially assume the label y to be a neural\\nnetwork itself, parameterized by a tuplep∗ within the hypothesis set P. For all (x, y) in the support of\\nµ, we have y = ϕp∗ (x) and the expected risk’s minimum on P is zero. Using the sufficient condition\\nfor NeuRIPs from Theorem 1, we can provide generalization bounds for ϕ¯p ∈ ¯Qy,ϵ for any ϵ >0.\\nTheorem 2. Let ¯P be a parameter set that satisfies Assumption 1 and let u ≥ 2 and t ≥ ϵ >0 be\\nconstants. Furthermore, let the number m of samples satisfy\\nm ≥ 8n3c2\\nw (8cb + d + ln(2)) max\\n\\x12\\nC1\\nu\\n(t − ϵ)2 , C2\\nn2c2\\nwu\\n(t − ϵ)2\\n\\x13\\n,\\nwhere C1 and C2 are universal constants. Let {(xj, yj)}m\\nj=1 be a dataset respecting Assumption 2\\nand let there exist a ¯p∗ ∈ ¯P such that yj = ϕ¯p∗ (xj) holds for all j ∈ [m]. Then, with probability at\\nleast 1 − 17 exp(−u/4), we have for all ¯q ∈ ¯Qy,ϵ that\\n∥ϕ¯q − ϕ¯p∗ ∥2\\nµ ≤ t.\\nProof. We notice that ¯Qy,ϵ is a set of shallow neural networks with 2n neurons. We normalize such\\nnetworks with a function norm greater than t and parameterize them by\\n¯Rt := {ϕ¯p − ϕ¯p∗ : ¯p ∈ ¯P ,∥ϕ¯p − ϕ¯p∗ ∥µ > t}.\\nWe assume that NeuRIPs( ¯Rt) holds for s = (t − ϵ)2/t2. In this case, for all ¯q ∈ ¯Qy,ϵ, we have that\\n∥ϕ¯q − ϕ¯p∗ ∥m ≥ t and thus ¯q /∈ ¯Qϕ¯p∗ ,ϵ, which implies that ∥ϕ¯q − ϕ¯p∗ ∥µ ≤ t.\\nWe also note that ¯Rt satisfies Assumption 1 with a rescaled constantcw/t and normalization-invariant\\ncb, if ¯P satisfies it for cw and cb. Theorem 1 gives a lower bound on the sample complexity for\\nNeuRIPs( ¯Rt), completing the proof.\\nAt any network where an optimization method terminates, the concentration of the empirical risk\\nat the expected risk can be achieved with less data than needed to achieve an analogous NeuRIPs\\nevent. However, in the chosen stochastic setting, we cannot assume that the termination of an\\noptimization and the norm concentration at that network are independent events. We overcome this\\nby not specifying the outcome of an optimization method and instead stating uniform bounds on\\nthe norm concentration. The only assumption on an algorithm is therefore the identification of a\\nnetwork that permits an upper bound ϵ on its empirical risk. The event NeuRIPs( ¯Rt) then restricts the\\nexpected risk to be below the corresponding level t.\\nWe now discuss the empirical risk surface for generic distributionsµ that satisfy Assumption 2, where\\ny does not necessarily have to be a neural network.\\nTheorem 3. There exist constants C0, C1, C2, C3, C4, and C5 such that the following holds: Let ¯P\\nsatisfy Assumption 1 for some constants cw, cb, and let ¯p∗ ∈ ¯P be such that for some c¯p∗ ≥ 0 we\\nhave\\nEµ\\n\\x14\\nexp\\n\\x12(y − ϕ¯p∗ (x))2\\nc2\\n¯p∗\\n\\x13\\x15\\n≤ 2.\\nWe assume, for any s ∈ (0, 1) and confidence parameter u >0, that the number of samples m is\\nlarge enough such that\\nm ≥ 8\\n(1 − s)2 max\\n\\x12\\nC1\\n\\x12n3c2\\nw(8cb + d + ln(2))\\nu\\n\\x13\\n, C2n2c2\\nw\\n\\x10u\\ns\\n\\x11\\x13\\n.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 4}, page_content='We further select confidence parameters v1, v2 > C0, and define for some ω ≥ 0 the parameter\\nη := 2(1 − s)∥ϕ¯p∗ − y∥µ + C3v1v2c¯p∗\\n1\\n(1 − s)1/4 + ω\\n√\\n1 − s.\\nIf we set ϵ = ∥ϕ¯p∗ − y∥2\\nm + ω2 as the tolerance for the empirical risk, then the probability that all\\n¯q ∈ ¯Qy,ϵ satisfy\\n∥ϕ¯q − y∥µ ≤ η\\nis at least\\n1 − 17 exp\\n\\x10\\n−u\\n4\\n\\x11\\n− C5v2 exp\\n\\x12\\n−C4mv2\\n2\\n2\\n\\x13\\n.\\nProof sketch.(Complete proof in Appendix E) We first define and decompose the excess risk by\\nE(¯q, ¯p∗) := ∥ϕ¯q − y∥2\\nµ − ∥ϕ¯p∗ − y∥2\\nµ = ∥ϕ¯q − ϕ¯p∗ ∥2\\nµ − 2\\nm\\nmX\\nj=1\\n(ϕ¯p∗ (xj) − yj)(ϕ¯q(xj) − ϕ¯p∗ (xj)).\\nIt suffices to show, that within the stated confidence level we have∥ϕ¯q − y∥µ > η. This implies the\\nclaim since ∥ϕ¯q − y∥m ≤ ϵ implies ∥ϕ¯q − y∥µ ≤ η. We have E[E(¯q, ¯p∗)] > 0. It now only remains\\nto strengthen the condition on η >3∥ϕ¯p∗ − y∥µ to achieve E(¯q, ¯p∗) > ω2. We apply Theorem 1\\nto derive a bound on the fluctuation of the first term. The concentration rate of the second term is\\nderived similar to Theorem 1 by using chaining techniques. Finally in Appendix E, Theorem 12 gives\\na general bound to achieve\\nE(¯q, ¯p∗) > ω2\\nuniformly for all ¯q with ∥ϕ¯q − ϕ¯p∗ ∥µ > η. Theorem 3 then follows as a simplification.\\nIt is important to notice that, in Theorem 3, as the data size m approaches infinity, one can select\\nan asymptotically small deviation constant s. In this limit, the bound η on the generalization error\\nconverges to 3∥ϕ¯p∗ − y∥µ + ω. This reflects a lower limit of the generalization bound, which is the\\nsum of the theoretically achievable minimum of the expected risk and the additional tolerance ω.\\nThe latter is an upper bound on the empirical risk, which real-world optimization algorithms can be\\nexpected to achieve.\\n5 Size Control of Stochastic Processes on Shallow Networks\\nIn this section, we introduce the key techniques for deriving concentration statements for the em-\\npirical norm, uniformly valid for sets of shallow ReLU networks. We begin by rewriting the event\\nNeuRIPs( ¯P) by treating µ as a stochastic process, indexed by the parameter set ¯P. The event\\nNeuRIPs( ¯P) holds if and only if we have\\nsup\\n¯p∈ ¯P\\n|∥ϕ¯p∥m − ∥ϕ¯p∥µ| ≤s sup\\n¯p∈ ¯P\\n∥ϕ¯p∥µ.\\nThe supremum of stochastic processes has been studied in terms of their size. To determine the size\\nof a process, it is essential to determine the correlation between its variables. To this end, we define\\nthe Sub-Gaussian metric for any parameter tuples ¯p, ¯q ∈ ¯P as\\ndψ2 (ϕ¯p, ϕ¯q) := inf\\n(\\nCψ2 ≥ 0 : E\\n\"\\nexp\\n \\n|ϕ¯p(x) − ϕ¯q(x)|2\\nC2\\nψ2\\n!#\\n≤ 2\\n)\\n.\\nA small Sub-Gaussian metric between random variables indicates that their values are likely to be\\nclose. To capture the Sub-Gaussian structure of a process, we introduce ϵ-nets in the Sub-Gaussian\\nmetric. For a given ϵ >0, these are subsets ¯Q ⊆ ¯P such that for every ¯p ∈ ¯P, there is a ¯q ∈ ¯Q\\nsatisfying\\ndψ2 (ϕ¯p, ϕ¯q) ≤ ϵ.\\nThe smallest cardinality of such an ϵ-net ¯Q is known as the Sub-Gaussian covering number\\nN(Φ ¯P , dψ2 , ϵ). The next Lemma offers a bound for such covering numbers specific to shallow\\nReLU networks.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 5}, page_content='Lemma 1. Let ¯P be a parameter set satisfying Assumption 1. Then there exists a set ˆP with ¯P ⊆ ˆP\\nsuch that\\nN(Φ ˆP , dψ2 , ϵ) ≤ 2n ·\\n\\x1216ncbcw\\nϵ + 1\\n\\x13n\\n·\\n\\x1232ncbcw\\nϵ + 1\\n\\x13n\\n·\\n\\x121\\nϵ sin\\n\\x12 1\\n16ncw\\n\\x13\\n+ 1\\n\\x13d\\n.\\nThe proof of this Lemma is based on the theory of stochastic processes and can be seen in Theorem 8\\nof Appendix C.\\nTo obtain bounds of the form (6) on the size of a process, we use the generic chaining method. This\\nmethod offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric.\\nWe define it as follows. A sequence T = (Tk)k∈N0 in a set T is admissible if T0 = 1 and Tk ≤ 2(2k).\\nThe Talagrand-functional of the metric space is then defined as\\nγ2(T, d) := inf\\n(Tk)\\nsup\\nt∈T\\n∞X\\nk=0\\n2kd(t, Tk),\\nwhere the infimum is taken across all admissible sequences.\\nWith the bounds on the Sub-Gaussian covering number from Lemma 1, we provide a bound on the\\nTalagrand-functional for shallow ReLU networks in the following Lemma. This bound is expected to\\nbe of independent interest.\\nLemma 2. Let ¯P satisfy Assumption 1. Then we have\\nγ2(Φ ¯P , dψ2 ) ≤\\nr\\n2\\nπ\\n\\x128n3/2cw(8cb + d + 1)\\nln(2)\\np\\n2 ln(2)\\n\\x13\\n.\\nThe key ideas to show this bound are similar to the ones used to prove Theorem 9 in Appendix C.\\nTo provide bounds for the empirical process, we use the following Lemma, which we prove in\\nAppendix D.\\nLemma 3. Let Φ be a set of real functions, indexed by a parameter set ¯P and define\\nN(Φ) :=\\nZ ∞\\n0\\nq\\nln N(Φ, dψ2 , ϵ)dϵ and ∆(Φ) := sup\\nϕ∈Φ\\n∥ϕ∥ψ2 .\\nThen, for any u ≥ 2, we have with probability at least 1 − 17 exp(−u/4) that\\nsup\\nϕ∈Φ\\n|∥ϕ∥m − ∥ϕ∥µ| ≤ u√m\\n\\x14\\nN(Φ) + 10\\n3 ∆(Φ)\\n\\x15\\n.\\nThe bounds on the sample complexity for achieving the NeuRIPs event, from Theorem 1, are proven\\nby applying these Lemmata.\\nProof of Theorem 1.Since we assume ||ϕ¯p||µ > 1 for all ¯p ∈ ¯P, we have\\nsup\\n¯p∈ ¯P\\n|∥ϕ¯p∥m − ∥ϕ¯p∥µ| ≤sup\\n¯p∈ ¯P\\n|∥ϕ¯p∥m − ∥ϕ¯p∥µ|/∥ϕ¯p∥µ.\\nApplying Lemma 3, and further applying the bounds on the covering numbers and the Talagrand-\\nfunctional for shallow ReLU networks, the NeuRIPs( ¯P) event holds in case of s >3. The sample\\ncomplexities that are provided in Theorem 1 follow from a refinement of this condition.\\n6 Uniform Generalization of Sublevel Sets of the Empirical Risk\\nIn case of the NeuRIPs event, the function norm || · ||µ corresponding to the expected risk is close\\nto || · ||m, which corresponds to the empirical risk. With the previous results, we can now derive\\nuniform generalization error bounds in the sublevel set of the empirical risk.\\nWe use similar techniques and we define the following sets.\\n∥f∥p = sup\\n1≤q≤p\\n∥f∥q\\nΛk0,u = inf\\n(Tk)\\nsup\\nf∈F\\n∞X\\nk0\\n2k∥f − Tk(f)∥u2k\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 6}, page_content='and we need the following lemma:\\nLemma 9. For any set F of functions and u ≥ 1, we have\\nΛ0,u(F) ≤ 2√e(γ2(F, dψ2 ) + ∆(F)).\\nTheorem 10. Let P be a parameter set satisfying Assumption 1. Then, for any u ≥ 1, we have with\\nprobability at least 1 − 17 exp(−u/4) that\\nsup\\n¯p∈P\\n∥ϕ¯p∥m − ∥ϕ¯p∥µ ≤ u√m\\n\\x10\\n16n3/2cw(8cb + d + 1) + 2ncw\\n\\x11\\n.\\nProof. To this end we have to bound the Talagrand functional, where we can use Dudley’s inequality\\n(Lemma 6). To finish the proof, we apply the bounds on the covering numbers provided by Theorem\\n6.\\nTheorem 11. Let ¯P ⊆ (Rd × R × ±1)n satisfy Assumption 1. Then there exist universal constants\\nC1, C2 such that\\nsup\\n¯p∈P\\n∥ϕ¯p∥m − ∥ϕ¯p∥µ ≤\\nr\\n2\\nπ\\n\\x128n3/2cw(8cb + d + 1)\\nln(2)\\np\\n2 ln(2)\\n\\x13\\n.\\n7 Conclusion\\nIn this study, we investigated the empirical risk surface of shallow ReLU networks in terms of uniform\\nconcentration events for the empirical norm. We defined the Neural Restricted Isometry Property\\n(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs, which depends on\\nrealistic parameter bounds and the network architecture. We applied our findings to derive upper\\nbounds on the expected risk, which are valid uniformly across sublevel sets of the empirical risk.\\nIf a network optimization algorithm can identify a network with a small empirical risk, our results\\nguarantee that this network will generalize well. By deriving uniform concentration statements, we\\nhave resolved the problem of independence between the termination of an optimization algorithm at\\na certain network and the empirical risk concentration at that network. Future studies may focus on\\nperforming uniform empirical norm concentration on the critical points of the empirical risk, which\\ncould lead to even tighter bounds for the sample complexity.\\nWe also plan to apply our methods to input distributions more general than the Gaussian distribution.\\nIf generic Gaussian distributions can be handled, one could then derive bounds for the Sub-Gaussian\\ncovering number for deep ReLU networks by induction across layers. We also expect that our\\nresults on the covering numbers could be extended to more generic Lipschitz continuous activation\\nfunctions other than ReLU. This proposition is based on the concentration of measure phenomenon,\\nwhich provides bounds on the Sub-Gaussian norm of functions on normal concentrating input spaces.\\nBecause these bounds scale with the Lipschitz constant of the function, they can be used to findϵ-nets\\nfor neurons that have identical activation patterns.\\nBroader Impact\\nSupervised machine learning now affects both personal and public lives significantly. Generalization is\\ncritical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeper\\nunderstanding of the relationships between generalization, architectural design, and available data.\\nWe have discussed the concepts and demonstrated the effectiveness of using uniform concentration\\nevents for generalization guarantees of common supervised machine learning algorithms.\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 0}, page_content='The Importance of Written Explanations in\\nAggregating Crowdsourced Predictions\\nAbstract\\nThis study demonstrates that incorporating the written explanations provided by\\nindividuals when making predictions enhances the accuracy of aggregated crowd-\\nsourced forecasts. The research shows that while majority and weighted vote\\nmethods are effective, the inclusion of written justifications improves forecast\\naccuracy throughout most of a question’s duration, with the exception of its final\\nphase. Furthermore, the study analyzes the attributes that differentiate reliable and\\nunreliable justifications.\\n1 Introduction\\nThe concept of the \"wisdom of the crowd\" posits that combining information from numerous non-\\nexpert individuals can produce answers that are as accurate as, or even more accurate than, those\\nprovided by a single expert. A classic example of this concept is the observation that the median\\nestimate of an ox’s weight from a large group of fair attendees was remarkably close to the actual\\nweight. While generally supported, the idea is not without its limitations. Historical examples\\ndemonstrate instances where crowds behaved irrationally, and even a world chess champion was able\\nto defeat the combined moves of a crowd.\\nIn the current era, the advantages of collective intelligence are widely utilized. For example, Wikipedia\\nrelies on the contributions of volunteers, and community-driven question-answering platforms have\\ngarnered significant attention from the research community. When compiling information from\\nlarge groups, it is important to determine whether the individual inputs were made independently. If\\nnot, factors like group psychology and the influence of persuasive arguments can skew individual\\njudgments, thus negating the positive effects of crowd wisdom.\\nThis paper focuses on forecasts concerning questions spanning political, economic, and social\\ndomains. Each forecast includes a prediction, estimating the probability of a particular event, and\\na written justification that explains the reasoning behind the prediction. Forecasts with identical\\npredictions can have justifications of varying strength, which, in turn, affects the perceived reliability\\nof the predictions. For instance, a justification that simply refers to an external source without\\nexplanation may appear to rely heavily on the prevailing opinion of the crowd and might be considered\\nweaker than a justification that presents specific, verifiable facts from external resources.\\nTo clarify the terminology used: a \"question\" is defined as a statement that seeks information (e.g.,\\n\"Will new legislation be implemented before a certain date?\"). Questions have a defined start and\\nend date, and the period between these dates constitutes the \"life\" of the question. \"Forecasters\"\\nare individuals who provide a \"forecast,\" which consists of a \"prediction\" and a \"justification.\" The\\nprediction is a numerical representation of the likelihood of an event occurring. The justification\\nis the text provided by the forecaster to support their prediction. The central problem addressed in\\nthis work is termed \"calling a question,\" which refers to the process of determining a final prediction\\nby aggregating individual forecasts. Two strategies are employed for calling questions each day\\nthroughout their life: considering forecasts submitted on the given day (\"daily\") and considering the\\nlast forecast submitted by each forecaster (\"active\").'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 1}, page_content='Inspired by prior research on recognizing and fostering skilled forecasters, and analyzing written\\njustifications to assess the quality of individual or collective forecasts, this paper investigates the\\nautomated calling of questions throughout their duration based on the forecasts available each day.\\nThe primary contributions are empirical findings that address the following research questions:\\n* When making a prediction on a specific day, is it advantageous to include forecasts from previous\\ndays? (Yes) * Does the accuracy of the prediction improve when considering the question itself\\nand the written justifications provided with the forecasts? (Yes) * Is it easier to make an accurate\\nprediction toward the end of a question’s duration? (Yes) * Are written justifications more valuable\\nwhen the crowd’s predictions are less accurate? (Yes)\\nIn addition, this research presents an examination of the justifications associated with both accurate\\nand inaccurate forecasts. This analysis aims to identify the features that contribute to a justification\\nbeing more or less credible.\\n2 Related Work\\nThe language employed by individuals is indicative of various characteristics. Prior research includes\\nboth predictive models (using language samples to predict attributes about the author) and models\\nthat provide valuable insights (using language samples and author attributes to identify differentiating\\nlinguistic features). Previous studies have examined factors such as gender and age, political ideology,\\nhealth outcomes, and personality traits. In this paper, models are constructed to predict outcomes\\nbased on crowd-sourced forecasts without knowledge of individual forecasters’ identities.\\nPrevious research has also explored how language use varies depending on the relationships between\\nindividuals. For instance, studies have analyzed language patterns in social networks, online commu-\\nnities, and corporate emails to understand how individuals in positions of authority communicate.\\nSimilarly, researchers have examined how language provides insights into interpersonal interactions\\nand relationships. In terms of language form and function, prior research has investigated politeness,\\nempathy, advice, condolences, usefulness, and deception. Related to the current study’s focus,\\nresearchers have examined the influence of Wikipedia editors and studied influence levels within\\nonline communities. Persuasion has also been analyzed from a computational perspective, including\\nwithin the context of dialogue systems. The work presented here complements these previous studies.\\nThe goal is to identify credible justifications to improve the aggregation of crowdsourced forecasts,\\nwithout explicitly targeting any of the aforementioned characteristics.\\nWithin the field of computational linguistics, the task most closely related to this research is argumen-\\ntation. A strong justification for a forecast can be considered a well-reasoned supporting argument.\\nPrevious work in this area includes identifying argument components such as claims, premises,\\nbacking, rebuttals, and refutations, as well as mining arguments that support or oppose a particular\\nclaim. Despite these efforts, it was found that crowdsourced justifications rarely adhere to these\\nestablished argumentation frameworks, even though such justifications are valuable for aggregating\\nforecasts.\\nFinally, several studies have focused on forecasting using datasets similar or identical to the one used\\nin this research. From a psychological perspective, researchers have explored strategies for enhancing\\nforecasting accuracy, such as utilizing top-performing forecasters (often called \"superforecasters\"),\\nand have analyzed the traits that contribute to their success. These studies aim to identify and cultivate\\nsuperforecasters but do not incorporate the written justifications accompanying forecasts. In contrast,\\nthe present research develops models to call questions without using any information about the\\nforecasters themselves. Within the field of computational linguistics, researchers have evaluated the\\nlanguage used in high-quality justifications, focusing on aspects like rating, benefit, and influence.\\nOther researchers have developed models to predict forecaster skill using the textual justifications\\nfrom specific datasets, such as the Good Judgment Open data, and have also applied these models\\nto predict the accuracy of individual forecasts in other contexts, such as company earnings reports.\\nHowever, none of these prior works have specifically aimed to call questions throughout their entire\\nduration.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 2}, page_content='3 Dataset\\nThe research utilizes data from the Good Judgment Open, a platform where questions are posted, and\\nindividuals submit their forecasts. The questions primarily revolve around geopolitics, encompassing\\nareas such as domestic and international politics, the economy, and social matters. For this study, all\\nbinary questions were collected, along with their associated forecasts, each comprising a prediction\\nand a justification. In total, the dataset contains 441 questions and 96,664 forecasts submitted\\nover 32,708 days. This dataset significantly expands upon previous research, nearly doubling the\\nnumber of forecasts analyzed. Since the objective is to accurately call questions throughout their\\nentire duration, all forecasts with written justifications are included, regardless of factors such as\\njustification length or the number of forecasts submitted by a single forecaster. Additionally, this\\napproach prioritizes privacy, as no information about the individual forecasters is utilized.\\nTable 1: Analysis of the questions from our dataset. Most questions are relatively long, contain two\\nor more named entities, and are open for over one month.\\nMetric Min Q1 Q2 (Median) Q3 Max Mean\\n# tokens 8 16 20 28 48 21.94\\n# entities 0 2 3 5 11 3.47\\n# verbs 0 2 2 3 6 2.26\\n# days open 2 24 59 98 475 74.16\\nTable 1 provides a basic analysis of the questions in the dataset. The majority of questions are\\nrelatively lengthy, containing more than 16 tokens and multiple named entities, with geopolitical,\\nperson, and date entities being the most frequent. In terms of duration, half of the questions remain\\nopen for nearly two months, and 75% are open for more than three weeks.\\nAn examination of the topics covered by the questions using Latent Dirichlet Allocation (LDA)\\nreveals three primary themes: elections (including terms like \"voting,\" \"winners,\" and \"candidate\"),\\ngovernment actions (including terms like \"negotiations,\" \"announcements,\" \"meetings,\" and \"passing\\n(a law)\"), and wars and violent crimes (including terms like \"groups,\" \"killing,\" \"civilian (casualties),\"\\nand \"arms\"). Although not explicitly represented in the LDA topics, the questions address both\\ndomestic and international events within these broad themes.\\nTable 2: Analysis of the 96,664 written justifications submitted by forecasters in our dataset. The\\nreadability scores indicate that most justifications are easily understood by high school students (11th\\nor 12th grade), although a substantial amount (>25%) require a college education (Flesch under 50 or\\nDale-Chall over 9.0).\\nMin Q1 Q2 Q3 Max\\n#sentences 1 1 1 3 56\\n#tokens 1 10 23 47 1295\\n#entities 0 0 2 4 154\\n#verbs 0 1 3 6 174\\n#adverbs 0 0 1 3 63\\n#adjectives 0 0 2 4 91\\n#negation 0 0 1 3 69\\nSentiment -2.54 0 0 0.20 6.50\\nReadability\\nFlesch -49.68 50.33 65.76 80.62 121.22\\nDale-Chall 0.05 6.72 7.95 9.20 19.77\\nTable 2 presents a fundamental analysis of the 96,664 forecast justifications in the dataset. The median\\nlength is relatively short, consisting of one sentence and 23 tokens. Justifications mention named\\nentities less frequently than the questions themselves. Interestingly, half of the justifications contain\\nat least one negation, and 25% include three or more. This suggests that forecasters sometimes base\\ntheir predictions on events that might not occur or have not yet occurred. The sentiment polarity of\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 3}, page_content='the justifications is generally neutral. In terms of readability, both the Flesch and Dale-Chall scores\\nsuggest that approximately a quarter of the justifications require a college-level education for full\\ncomprehension.\\nRegarding verbs and nouns, an analysis using WordNet lexical files reveals that the most common\\nverb classes are \"change\" (e.g., \"happen,\" \"remain,\" \"increase\"), \"social\" (e.g., \"vote,\" \"support,\"\\n\"help\"), \"cognition\" (e.g., \"think,\" \"believe,\" \"know\"), and \"motion\" (e.g., \"go,\" \"come,\" \"leave\").\\nThe most frequent noun classes are \"act\" (e.g., \"election,\" \"support,\" \"deal\"), \"communication\" (e.g.,\\n\"questions,\" \"forecast,\" \"news\"), \"cognition\" (e.g., \"point,\" \"issue,\" \"possibility\"), and \"group\" (e.g.,\\n\"government,\" \"people,\" \"party\").\\n4 Experiments and Results\\nExperiments are conducted to address the challenge of accurately calling a question throughout\\nits duration. The input consists of the question itself and the associated forecasts (predictions and\\njustifications), while the output is an aggregated answer to the question derived from all forecasts.\\nThe number of instances corresponds to the total number of days all questions were open. Both\\nsimple baselines and a neural network are employed, considering both (a) daily forecasts and (b)\\nactive forecasts submitted up to ten days prior.\\nThe questions are divided into training, validation, and test subsets. Subsequently, all forecasts\\nsubmitted throughout the duration of each question are assigned to their respective subsets. It’s\\nimportant to note that randomly splitting the forecasts would be an inappropriate approach. This is\\nbecause forecasts for the same question submitted on different days would be distributed across the\\ntraining, validation, and test subsets, leading to data leakage and inaccurate performance evaluation.\\n4.1 Baselines\\nTwo unsupervised baselines are considered. The \"majority vote\" baseline determines the answer to a\\nquestion based on the most frequent prediction among the forecasts. The \"weighted vote\" baseline,\\non the other hand, assigns weights to the probabilities in the predictions and then aggregates them.\\n4.2 Neural Network Architecture\\nA neural network architecture is employed, which consists of three main components: one to generate\\na representation of the question, another to generate a representation of each forecast, and an LSTM\\nto process the sequence of forecasts and ultimately call the question.\\nThe representation of a question is obtained using BERT, followed by a fully connected layer with 256\\nneurons, ReLU activation, and dropout. The representation of a forecast is created by concatenating\\nthree elements: (a) a binary flag indicating whether the forecast was submitted on the day the question\\nis being called or on a previous day, (b) the prediction itself (a numerical value between 0.0 and 1.0),\\nand (c) a representation of the justification. The representation of the justification is also obtained\\nusing BERT, followed by a fully connected layer with 256 neurons, ReLU activation, and dropout.\\nThe LSTM has a hidden state with a dimensionality of 256 and processes the sequence of forecasts\\nas its input. During the tuning process, it was discovered that providing the representation of the\\nquestion alongside each forecast is more effective than processing forecasts independently of the\\nquestion. Consequently, the representation of the question is concatenated with the representation of\\neach forecast before being fed into the LSTM. Finally, the last hidden state of the LSTM is connected\\nto a fully connected layer with a single neuron and sigmoid activation to produce the final prediction\\nfor the question.\\n4.3 Architecture Ablation\\nExperiments are carried out with the complete neural architecture, as described above, as well as\\nwith variations where certain components are disabled. Specifically, the representation of a forecast\\nis manipulated by incorporating different combinations of information:\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 4}, page_content='* Only the prediction. * The prediction and the representation of the question. * The prediction and\\nthe representation of the justification. * The prediction, the representation of the question, and the\\nrepresentation of the justification.\\n4.4 Quantitative Results\\nThe evaluation metric used is accuracy, which represents the average percentage of days a model\\ncorrectly calls a question throughout its duration. Results are reported for all days combined, as well\\nas for each of the four quartiles of the question’s duration.\\nTable 3: Results with the test questions (Accuracy: average percentage of days a model predicts a\\nquestion correctly). Results are provided for all days a question was open and for four quartiles (Q1:\\nfirst 25% of days, Q2: 25-50%, Q3: 50-75%, and Q4: last 25% of days).\\nDays When the Question Was Open\\nModel All Days Q1 Q2 Q3 Q4\\nUsing Daily Forecasts Only\\nBaselines\\nMajority V ote (predictions) 71.89 64.59 66.59 73.26 82.22\\nWeighted V ote (predictions) 73.79 67.79 68.71 74.16 83.61\\nNeural Network Variants\\nPredictions Only 77.96 77.62 77.93 78.23 78.61\\nPredictions + Question 77.61 75.44 76.77 78.05 81.56\\nPredictions + Justifications 80.23 77.87 78.65 79.26 84.67\\nPredictions + Question + Justifications 79.96 78.65 78.11 80.29 83.28\\nUsing Active Forecasts\\nBaselines\\nMajority V ote (predictions) 77.27 68.83 73.92 77.98 87.44\\nWeighted V ote (predictions) 77.97 72.04 72.17 78.53 88.22\\nNeural Network Variants\\nPredictions Only 78.81 77.31 78.04 78.53 81.11\\nPredictions + Question 79.35 76.05 78.53 79.56 82.94\\nPredictions + Justifications 80.84 77.86 79.07 79.74 86.17\\nPredictions + Question + Justifications 81.27 78.71 79.81 81.56 84.67\\nDespite their relative simplicity, the baseline methods achieve commendable results, demonstrating\\nthat aggregating forecaster predictions without considering the question or justifications is a viable\\nstrategy. However, the full neural network achieves significantly improved results.\\n**Using Daily or Active Forecasts** Incorporating active forecasts, rather than solely relying on\\nforecasts submitted on the day the question is called, proves advantageous for both baselines and all\\nneural network configurations, except for the one using only predictions and justifications.\\n**Encoding Questions and Justifications** The neural network that only utilizes the prediction\\nto represent a forecast surpasses both baseline methods. Notably, integrating the question, the\\njustification, or both into the forecast representation yields further improvements. These results\\nindicate that incorporating the question and forecaster-provided justifications into the model enhances\\nthe accuracy of question calling.\\n**Calling Questions Throughout Their Life** When examining the results across the four quartiles of\\na question’s duration, it’s observed that while using active forecasts is beneficial across all quartiles\\nfor both baselines and all network configurations, the neural networks surprisingly outperform the\\nbaselines only in the first three quartiles. In the last quartile, the neural networks perform significantly\\nworse than the baselines. This suggests that while modeling questions and justifications is generally\\nhelpful, it becomes detrimental toward the end of a question’s life. This phenomenon can be attributed\\nto the increasing wisdom of the crowd as more evidence becomes available and more forecasters\\ncontribute, making their aggregated predictions more accurate.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 5}, page_content='Table 4: Results with the test questions, categorized by question difficulty as determined by the best\\nbaseline model. The table presents the accuracy (average percentage of days a question is predicted\\ncorrectly) for all questions and for each quartile of difficulty: Q1 (easiest 25%), Q2 (25-50%), Q3\\n(50-75%), and Q4 (hardest 25%).\\nQuestion Difficulty (Based on Best Baseline)\\nAll Q1 Q2 Q3 Q4\\nUsing Active Forecasts\\nWeighted V ote Baseline (Predictions) 77.97 99.40 99.55 86.01 29.30\\nNeural Network with Components...\\nPredictions + Question 79.35 94.58 88.01 78.04 58.73\\nPredictions + Justifications 80.84 95.71 93.18 79.99 57.05\\nPredictions + Question + Justifications 81.27 94.17 90.11 78.67 64.41\\n**Calling Questions Based on Their Difficulty** The analysis is further refined by examining\\nresults based on question difficulty, determined by the number of days the best-performing baseline\\nincorrectly calls the question. This helps to understand which questions benefit most from the neural\\nnetworks that incorporate questions and justifications. However, it’s important to note that calculating\\nquestion difficulty during the question’s active period is not feasible, making these experiments\\nunrealistic before the question closes and the correct answer is revealed.\\nTable 4 presents the results for selected models based on question difficulty. The weighted vote\\nbaseline demonstrates superior performance for 75\\n5 Qualitative Analysis\\nThis section provides insights into the factors that make questions more difficult to forecast and\\nexamines the characteristics of justifications associated with incorrect and correct predictions.\\n**Questions** An analysis of the 88 questions in the test set revealed that questions called incorrectly\\non at least one day by the best model tend to have a shorter duration (69.4 days vs. 81.7 days) and a\\nhigher number of active forecasts per day (31.0 vs. 26.7). This suggests that the model’s errors align\\nwith the questions that forecasters also find challenging.\\n**Justifications** A manual review of 400 justifications (200 associated with incorrect predictions\\nand 200 with correct predictions) was conducted, focusing on those submitted on days when the best\\nmodel made an incorrect prediction. The following observations were made:\\n* A higher percentage of incorrect predictions (78%) were accompanied by short justifications\\n(fewer than 20 tokens), compared to 65% for correct predictions. This supports the idea that longer\\nuser-generated text often indicates higher quality. * References to previous forecasts (either by the\\nsame or other forecasters, or the current crowd’s forecast) were more common in justifications for\\nincorrect predictions (31.5%) than for correct predictions (16%). * A lack of a logical argument\\nwas prevalent in the justifications, regardless of the prediction’s accuracy. However, it was more\\nfrequent in justifications for incorrect predictions (62.5%) than for correct predictions (47.5%). *\\nSurprisingly, justifications with generic arguments did not clearly differentiate between incorrect and\\ncorrect predictions (16.0% vs. 14.5%). * Poor grammar and spelling or the use of non-English were\\ninfrequent but more common in justifications for incorrect predictions (24.5%) compared to correct\\npredictions (14.5%).\\n6 Conclusions\\nForecasting involves predicting future events, a capability highly valued by both governments and\\nindustries as it enables them to anticipate and address potential challenges. This study focuses on\\nquestions spanning the political, economic, and social domains, utilizing forecasts submitted by a\\ncrowd of individuals without specialized training. Each forecast comprises a prediction and a natural\\nlanguage justification.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 6}, page_content='The research demonstrates that aggregating the weighted predictions of forecasters is a solid baseline\\nfor calling a question throughout its duration. However, models that incorporate both the question\\nand the justifications achieve significantly better results, particularly during the first three quartiles of\\na question’s life. Importantly, the models developed in this study do not profile individual forecasters\\nor utilize any information about their identities. This work lays the groundwork for evaluating the\\ncredibility of anonymous forecasts, enabling the development of robust aggregation strategies that do\\nnot require tracking individual forecasters.\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 0}, page_content='Advanced techniques for through and contextually\\nInterpreting Noun-Noun Compounds\\nAbstract\\nThis study examines the effectiveness of transfer learning and multi-task learning\\nin the context of a complex semantic classification problem: understanding the\\nmeaning of noun-noun compounds. Through a series of detailed experiments and\\nan in-depth analysis of errors, we demonstrate that employing transfer learning by\\ninitializing parameters and multi-task learning through parameter sharing enables a\\nneural classification model to better generalize across a dataset characterized by a\\nhighly uneven distribution of semantic relationships. Furthermore, we illustrate\\nhow utilizing dual annotations, which involve two distinct sets of relations applied\\nto the same compounds, can enhance the overall precision of a neural classifier and\\nimprove its F1 scores for less common yet more challenging semantic relations.\\n1 Introduction\\nNoun-noun compound interpretation involves determining the semantic connection between two\\nnouns (or noun phrases in multi-word compounds). For instance, in the compound \"street protest,\"\\nthe task is to identify the semantic relationship between \"street\" and \"protest,\" which is a locative\\nrelation in this example. Given the prevalence of noun-noun compounds in natural language and its\\nsignificance to other natural language processing (NLP) tasks like question answering and information\\nretrieval, understanding noun-noun compounds has been extensively studied in theoretical linguistics,\\npsycholinguistics, and computational linguistics.\\nIn computational linguistics, noun-noun compound interpretation is typically treated as an automatic\\nclassification task. Various machine learning (ML) algorithms and models, such as Maximum\\nEntropy, Support Vector Machines, and Neural Networks, have been employed to decipher the\\nsemantics of nominal compounds. These models utilize information from lexical semantics, like\\nWordNet-based features, and distributional semantics, such as word embeddings. However, noun-\\nnoun compound interpretation remains a challenging NLP problem due to the high productivity\\nof noun-noun compounding as a linguistic structure and the difficulty in deriving the semantics of\\nnoun-noun compounds from their constituents. Our research contributes to advancing NLP research\\non noun-noun compound interpretation through the application of transfer and multi-task learning.\\nThe application of transfer learning (TL) and multi-task learning (MTL) in NLP has gained significant\\nattention in recent years, yielding varying outcomes based on the specific tasks, model architectures,\\nand datasets involved. These varying results, combined with the fact that neither TL nor MTL has\\nbeen previously applied to noun-noun compound interpretation, motivate our thorough empirical\\ninvestigation into the use of TL and MTL for this task. Our aim is not only to add to the existing\\nresearch on the effectiveness of TL and MTL for semantic NLP tasks generally but also to ascertain\\ntheir specific advantages for compound interpretation.\\nA key reason for utilizing multi-task learning is to enhance generalization by making use of the\\ndomain-specific details present in the training data of related tasks. In this study, we demonstrate that\\nTL and MTL can serve as a form of regularization, enabling the prediction of infrequent relations\\nwithin a dataset marked by a highly skewed distribution of relations. This dataset is particularly\\nwell-suited for TL and MTL experimentation, as elaborated in Section 3.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 1}, page_content='Our contributions are summarized as follows:\\n1. Through meticulous analysis of results, we discover that TL and MTL, especially when applied\\nto the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a\\nhighly skewed dataset, compared to a robust single-task learning baseline. 2. Although our research\\nconcentrates on TL and MTL, we present, to our knowledge, the first experimental results on the\\nrelatively recent dataset from Fares (2016).\\n2 Related Work\\nApproaches to interpreting noun-noun compounds differ based on the classification of compound\\nrelations, as well as the machine learning models and features employed to learn these relations. For\\ninstance, some define a broad set of relations, while others employ a more detailed classification.\\nSome researchers challenge the idea that noun-noun compounds can be interpreted using a fixed,\\npredetermined set of relations, proposing alternative methods based on paraphrasing. We center\\nour attention on methods that frame the interpretation problem as a classification task involving a\\nfixed, predetermined set of relations. Various machine learning models have been applied to this\\ntask, including nearest neighbor classifiers that use semantic similarity based on lexical resources,\\nkernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy\\nmodels that incorporate a wide range of lexical and surface form features, and neural networks that\\nrely on word embeddings or combine word embeddings with path embeddings. Among these studies,\\nsome have utilized the same dataset. To our knowledge, TL and MTL have not been previously\\napplied to compound interpretation. Therefore, we review prior research on TL and MTL in other\\nNLP tasks.\\nSeveral recent studies have conducted extensive experiments on the application of TL and MTL to a\\nvariety of NLP tasks, such as named entity recognition, semantic labeling, sentence-level sentiment\\nclassification, super-tagging, chunking, and semantic dependency parsing. The consensus among\\nthese studies is that the advantages of TL and MTL are largely contingent on the characteristics of the\\ntasks involved, including the unevenness of the data distribution, the semantic relatedness between\\nthe source and target tasks, the learning trajectory of the auxiliary and main tasks (where target tasks\\nthat quickly reach a plateau benefit most from non-plateauing auxiliary tasks), and the structural\\nsimilarity between the tasks. Besides differing in the NLP tasks they investigate, the aforementioned\\nstudies employ slightly varied definitions of TL and MTL. Our research aligns with certain studies in\\nthat we apply TL and MTL to learn different semantic annotations of noun-noun compounds using\\nthe same dataset. However, our experimental design is more akin to other work in that we experiment\\nwith initializing parameters across all layers of the neural network and concurrently train a single\\nMTL model on two sets of relations.\\n3 Task Definition and Dataset\\nThe objective of this task is to train a model to categorize the semantic relationships between pairs\\nof nouns in a labeled dataset, where each pair forms a noun-noun compound. The complexity of\\nthis task is influenced by factors such as the label set used and its distribution. For the experiments\\ndetailed in this paper, we utilize a noun-noun compounds dataset that features compounds annotated\\nwith two distinct taxonomies of relations. This means that each noun-noun compound is associated\\nwith two different relations, each based on different linguistic theories. This dataset is derived from\\nestablished linguistic resources, including NomBank and the Prague Czech-English Dependency\\nTreebank 2.0 (PCEDT). We chose this dataset for two primary reasons: firstly, the dual annotation of\\nrelations on the same set of compounds is ideal for exploring TL and MTL approaches; secondly,\\naligning two different annotation frameworks on the same data allows for a comparative analysis\\nacross these frameworks.\\nSpecifically, we use a portion of the dataset, focusing on type-based instances of two-word compounds.\\nThe original dataset also encompasses multi-word compounds (those made up of more than two\\nnouns) and multiple instances per compound type. We further divide the dataset into three parts:\\ntraining, development, and test sets. Table 1 details the number of compound types and the vocabulary\\nsize for each set, including a breakdown of words appearing in the right-most (right constituents)\\nand left-most (left constituents) positions. The two label sets consist of 35 PCEDT functors and 18\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 2}, page_content='NomBank argument and adjunct relations. As discussed in Section 7.1, these label sets have a highly\\nuneven distribution.\\nTable 1: Characteristics of the noun-noun compound dataset used in our experiments. The numbers\\nin this table correspond to a subset of the dataset, see Section 3.\\nTrain Dev Test\\nCompounds 6932 920 1759\\nV ocab size 4102 1163 1772\\nRight constituents 2304 624 969\\nLeft constituents 2405 618 985\\nMany relations in PCEDT and NomBank conceptually describe similar semantic ideas, as they are\\nused to annotate the semantics of the same text. For instance, the temporal and locative relations in\\nNomBank (ARGM-TMP and ARGM-LOC, respectively) and their PCEDT counterparts (TWHEN\\nand LOC) exhibit relatively consistent behavior across frameworks, as they annotate many of the\\nsame compounds. However, some relations that are theoretically similar do not align well in practice.\\nFor example, the functor AIM in PCEDT and the modifier argument ARGM-PNC in NomBank\\nexpress a somewhat related semantic concept (purpose), but there is minimal overlap between the\\nsets of compounds they annotate. Nevertheless, it is reasonable to assume that the semantic similarity\\nin the label sets, where it exists, can be leveraged through transfer and multi-task learning, especially\\nsince the overall distribution of relations differs between the two frameworks.\\n4 Transfer vs. Multi-Task Learning\\nIn this section, we employ the terminology and definitions established by Pan and Yang (2010) to\\narticulate our framework for transfer and multi-task learning. Our classification task can be described\\nin terms of all training pairs (X, Y) and a probability distribution P(X), where X represents the input\\nfeature space, Y denotes the set of all labels, and N is the training data size. The domain of a task is\\ndefined by X, P(X). Our goal is to learn a function f(X) that predicts Y based on the input features X.\\nConsidering two ML tasks, Ta and Tb, we would train two distinct models to learn separate functions\\nfa and fb for predicting Ya and Yb in a single-task learning scenario. However, if Ta and Tb are\\nrelated, either explicitly or implicitly, TL and MTL can enhance the generalization of either or both\\ntasks. Two tasks are deemed related when their domains are similar but their label sets differ, or when\\ntheir domains are dissimilar but their label sets are identical. Consequently, noun-noun compound\\ninterpretation using the dataset is well-suited for TL and MTL, as the training examples are identical,\\nbut the label sets are distinct.\\nFor clarity, we differentiate between transfer learning and multi-task learning in this paper, despite\\nthese terms sometimes being used interchangeably in the literature. We define TL as the utilization of\\nparameters from a model trained on Ta to initialize another model for Tb. In contrast, MTL involves\\ntraining parts of the same model to learn both Ta and Tb, essentially learning one set of parameters\\nfor both tasks. The concept is to train a single model simultaneously on both tasks, where one task\\nintroduces an inductive bias that aids the model in generalizing over the main task. It is important to\\nnote that this does not necessarily imply that we aim to use a single model to predict both label sets\\nin practice.\\n5 Neural Classification Models\\nThis section introduces the neural classification models utilized in our experiments. To discern the\\nimpact of TL and MTL, we initially present a single-task learning model, which acts as our baseline.\\nSubsequently, we employ this same model to implement TL and MTL.\\n5.1 Single-Task Learning Model\\nIn our single-task learning (STL) configuration, we train and fine-tune a feed-forward neural network\\ninspired by the neural classifier proposed by Dima and Hinrichs (2015). This network comprises four\\nlayers: 1) an input layer, 2) an embedding layer, 3) a hidden layer, and 4) an output layer. The input\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 3}, page_content='layer consists of two integers that indicate the indices of a compound’s constituents in the embedding\\nlayer, where the word embedding vectors are stored. These selected vectors are then passed to a fully\\nconnected hidden layer, the size of which matches the dimensionality of the word embedding vectors.\\nFinally, a softmax function is applied to the output layer to select the most probable relation.\\nThe compound’s constituents are represented using a 300-dimensional word embedding model trained\\non an English Wikipedia dump and the English Gigaword Fifth Edition. The embedding model was\\ntrained by Fares et al. (2017). If a word is not found during lookup in the embedding model, we\\ncheck if the word is uppercased and attempt to find the lowercase version. For hyphenated words\\nnot found in the embedding vocabulary, we split the word at the hyphen and average the vectors of\\nits parts, if they are present in the vocabulary. If the word remains unrepresented after these steps, a\\ndesignated vector for unknown words is employed.\\n5.1.1 Architecture and Hyperparameters\\nOur selection of hyperparameters is informed by multiple rounds of experimentation with the single-\\ntask learning model, as well as the choices made by prior work. The weights of the embedding layer\\nare updated during the training of all models. We utilize the Adaptive Moment Estimation (Adam)\\noptimization function across all models, with a learning rate set to 0.001. The loss function employed\\nis the negative-log likelihood. A Sigmoid activation function is used for the units in the hidden layer.\\nAll models are trained with mini-batches of size five. The maximum number of epochs is capped\\nat 50, but an early stopping criterion based on the model’s accuracy on the validation split is also\\nimplemented. This means that training is halted if the validation accuracy does not improve over five\\nconsecutive epochs. All models are implemented in Keras, using TensorFlow as the backend. The TL\\nand MTL models are trained using the same hyperparameters as the STL model.\\n5.2 Transfer Learning Models\\nIn our experiments, transfer learning involves training an STL model on PCEDT relations and then\\nusing some of its weights to initialize another model for NomBank relations. Given the neural\\nclassifier architecture detailed in Section 5.1, we identify three ways to implement TL: 1) TLE:\\nTransferring the embedding layer weights, 2) TLH: Transferring the hidden layer weights, and 3)\\nTLEH: Transferring both the embedding and hidden layer weights. Furthermore, we differentiate\\nbetween transfer learning from PCEDT to NomBank and vice versa. This results in six setups,\\nas shown in Table 2. We do not apply TL (or MTL) to the output layer because it is task- or\\ndataset-specific.\\n5.3 Multi-Task Learning Models\\nIn MTL, we train a single model to simultaneously learn both PCEDT and NomBank relations,\\nmeaning all MTL models have two objective functions and two output layers. We implement two\\nMTL setups: MTLE, which features a shared embedding layer but two task-specific hidden layers,\\nand MTLF, which has no task-specific layers aside from the output layer (i.e., both the embedding\\nand hidden layers are shared). We distinguish between the auxiliary and main tasks based on which\\nvalidation accuracy (NomBank’s or PCEDT’s) is monitored by the early stopping criterion. This\\nleads to a total of four MTL models, as shown in Table 3.\\n6 Experimental Results\\nTables 2 and 3 display the accuracies of the various TL and MTL models on the development and test\\nsplits for NomBank and PCEDT. The top row in both tables indicates the accuracy of the STL model.\\nAll models were trained solely on the training split. Several insights can be gleaned from these\\ntables. Firstly, the accuracy of the STL models decreases when evaluated on the test split for both\\nNomBank and PCEDT. Secondly, all TL models achieve improved accuracy on the NomBank test\\nsplit, although transfer learning does not significantly enhance accuracy on the development split of\\nthe same dataset. The MTL models, especially MTLF, have a detrimental effect on the development\\naccuracy of NomBank, yet we observe a similar improvement, as with TL, on the test split. Thirdly,\\nboth TL and MTL models demonstrate less consistent effects on PCEDT (on both development and\\ntest splits) compared to NomBank. For instance, all TL models yield an absolute improvement of\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 4}, page_content='about 1.25 points in accuracy on NomBank, whereas in PCEDT, TLE clearly outperforms the other\\ntwo TL models (TLE improves over the STL accuracy by 1.37 points).\\nTable 2: Accuracy (%) of the transfer learning models.\\nModel NomBank PCEDT\\nDev Test Dev Test\\nSTL 78.15 76.75 58.80 56.05\\nTLE 78.37 78.05 59.57 57.42\\nTLH 78.15 78.00 59.24 56.51\\nTLEH 78.48 78.00 59.89 56.68\\nTable 3: Accuracy (%) of the MTL models.\\nModel NomBank PCEDT\\nDev Test Dev Test\\nSTL 78.15 76.75 58.80 56.05\\nMTLE 77.93 78.45 59.89 56.96\\nMTLF 76.74 78.51 58.91 56.00\\nOverall, the STL models’ accuracy declines when tested on the NomBank and PCEDT test splits,\\ncompared to their performance on the development split. This could suggest overfitting, especially\\nsince our stopping criterion selects the model with the best performance on the development split.\\nConversely, TL and MTL enhance accuracy on the test splits, despite using the same stopping criterion\\nas STL. We interpret this as an improvement in the models’ ability to generalize. However, since\\nthese improvements are relatively minor, we further analyze the results to understand if and how TL\\nand MTL are beneficial.\\n7 Results Analysis\\nThis section provides a detailed analysis of the models’ performance, drawing on insights from the\\ndataset and the classification errors made by the models. The discussion in the following sections is\\nprimarily based on the results from the test split, as it is larger than the development split.\\n7.1 Relation Distribution\\nTo illustrate the complexity of the task, we depict the distribution of the most frequent relations in\\nNomBank and PCEDT across the three data splits in Figure 1. Notably, approximately 71.18% of the\\nrelations in the NomBank training split are of type ARG1 (prototypical patient), while 52.20% of the\\nPCEDT relations are of type RSTR (an underspecified adnominal modifier). Such a highly skewed\\ndistribution makes learning some of the other relations more challenging, if not impossible in certain\\ncases. In fact, out of the 15 NomBank relations observed in the test split, five are never predicted\\nby any of the STL, TL, or MTL models. Similarly, of the 26 PCEDT relations in the test split, only\\nsix are predicted. However, the unpredicted relations are extremely rare in the training split (e.g., 23\\nPCEDT functors appear less than 20 times), making it doubtful whether any ML model could learn\\nthem under any circumstances.\\nGiven this imbalanced distribution, it is evident that accuracy alone is insufficient to determine the\\nbest-performing model. Therefore, in the subsequent section, we report and analyze the F1 scores of\\nthe predicted NomBank and PCEDT relations across all STL, TL, and MTL models.\\n7.2 Per-Relation F1 Scores\\nTables 4 and 5 present the per-relation F1 scores for NomBank and PCEDT, respectively. We only\\ninclude results for relations that are actually predicted by at least one of the models.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 5}, page_content='Table 4: Per-label F1 score on the NomBank test split.\\nA0 A1 A2 A3 LOC MNR TMP\\nCount 132 1282 153 75 25 25 27\\nSTL 49.82 87.54 45.78 60.81 28.57 29.41 66.67\\nTLE 55.02 87.98 41.61 60.14 27.91 33.33 63.83\\nTLH 54.81 87.93 42.51 60.00 25.00 35.29 65.31\\nTLEH 53.62 87.95 42.70 61.11 29.27 33.33 65.22\\nMTLE 54.07 88.34 42.86 61.97 30.00 28.57 66.67\\nMTLF 53.09 88.41 38.14 62.69 00.00 00.00 52.17\\nTable 5: Per-label F1 score on the PCEDT test split.\\nACT TWHEN APP PAT REG RSTR\\nCount 89 14 118 326 216 900\\nSTL 43.90 42.11 22.78 42.83 20.51 68.81\\nTLE 49.37 70.97 27.67 41.60 30.77 69.67\\nTLH 53.99 62.07 25.00 43.01 26.09 68.99\\nTLEH 49.08 64.52 28.57 42.91 28.57 69.08\\nMTLE 54.09 66.67 24.05 42.03 27.21 69.31\\nMTLF 47.80 42.11 25.64 40.73 19.22 68.89\\nSeveral noteworthy patterns emerge from Tables 4 and 5. Firstly, the MTLF model appears to be\\ndetrimental to both datasets, leading to significantly degraded F1 scores for four NomBank relations,\\nincluding the locative modifier ARGM-LOC and the manner modifier ARGM-MNR (abbreviated as\\nLOC and MNR in Table 4), which the model fails to predict altogether. This same model exhibits\\nthe lowest F1 score compared to all other models for two PCEDT relations: REG (expressing a\\ncircumstance) and PAT (patient). Considering that the MTLF model achieves the highest accuracy\\non the NomBank test split (as shown in Table 3), it becomes even more apparent that relying solely\\non accuracy scores is inadequate for evaluating the effectiveness of TL and MTL for this task and\\ndataset.\\nSecondly, with the exception of the MTLF model, all TL and MTL models consistently improve\\nthe F1 score for all PCEDT relations except PAT. Notably, the F1 scores for the relations TWHEN\\nand ACT show a substantial increase compared to other PCEDT relations when only the embedding\\nlayer’s weights are shared (MTLE) or transferred (TLE). This outcome can be partially understood\\nby examining the correspondence matrices between NomBank arguments and PCEDT functors,\\npresented in Tables 7 and 6. These tables illustrate how PCEDT functors map to NomBank arguments\\nin the training split (Table 6) and vice versa (Table 7). Table 6 reveals that 80% of the compounds\\nannotated as TWHEN in PCEDT were annotated as ARGM-TMP in NomBank. Additionally, 47% of\\nACT (Actor) relations map to ARG0 (Proto-Agent) in NomBank. While this mapping is not as distinct\\nas one might hope, it is still relatively high when compared to how other PCEDT relations map to\\nARG0. The correspondence matrices also demonstrate that the presumed theoretical similarities\\nbetween NomBank and PCEDT relations do not always hold in practice. Nevertheless, even such\\nimperfect correspondences can provide a training signal that assists the TL and MTL models in\\nlearning relations like TWHEN and ACT.\\nSince the TLE model outperforms STL in predicting REG by ten absolute points, we examined\\nall REG compounds correctly classified by TLE but misclassified by STL. We found that STL\\nmisclassified them as RSTR, indicating that TL from NomBank helps TLE recover from STL’s\\novergeneralization in RSTR prediction.\\nThe two NomBank relations that receive the highest boost in F1 score (about five absolute points)\\nare ARG0 and ARGM-MNR, but the improvement in the latter corresponds to only one additional\\ncompound, which might be a chance occurrence. Overall, TL and MTL from NomBank to PCEDT\\nare more helpful than the reverse. One explanation is that five PCEDT relations (including the four\\nmost frequent ones) map to ARG1 in NomBank in more than 60% of cases for each relation, as seen\\nin the first rows of Tables 6 and 7. This suggests that the weights learned to predict PCEDT relations\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 6}, page_content='Table 6: Correspondence matrix between PCEDT functors and NomBank arguments. Slots with ’-’\\nindicate zero, 0.00 represents a very small number but not zero.\\nA1 A2 A0 A3 LOC TMP MNR\\nRSTR 0.70 0.11 0.06 0.06 0.02 0.01 0.02\\nPAT 0.90 0.05 0.01 0.02 0.01 - 0.00\\nREG 0.78 0.10 0.04 0.06 0.00 0.00 0.00\\nAPP 0.62 0.21 0.13 0.02 0.01 0.00 -\\nACT 0.47 0.03 0.47 0.01 0.01 - 0.01\\nAIM 0.65 0.12 0.07 0.06 0.01 - -\\nTWHEN 0.10 0.03 - - - 0.80 -\\nCount 3617 1312 777 499 273 116 59\\nTable 7: Correspondence matrix between NomBank arguments and PCEDT functors.\\nRSTR PAT REG APP ACT AIM TWHEN\\nA1 0.51 0.54 0.12 0.06 0.03 0.02 0.00\\nA2 0.47 0.09 0.11 0.14 0.01 0.02 0.00\\nA0 0.63 0.03 0.07 0.13 0.26 0.02 -\\nA3 0.66 0.08 0.13 0.03 0.01 0.02 -\\nLOC 0.36 0.07 0.02 0.05 0.03 0.01 -\\nTMP 0.78 - 0.01 0.01 - - 0.01\\nMNR 0.24 0.05 0.01 - 0.03 - -\\nCount 4932 715 495 358 119 103 79\\noffer little to no inductive bias for NomBank relations. Conversely, the mapping from NomBank to\\nPCEDT shows that although many NomBank arguments map to RSTR in PCEDT, the percentages\\nare lower, making the mapping more diverse and discriminative, which seems to aid TL and MTL\\nmodels in learning less frequent PCEDT relations.\\nTo understand why the PCEDT functor AIM is never predicted despite being more frequent than\\nTWHEN, we found that AIM is almost always misclassified as RSTR by all models. Furthermore,\\nAIM and RSTR have the highest lexical overlap in the training set among all PCEDT relation pairs:\\n78.35% of left constituents and 73.26% of right constituents of compounds annotated as AIM occur\\nin other compounds annotated as RSTR. This explains the models’ inability to learn AIM but raises\\nquestions about their ability to learn relational representations, which we explore further in Section\\n7.3.\\nTable 8: Macro-average F1 score on the test split.\\nModel NomBank PCEDT\\nSTL 52.66 40.15\\nTLE 52.83 48.34\\nTLH 52.98 46.52\\nTLEH 53.31 47.12\\nMTLE 53.21 47.23\\nMTLF 42.07 40.73\\nFinally, to demonstrate the benefits of TL and MTL for NomBank and PCEDT, we report the F1\\nmacro-average scores in Table 8. This is arguably the appropriate evaluation measure for imbalanced\\nclassification problems. Note that relations not predicted by any model are excluded from the macro-\\naverage calculation. Table 8 clearly shows that TL and MTL on the embedding layer yield significant\\nimprovements for PCEDT, with about a 7-8 point increase in macro-average F1, compared to just\\n0.65 in the best case for NomBank.\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 7}, page_content='7.3 Generalization on Unseen Compounds\\nWe now analyze the models’ ability to generalize to compounds not seen during training. Recent\\nresearch suggests that gains in noun-noun compound interpretation using word embeddings and\\nsimilar neural classification models might be due to lexical memorization. In other words, the models\\nlearn that specific nouns are strong indicators of specific relations. To assess the role of lexical\\nmemorization in our models, we quantify the number of unseen compounds that the STL, TL, and\\nMTL models predict correctly.\\nWe differentiate between ’partly’ and ’completely’ unseen compounds. A compound is ’partly’\\nunseen if one of its constituents (left or right) is not present in the training data. A ’completely’\\nunseen compound is one where neither the left nor the right constituent appears in the training data.\\nOverall, nearly 20% of the compounds in the test split have an unseen left constituent, about 16%\\nhave an unseen right constituent, and 4% are completely unseen. Table 9 compares the performance\\nof the different models on these three groups in terms of the proportion of compounds misclassified\\nin each group.\\nTable 9: Generalization error on the subset of unseen compounds in the test split. L: Left constituent.\\nR: Right constituent. L&R: Completely unseen.\\nNomBank PCEDT\\nModel L R L&R L R L&R\\nCount 351 286 72 351 286 72\\nSTL 27.92 39.51 50.00 45.01 47.55 41.67\\nTLE 25.93 36.71 48.61 43.87 47.55 41.67\\nTLH 26.21 38.11 50.00 46.15 49.30 47.22\\nTLEH 26.50 38.81 52.78 45.87 47.55 43.06\\nMTLE 24.50 33.22 38.89 44.44 47.20 43.06\\nMTLF 22.79 34.27 40.28 44.16 47.90 38.89\\nTable 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce\\ngeneralization error in NomBank across all scenarios, with the exception of TLH and TLEH for\\ncompletely unseen compounds, where error increases. The greatest error reductions are achieved\\nby MTL models across all three types of unseen compounds. Specifically, MTLE reduces the error\\nby approximately six points for compounds with unseen right constituents and by eleven points for\\nfully unseen compounds. Moreover, MTLF reduces the error by five points when the left constituent\\nis unseen. It’s important to interpret these results in conjunction with the Count row in Table 9 for\\na comprehensive view. For example, the eleven-point error decrease in fully unseen compounds\\nrepresents eight compounds. In PCEDT, the largest error reduction is on unseen left constituents,\\nwhich is about 1.14 points, corresponding to four compounds; it’s 0.35 on unseen right constituents\\n(one compound) and 2.7 on fully unseen compounds, or two compounds.\\nUpon manual inspection of compounds that led to substantial reductions in the generalization error,\\nspecifically within NomBank, we examined the distribution of relations within correctly predicted\\nunseen compound sets. Compared to the STL model, MTLE reduces generalization error for\\ncompletely unseen compounds by a total of eight compounds, of which seven are annotated with the\\nrelation ARG1, which is the most common in NomBank. Regarding the unseen right constituents,\\nMTLE’s 24 improved compounds consist of 18 ARG1, 5 ARG0, and 1 ARG2 compounds. A\\nsimilar pattern arises when examining TLE model improvements, where most gains come from better\\npredictions of ARG1 and ARG0 relations.\\nA large portion of unseen compounds, whether partly or entirely unseen, that were misclassified by\\nevery model, were not of type ARG1 in NomBank, or RSTR in PCEDT. This pattern, along with\\ncorrectly predicted unseen compounds primarily annotated with the most common relations, suggests\\nthat classification models rely on lexical memorization to learn the compound relation interpretation.\\nTo better comprehend lexical memorization’s impact, we present the ratio of relation-specific con-\\nstituents in both NomBank and PCEDT, as depicted in Figure 2. We define a relation-specific\\nconstituent as a left or right constituent that appears with only one specific relation within the training\\ndata. Its ratio is calculated as its proportion in the full set of left or right constituents for each\\n8'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 8}, page_content='relation. Analyzing Figure 2 reveals that NomBank relations possess higher ratios of relation-specific\\nconstituents compared to PCEDT. This potentially makes learning the former easier if the model\\nsolely relies on lexical memorization. Additionally, ARGM-TMP in NomBank and TWHEN in\\nPCEDT have distinctly high ratios compared to other relations in Figure 2. These relations also\\nhave the second-highest F1 score in their datasets—except for STL on PCEDT (see Tables 4 and\\n5). Lexical memorization is therefore a likely cause of these high F1 scores. We also observed that\\nlower ratios of relation-specific constituents correlate with lower F1 scores, such as APP and REG in\\nPCEDT. Based on these insights, we can’t dismiss the possibility that our models show some degree\\nof lexical memorization, despite manual analysis also presenting cases where models demonstrate\\ngeneralization and correct predictions in situations where lexical memorization is impossible.\\n8 Conclusion\\nThe application of transfer and multi-task learning in natural language processing has gained sig-\\nnificant traction, yet considerable ambiguity persists regarding the effectiveness of particular task\\ncharacteristics and experimental setups. This research endeavors to clarify the benefits of TL and\\nMTL in the context of semantic interpretation of noun-noun compounds. By executing a sequence of\\nminimally contrasting experiments and conducting thorough analysis of results and prediction errors,\\nwe demonstrate how both TL and MTL can mitigate the effects of class imbalance and drastically\\nenhance predictions for low-frequency relations. Overall, our TL, and particularly our MTL models,\\nare better at making predictions both quantitatively and qualitatively. Notably, the improvements are\\nobserved on the ’most challenging’ inputs that include at least one constituent that was not present in\\nthe training data. However, clear indications of ’lexical memorization’ effects are evident in our error\\nanalysis of unseen compounds.\\nTypically, the transfer of representations or sharing between tasks is more effective at the embedding\\nlayers, which represent the model’s internal representation of the compound constituents. Furthermore,\\nin multi-task learning, the complete sharing of model architecture across tasks degrades its capacity\\nto generalize when it comes to less frequent relations.\\nThe dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound\\ninterpretation because it links this sub-problem with broad-coverage semantic role labeling or\\nsemantic dependency parsing in PCEDT and NomBank. Future research will focus on incorporating\\nadditional natural language processing tasks defined using these frameworks to understand noun-noun\\ncompound interpretation using TL and MTL.\\n9')]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt7NfrluV6m9",
        "outputId": "70d95b7a-f5bd-4f79-f527-ed8bfaa00ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.3.29)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.14)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.10.3)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.23.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.3)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.2.2)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "fqW9UpoNVHQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "2ynZqimtqSEb",
        "outputId": "41677fb6-9a21-4e08-fb54-ad878c2c7103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.27.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.3.29)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (3.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "ApaTXsekqUXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "OjHmENBlqYfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "rHsF57cXq-HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "CPlVXQhHrxr5",
        "outputId": "288adf0b-3118-4797-e5a8-7acdf88f3457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "569fda7db5344814af2a1eceaeba3128",
            "306bc8af4a5d4cf9b2fc37f0f5d7c0b6",
            "ce9eea41471f481c8ad62421cda068a8",
            "e9fa5e11f4b04d4b942d6aa081dfd2df",
            "5aefff37480a463a83f03fe61ecba183",
            "edaed9cfaaaf4524b03746d327fa39e0",
            "7fc1c0edf94749118a301d2bc1181eab",
            "002164072a6f4df38326401243166da5",
            "52666c944c5941218b04289a3dea79bb",
            "cad8297c31dc4eb9b14fe2059b9a2348",
            "9a709b6d3f0b49ebb9b7e302f3b17719",
            "d7190309e6fd4a2099892962c22651be",
            "c8452b355c144e7fac57ea7b07937d64",
            "d690e5c8b80a4f81882181145a82e277",
            "2fc4422ef4234548aa92c749eb88be08",
            "a4dc6d6d30b248fb828e0158164c3278",
            "7ac0690a369c464da3f00e2518f2d2dd",
            "b247846bcd3342779058c620fe5f28cf",
            "f3bc6539a69d454ca23b3175b02899f5",
            "1d338d73a51a4c07b070aadfcaa24a2d",
            "49f5864e96a7410595262520daa82d43",
            "9d2916fe0c884c2b82277586abe5872f",
            "c1f26e64905045c490c024ffc82fbc3e",
            "825b269911cc4dd9b3d0d50b103fdc1b",
            "fb8577bb624c4a109eee888b0b912b3b",
            "1dd660f7c1e54d9a8b22e71e549529d0",
            "a6897d49da0a4d5484a28bcc2d776441",
            "38ca7b60944a40769ad940532133d778",
            "abdecddf210c45f09c4c4fb1fa08e7a5",
            "e392cac47e0a4c1bb31b37266f3ff69d",
            "81cf8771c50349f3a891c23b51756afc",
            "91ca4396be4c4889a968e5258efcdd8c",
            "017458f850c3422b8bce0bfb479a35d8",
            "1bb2210a560b469fae1da2c063ac030b",
            "0cab713f204747c4a0f9f73868da3916",
            "7907661bc0654d99b94788f83c284bab",
            "1eca0816e28540aeac586c68c9bdd29a",
            "955364f45fd14b92867a1a06bfe08d77",
            "d3d3b31d988c4be1a766fe3f62069453",
            "2df935a7c4a14b39b845ad6c6e6a57c9",
            "42c81adde5ce4933a456d66c47856f4e",
            "b8c6b1ba80504779b341e6755c8abedb",
            "f02e6cff03e54b3bb07711d4924e4bf2",
            "d7bb742b1954493eaac42d1cec4596f5",
            "d4defa9a2a244ee1892cacfeba8ce0ed",
            "ffd02cc04be049e1afa5a9e3d797814d",
            "46fceb66b96f44aba7a54ba574b0c29d",
            "d43e45eb9115445ba514ca7450a1b5de",
            "597d217d58d243509539fc6dcff5d033",
            "e76db00ecbe54aeda02183acc590ed66",
            "4d2f7394d05c4a88af1dfd519092d9a9",
            "d7f705415d84406dac00085b0fba381e",
            "eaa24e19df3a427cb730bdac69db014f",
            "2247abc69733433684ed195fad595415",
            "eaa54249dd6a4dd8bf12a8f1e239e55c",
            "920ddc375a814854a3239db968af001b",
            "13de84b9c5da454ebfce26d0ab3b155d",
            "ac83c71942b544679e93c3df6bbfd8c6",
            "f225fe2222c14a5887e955445b1c4391",
            "be0e56cc1c1b403bb5f2d31a35a258bc",
            "ece383bb40cb44cbb498ebbca5d65190",
            "cf4bba06f13f400baa31cce26091394a",
            "8be93917218449c18f19086702f74062",
            "5ef47d87512d4a2b93bf75961058f384",
            "3efd9273d6d1405a8959bc84733dfbc5",
            "c59df1a4753c4f899cc905cb8032075f",
            "705e9f0a93374b7790f675ce11b5378c",
            "30273d88e9e24b46881d58758f09863d",
            "e14f2b58f0344bab9b558a28afad8de3",
            "0cf92667e9444798af8b36df9c52a88c",
            "c29fcc4eea9e4348a60b133142129238",
            "fd750e90dd074ed182fd8ce58aea4df1",
            "09670c0725a746a3a4bfe14cf066b98d",
            "646bbe720dcd43d6bb9d693a6eef6570",
            "7bdf859445d7453cb8d9593a6793b2db",
            "12583b3e493e4b58a40fcadca05ffcd9",
            "9be039b11d3648bc9869a7bc920856c0",
            "29402f16b56744f293f3f51f9e499694",
            "7f6b49db3aff4a5fb87653dbf2ea56b0",
            "3154fa83295d4690bcf16a3b5857d685",
            "b9eafa9d6b3347dfb94940f03cb06c6a",
            "e65cf06eddfc42c393038fb273f5ddb4",
            "69f306888c52451e9ff9e0d661c82ad2",
            "eaabf3522d1c4a0f8c29b7faf2956021",
            "ae24c06dc8564dab8cd533761c26a668",
            "3b4027fcccb44ee0b935a3a8698783b0",
            "b747fbc6e57d4af499582f20f1aa1171",
            "10e73b3d37c64d46970f9d0a6c157848",
            "4cb88c66923548db8b3b7afa76ae49be",
            "3ceddc3f1b22426aaa96ece97b6b929c",
            "10e26108b01a45cea03a36f035370012",
            "e46e13c0cdf140839624945effdbf69d",
            "d2c3f2125be843019d0e5abd61ef957d",
            "e777a7426598411a902a14bcd235e04e",
            "4734db1a7f4849268fa23fd1cad3d6d0",
            "b5a049c12ad749c9ae6b44c00d426610",
            "a7dc10f2bb7240fea154adfec54af70e",
            "18961f6d8cd0401989216fe5eac6e4ce",
            "9ecd11dcf21a47958d04515ba72791bd",
            "83aff04d816f4d6a897892a79cb847de",
            "22b673e0879946bca4ee0d8e634732d7",
            "8040eb7863f641d3ad7086c0ae581fd9",
            "e0333433064949278c035e2e52ce60ab",
            "49dff49e40fb44aaa34710825ddfae74",
            "ee1a463fc8cd499c94628e98ec52f006",
            "bde0bde94bb84e96bdc7716b2a622387",
            "228d2550de404a018f52c2a76ca9cf54",
            "bffade15395c40019d401bb0c1803c5d",
            "33635f155ef44d9380b7fd3aadd7460e",
            "5f46e184031d490eb6b0bbad62ed906b",
            "7658516b4c0b47f78905e4eb56594992",
            "403b1765fe4841a4b64eed282cb0f40d",
            "1e5da9483ff14fa19347b9944413e976",
            "3ca1c509c411484f83658f33a4bbae36",
            "02d4ea757d444e0f905195d97af0755d",
            "95441b82db094152979b9303115eca9d",
            "37edfea6176247fd91c4193610ccb29f",
            "ce33b218157a49f89f7715c39c9c15f5",
            "7d948ea8167543a1995bea6b9b0571ef",
            "7e262467279d443f9485e068a5d552de",
            "a1c665cee85a4cb79526897fbea60d01"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "569fda7db5344814af2a1eceaeba3128"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7190309e6fd4a2099892962c22651be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1f26e64905045c490c024ffc82fbc3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bb2210a560b469fae1da2c063ac030b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4defa9a2a244ee1892cacfeba8ce0ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "920ddc375a814854a3239db968af001b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "705e9f0a93374b7790f675ce11b5378c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29402f16b56744f293f3f51f9e499694"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cb88c66923548db8b3b7afa76ae49be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83aff04d816f4d6a897892a79cb847de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7658516b4c0b47f78905e4eb56594992"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(document_list)"
      ],
      "metadata": {
        "id": "3hmB_abesNhT",
        "outputId": "57166fa8-b2b6-42be-8339-a863097cca3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_list[0][0].page_content"
      ],
      "metadata": {
        "id": "xP0IHRdo0uJO",
        "outputId": "e5171265-b3b9-4b63-ee22-a650c609abfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Analyzing Real-Time Group Coordination in\\nAugmented Dance Performances: An LSTM-Based\\nGesture Modeling Approach\\nAbstract\\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\\nresearch avenue to explore group cohesion through gesture forecasting. By employ-\\ning LSTM neural networks, this study predicts dancers’ gestures and correlates\\naccuracy with synchronization, emotional expression, and creativity—key cohesion\\nmetrics.\\nA \"virtual flamenco guru\" provides real-time feedback, enhancing synchronization\\nand fostering gesture resonance, where dancers align movements via a shared vir-\\ntual space. AR amplifies this effect, especially with gesture-sensing garments. This\\ninterdisciplinary research highlights flamenco’s cultural depth, therapeutic bene-\\nfits, and technological applications in dance therapy, human-computer interaction,\\nand entertainment, pushing the boundaries of creativity and collective behavior\\nanalysis.\\n1 Introduction\\nThe realm of coordinated dance rituals has long been a fascinating area of study, with the intricate\\npatterns and movements of synchronized performances captivating audiences and inspiring new\\navenues of research. Among the various forms of dance, flamenco stands out for its passionate and\\nexpressive nature, characterized by complex hand and foot movements that require a high degree of\\ncoordination and timing. Recent advancements in augmented reality (AR) technology have opened\\nup new possibilities for enhancing and analyzing these performances, allowing for the creation of\\nimmersive and interactive experiences that blur the lines between the physical and virtual worlds.\\nOne of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\\nlevel of group cohesion among the performers. This can be a difficult task, as it requires measuring\\nthe complex interactions and relationships between individual dancers, as well as their ability to work\\ntogether as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can\\nprovide some insight into the dynamics of the group, but they are often limited by their subjective\\nnature and inability to capture the nuances of nonverbal communication.\\nIn response to these limitations, researchers have begun to explore the use of machine learning\\nalgorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\\nand movements of dancers. These models have shown great promise in their ability to learn and\\npredict complex patterns of movement, allowing for a more objective and quantitative assessment\\nof group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper\\nunderstanding of the factors that contribute to successful coordinated dance performances, and\\ndevelop new strategies for improving the cohesion and effectiveness of dance groups.\\nHowever, the application of LSTM-based gesture forecasting to coordinated dance rituals is not\\nwithout its challenges. One of the most significant difficulties is the need to develop a system that\\ncan accurately capture and interpret the complex movements and gestures of the dancers. This\\nrequires the creation of sophisticated sensors and data collection systems, capable of tracking the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_list[0][0].page_content"
      ],
      "metadata": {
        "id": "EbLcbF-Pu_kQ",
        "outputId": "8a37fdd2-8a8f-4f45-800b-062a133cbcb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Analyzing Real-Time Group Coordination in\\nAugmented Dance Performances: An LSTM-Based\\nGesture Modeling Approach\\nAbstract\\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\\nresearch avenue to explore group cohesion through gesture forecasting. By employ-\\ning LSTM neural networks, this study predicts dancers’ gestures and correlates\\naccuracy with synchronization, emotional expression, and creativity—key cohesion\\nmetrics.\\nA \"virtual flamenco guru\" provides real-time feedback, enhancing synchronization\\nand fostering gesture resonance, where dancers align movements via a shared vir-\\ntual space. AR amplifies this effect, especially with gesture-sensing garments. This\\ninterdisciplinary research highlights flamenco’s cultural depth, therapeutic bene-\\nfits, and technological applications in dance therapy, human-computer interaction,\\nand entertainment, pushing the boundaries of creativity and collective behavior\\nanalysis.\\n1 Introduction\\nThe realm of coordinated dance rituals has long been a fascinating area of study, with the intricate\\npatterns and movements of synchronized performances captivating audiences and inspiring new\\navenues of research. Among the various forms of dance, flamenco stands out for its passionate and\\nexpressive nature, characterized by complex hand and foot movements that require a high degree of\\ncoordination and timing. Recent advancements in augmented reality (AR) technology have opened\\nup new possibilities for enhancing and analyzing these performances, allowing for the creation of\\nimmersive and interactive experiences that blur the lines between the physical and virtual worlds.\\nOne of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\\nlevel of group cohesion among the performers. This can be a difficult task, as it requires measuring\\nthe complex interactions and relationships between individual dancers, as well as their ability to work\\ntogether as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can\\nprovide some insight into the dynamics of the group, but they are often limited by their subjective\\nnature and inability to capture the nuances of nonverbal communication.\\nIn response to these limitations, researchers have begun to explore the use of machine learning\\nalgorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\\nand movements of dancers. These models have shown great promise in their ability to learn and\\npredict complex patterns of movement, allowing for a more objective and quantitative assessment\\nof group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper\\nunderstanding of the factors that contribute to successful coordinated dance performances, and\\ndevelop new strategies for improving the cohesion and effectiveness of dance groups.\\nHowever, the application of LSTM-based gesture forecasting to coordinated dance rituals is not\\nwithout its challenges. One of the most significant difficulties is the need to develop a system that\\ncan accurately capture and interpret the complex movements and gestures of the dancers. This\\nrequires the creation of sophisticated sensors and data collection systems, capable of tracking the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_list[1]"
      ],
      "metadata": {
        "id": "nfufHQp4x89y",
        "outputId": "a0526a23-a170-47d1-fd27-660bc3dc4c18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 0}, page_content='Transdimensional Properties of Graphite in Relation\\nto Cheese Consumption on Tuesday Afternoons\\nAbstract\\nGraphite research has led to discoveries about dolphins and their penchant for\\ncollecting rare flowers, which bloom only under the light of a full moon, while\\nsimultaneously revealing the secrets of dark matter and its relation to the perfect\\nrecipe for chicken parmesan, as evidenced by the curious case of the missing socks\\nin the laundry basket, which somehow correlates with the migration patterns of but-\\nterflies and the art of playing the harmonica underwater, where the sounds produced\\nare eerily similar to the whispers of ancient forests, whispering tales of forgotten\\ncivilizations and their advanced understanding of quantum mechanics, applied to\\nthe manufacture of sentient toasters that can recite Shakespearean sonnets, all of\\nwhich is connected to the inherent properties of graphite and its ability to conduct\\nthe thoughts of extraterrestrial beings, who are known to communicate through a\\ncomplex system of interpretive dance and pastry baking, culminating in a profound\\nunderstanding of the cosmos, as reflected in the intricate patterns found on the\\nsurface of a butterfly’s wings, and the uncanny resemblance these patterns bear to\\nthe molecular structure of graphite, which holds the key to unlocking the secrets of\\ntime travel and the optimal method for brewing coffee.\\n1 Introduction\\nThe fascinating realm of graphite has been juxtaposed with the intricacies of quantum mechanics,\\nwherein the principles of superposition and entanglement have been observed to influence the baking\\nof croissants, a phenomenon that warrants further investigation, particularly in the context of flaky\\npastry crusts, which, incidentally, have been found to exhibit a peculiar affinity for the sonnets\\nof Shakespeare, specifically Sonnet 18, whose themes of beauty and mortality have been linked\\nto the existential implications of graphitic carbon, a subject that has garnered significant attention\\nin recent years, notwithstanding the fact that the aerodynamic properties of graphite have been\\nstudied extensively in relation to the flight patterns of migratory birds, such as the Arctic tern, which,\\nintriguingly, has been known to incorporate graphite particles into its nest-building materials, thereby\\npotentially altering the structural integrity of the nests, a consideration that has led researchers to\\nexplore the role of graphite in the development of more efficient wind turbine blades, an application\\nthat has been hindered by the limitations of current manufacturing techniques, which, paradoxically,\\nhave been inspired by the ancient art of Egyptian hieroglyphics, whose symbolic representations of\\ngraphite have been interpreted as a harbinger of good fortune, a notion that has been debunked by\\nscholars of ancient mythology, who argue that the true significance of graphite lies in its connection to\\nthe mythological figure of the phoenix, a creature whose cyclical regeneration has been linked to the\\nunique properties of graphitic carbon, including its exceptional thermal conductivity, which, curiously,\\nhas been found to be inversely proportional to the number of times one listens to the music of Mozart,\\na composer whose works have been shown to have a profound impact on the crystalline structure of\\ngraphite, causing it to undergo a phase transition from a hexagonal to a cubic lattice, a phenomenon\\nthat has been observed to occur spontaneously in the presence of a specific type of fungus, whose\\nmycelium has been found to exhibit a peculiar affinity for the works of Kafka, particularly \"The\\nMetamorphosis,\" whose themes of transformation and identity have been linked to the ontological\\nimplications of graphitic carbon, a subject that has been explored extensively in the context of'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 1}, page_content='postmodern philosophy, where the notion of graphite as a metaphor for the human condition has been\\nproposed, an idea that has been met with skepticism by critics, who argue that the true significance\\nof graphite lies in its practical applications, such as its use in the manufacture of high-performance\\nsports equipment, including tennis rackets and golf clubs, whose aerodynamic properties have been\\noptimized through the strategic incorporation of graphite particles, a technique that has been inspired\\nby the ancient art of Japanese calligraphy, whose intricate brushstrokes have been found to exhibit a\\npeculiar similarity to the fractal patterns observed in the microstructure of graphite, a phenomenon\\nthat has been linked to the principles of chaos theory, which, incidentally, have been applied to the\\nstudy of graphitic carbon, revealing a complex web of relationships between the physical properties\\nof graphite and the abstract concepts of mathematics, including the Fibonacci sequence, whose\\nnumerical patterns have been observed to recur in the crystalline structure of graphite, a discovery\\nthat has led researchers to propose a new theory of graphitic carbon, one that integrates the principles\\nof physics, mathematics, and philosophy to provide a comprehensive understanding of this enigmatic\\nmaterial, whose mysteries continue to inspire scientific inquiry and philosophical contemplation,\\nmuch like the allure of a siren’s song, which, paradoxically, has been found to have a profound\\nimpact on the electrical conductivity of graphite, causing it to undergo a sudden and inexplicable\\nincrease in its conductivity, a phenomenon that has been observed to occur in the presence of a\\nspecific type of flower, whose petals have been found to exhibit a peculiar affinity for the works\\nof Dickens, particularly \"Oliver Twist,\" whose themes of poverty and redemption have been linked\\nto the social implications of graphitic carbon, a subject that has been explored extensively in the\\ncontext of economic theory, where the notion of graphite as a catalyst for social change has been\\nproposed, an idea that has been met with enthusiasm by advocates of sustainable development, who\\nargue that the strategic incorporation of graphite into industrial processes could lead to a significant\\nreduction in carbon emissions, a goal that has been hindered by the limitations of current technologies,\\nwhich, ironically, have been inspired by the ancient art of alchemy, whose practitioners believed in\\nthe possibility of transforming base metals into gold, a notion that has been debunked by modern\\nscientists, who argue that the true significance of graphite lies in its ability to facilitate the transfer\\nof heat and electricity, a property that has been exploited in the development of advanced materials,\\nincluding nanocomposites and metamaterials, whose unique properties have been found to exhibit a\\npeculiar similarity to the mythological figure of the chimera, a creature whose hybrid nature has been\\nlinked to the ontological implications of graphitic carbon, a subject that has been explored extensively\\nin the context of postmodern philosophy, where the notion of graphite as a metaphor for the human\\ncondition has been proposed, an idea that has been met with skepticism by critics, who argue that\\nthe true significance of graphite lies in its practical applications, such as its use in the manufacture\\nof high-performance sports equipment, including tennis rackets and golf clubs, whose aerodynamic\\nproperties have been optimized through the strategic incorporation of graphite particles, a technique\\nthat has been inspired by the ancient art of Japanese calligraphy, whose intricate brushstrokes have\\nbeen found to exhibit a peculiar similarity to the fractal patterns observed in the microstructure of\\ngraphite.\\nThe study of graphitic carbon has been influenced by a wide range of disciplines, including physics,\\nchemistry, materials science, and philosophy, each of which has contributed to our understanding\\nof this complex and enigmatic material, whose properties have been found to exhibit a peculiar\\nsimilarity to the principles of quantum mechanics, including superposition and entanglement, which,\\nincidentally, have been observed to influence the behavior of subatomic particles, whose wave\\nfunctions have been found to exhibit a peculiar affinity for the works of Shakespeare, particularly\\n\"Hamlet,\" whose themes of uncertainty and doubt have been linked to the existential implications of\\ngraphitic carbon, a subject that has been explored extensively in the context of postmodern philosophy,\\nwhere the notion of graphite as a metaphor for the human condition has been proposed, an idea that\\nhas been met with enthusiasm by advocates of existentialism, who argue that the true significance of\\ngraphite lies in its ability to inspire philosophical contemplation and introspection, a notion that has\\nbeen supported by the discovery of a peculiar correlation between the structure of graphitic carbon\\nand the principles of chaos theory, which, paradoxically, have been found to exhibit a similarity\\nto the mythological figure of the ouroboros, a creature whose cyclical nature has been linked to\\nthe ontological implications of graphitic carbon, a subject that has been explored extensively in\\nthe context of ancient mythology, where the notion of graphite as a symbol of transformation and\\nrenewal has been proposed, an idea that has been met with skepticism by critics, who argue that the\\ntrue significance of graphite lies in its practical applications, such as its use in the manufacture of\\nhigh-performance sports equipment, including tennis rackets and golf clubs, whose aerodynamic\\n2'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 2}, page_content='properties have been optimized through the strategic incorporation of graphite particles, a technique\\nthat has been inspired by the ancient art of Egyptian hieroglyphics, whose symbolic representations\\nof graphite have been interpreted as a harbinger of good fortune, a notion that has been debunked by\\nscholars of ancient mythology, who argue that the true significance of graphite lies in its connection\\nto the mythological figure of the phoenix, a creature whose cyclical regeneration has been linked\\nto the unique properties of graphitic carbon, including its exceptional thermal conductivity, which,\\ncuriously, has been found to be inversely proportional to the number of times one listens to the music\\nof Mozart, a composer whose works have been shown to have a profound impact on the crystalline\\nstructure of graphite, causing it to undergo a phase transition from a hexagonal to a cubic lattice,\\na phenomenon that has been observed to occur spontaneously in the presence of a specific type\\nof fungus, whose mycelium has been found to exhibit a peculiar affinity for the works of Kafka,\\nparticularly \"The Metamorphosis,\" whose themes of transformation and identity have been linked to\\nthe ontological implications of graphitic carbon, a subject that has been explored extensively in the\\ncontext of postmodern philosophy, where the notion of graphite as a metaphor for the human condition\\nhas been proposed, an idea that has been met with enthusiasm by advocates of existentialism, who\\nargue that the true significance of graphite lies in its ability to inspire philosophical contemplation\\nand introspection.\\nThe properties of graphitic carbon have been found to exhibit a peculiar similarity to the principles of\\nfractal geometry, whose self-similar patterns have been observed to recur in the microstructure of\\ngraphite, a phenomenon that has been linked to the principles of chaos theory, which, incidentally,\\nhave been applied to the study of graphitic carbon, revealing a complex web of relationships between\\nthe physical properties of graphite and the abstract concepts of mathematics, including the Fibonacci\\nsequence, whose numerical patterns have been observed to recur in the crystalline structure of\\ngraphite, a discovery that has led researchers to propose a new theory of graphitic carbon, one\\nthat integrates the principles of physics, mathematics, and philosophy to provide a comprehensive\\nunderstanding of this enigmatic material, whose mysteries continue to inspire scientific inquiry and\\nphilosophical contemplation, much like the allure of a siren’s song, which, paradoxically, has been\\nfound to have a profound impact on the electrical conductivity of graphite, causing it to undergo a\\nsudden and inexplicable increase in its conductivity, a phenomenon that has been observed to occur\\nin the presence of a specific type of flower, whose petals have been found to exhibit a peculiar affinity\\nfor the works of Dickens, particularly \"Oliver Twist,\" whose themes of poverty\\n2 Related Work\\nThe discovery of graphite has been linked to the migration patterns of Scandinavian furniture\\ndesigners, who inadvertently stumbled upon the mineral while searching for novel materials to\\ncraft avant-garde chair legs. Meanwhile, the aerodynamics of badminton shuttlecocks have been\\nshown to influence the crystalline structure of graphite, particularly in high-pressure environments.\\nFurthermore, an exhaustive analysis of 19th-century French pastry recipes has revealed a correlation\\nbetween the usage of graphite in pencil lead and the popularity of croissants among the aristocracy.\\nThe notion that graphite exhibits sentient properties has been debated by experts in the field of chrono-\\nbotany, who propose that the mineral’s conductivity is, in fact, a form of inter-species communication.\\nConversely, researchers in the field of computational narwhal studies have demonstrated that the\\nspiral patterns found on narwhal tusks bear an uncanny resemblance to the molecular structure of\\ngraphite. This has led to the development of novel narwhal-based algorithms for simulating graphite’s\\nthermal conductivity, which have been successfully applied to the design of more efficient toaster\\ncoils.\\nIn a surprising turn of events, the intersection of graphite and Byzantine mosaic art has yielded\\nnew insights into the optical properties of the mineral, particularly with regards to its reflectivity\\nunder various lighting conditions. This, in turn, has sparked a renewed interest in the application of\\ngraphite-based pigments in the restoration of ancient frescoes, as well as the creation of more durable\\nand long-lasting tattoos. Moreover, the intricate patterns found in traditional Kenyan basket-weaving\\nhave been shown to possess a fractal self-similarity to the atomic lattice structure of graphite, leading\\nto the development of novel basket-based composites with enhanced mechanical properties.\\nThe putative connection between graphite and the migratory patterns of North American monarch\\nbutterflies has been explored in a series of exhaustive studies, which have conclusively demonstrated\\n3'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 3}, page_content='that the mineral plays a crucial role in the butterflies’ ability to navigate across vast distances.\\nIn a related development, researchers have discovered that the sound waves produced by graphitic\\nmaterials under stress bear an uncanny resemblance to the haunting melodies of traditional Mongolian\\nthroat singing, which has inspired a new generation of musicians to experiment with graphite-based\\ninstruments.\\nAn in-depth examination of the linguistic structure of ancient Sumerian pottery inscriptions has\\nrevealed a hitherto unknown connection to the history of graphite mining in 17th-century Cornwall,\\nwhere the mineral was prized for its ability to enhance the flavor of locally brewed ale. Conversely,\\nthe aerodynamics of 20th-century supersonic aircraft have been shown to be intimately linked to\\nthe thermal expansion properties of graphite, particularly at high temperatures. This has led to the\\ndevelopment of more efficient cooling systems for high-speed aircraft, as well as a renewed interest in\\nthe application of graphitic materials in the design of more efficient heat sinks for high-performance\\ncomputing applications.\\nThe putative existence of a hidden graphitic quantum realm, where the laws of classical physics\\nare inverted, has been the subject of much speculation and debate among experts in the field of\\ntheoretical spaghetti mechanics. According to this theory, graphite exists in a state of superposition,\\nsimultaneously exhibiting both crystalline and amorphous properties, which has profound implications\\nfor our understanding of the fundamental nature of reality itself. In a related development, researchers\\nhave discovered that the sound waves produced by graphitic materials under stress can be used to\\ncreate a novel form of quantum entanglement-based cryptography, which has sparked a new wave of\\ninterest in the application of graphitic materials in the field of secure communication systems.\\nThe intricate patterns found in traditional Indian mandalas have been shown to possess a frac-\\ntal self-similarity to the atomic lattice structure of graphite, leading to the development of novel\\nmandala-based composites with enhanced mechanical properties. Moreover, the migratory patterns\\nof Scandinavian reindeer have been linked to the optical properties of graphite, particularly with\\nregards to its reflectivity under various lighting conditions. This has inspired a new generation of\\nartists to experiment with graphite-based pigments in their work, as well as a renewed interest in the\\napplication of graphitic materials in the design of more efficient solar panels.\\nIn a surprising turn of events, the intersection of graphite and ancient Egyptian scroll-making has\\nyielded new insights into the thermal conductivity of the mineral, particularly with regards to its\\nability to enhance the flavor of locally brewed coffee. This, in turn, has sparked a renewed interest in\\nthe application of graphite-based composites in the design of more efficient coffee makers, as well as\\na novel form of coffee-based cryptography, which has profound implications for our understanding\\nof the fundamental nature of reality itself. Furthermore, the aerodynamics of 20th-century hot air\\nballoons have been shown to be intimately linked to the sound waves produced by graphitic materials\\nunder stress, which has inspired a new generation of musicians to experiment with graphite-based\\ninstruments.\\nThe discovery of a hidden graphitic code, embedded in the molecular structure of the mineral, has been\\nthe subject of much speculation and debate among experts in the field of crypto-botany. According\\nto this theory, graphite contains a hidden message, which can be deciphered using a novel form of\\ngraphitic-based cryptography, which has sparked a new wave of interest in the application of graphitic\\nmaterials in the field of secure communication systems. In a related development, researchers have\\ndiscovered that the migratory patterns of North American monarch butterflies are intimately linked to\\nthe thermal expansion properties of graphite, particularly at high temperatures.\\nThe putative connection between graphite and the history of ancient Mesopotamian irrigation systems\\nhas been explored in a series of exhaustive studies, which have conclusively demonstrated that the\\nmineral played a crucial role in the development of more efficient irrigation systems, particularly with\\nregards to its ability to enhance the flow of water through narrow channels. Conversely, the sound\\nwaves produced by graphitic materials under stress have been shown to bear an uncanny resemblance\\nto the haunting melodies of traditional Inuit throat singing, which has inspired a new generation of\\nmusicians to experiment with graphite-based instruments. Moreover, the intricate patterns found in\\ntraditional African kente cloth have been shown to possess a fractal self-similarity to the atomic lattice\\nstructure of graphite, leading to the development of novel kente-based composites with enhanced\\nmechanical properties.\\n4'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 4}, page_content='In a surprising turn of events, the intersection of graphite and 19th-century Australian sheep herding\\nhas yielded new insights into the optical properties of the mineral, particularly with regards to its\\nreflectivity under various lighting conditions. This, in turn, has sparked a renewed interest in the\\napplication of graphite-based pigments in the restoration of ancient frescoes, as well as the creation\\nof more durable and long-lasting tattoos. Furthermore, the aerodynamics of 20th-century supersonic\\naircraft have been shown to be intimately linked to the thermal expansion properties of graphite,\\nparticularly at high temperatures, which has inspired a new generation of engineers to experiment\\nwith graphite-based materials in the design of more efficient cooling systems for high-speed aircraft.\\nThe discovery of a hidden graphitic realm, where the laws of classical physics are inverted, has\\nbeen the subject of much speculation and debate among experts in the field of theoretical jellyfish\\nmechanics. According to this theory, graphite exists in a state of superposition, simultaneously\\nexhibiting both crystalline and amorphous properties, which has profound implications for our\\nunderstanding of the fundamental nature of reality itself. In a related development, researchers have\\ndiscovered that the migratory patterns of Scandinavian reindeer are intimately linked to the sound\\nwaves produced by graphitic materials under stress, which has inspired a new generation of musicians\\nto experiment with graphite-based instruments.\\nThe intricate patterns found in traditional Chinese calligraphy have been shown to possess a fractal self-\\nsimilarity to the atomic lattice structure of graphite, leading to the development of novel calligraphy-\\nbased composites with enhanced mechanical properties. Moreover, the putative connection between\\ngraphite and the history of ancient Greek olive oil production has been explored in a series of\\nexhaustive studies, which have conclusively demonstrated that the mineral played a crucial role in the\\ndevelopment of more efficient olive oil extraction methods, particularly with regards to its ability\\nto enhance the flow of oil through narrow channels. Conversely, the aerodynamics of 20th-century\\nhot air balloons have been shown to be intimately linked to the thermal conductivity of graphite,\\nparticularly at high temperatures, which has inspired a new generation of engineers to experiment with\\ngraphite-based materials in the design of more efficient cooling systems for high-altitude balloons.\\nThe discovery of a hidden graphitic code, embedded in the molecular structure of the mineral, has\\nbeen the subject of much speculation and debate among experts in the field of crypto-entomology.\\nAccording to this theory, graphite contains a hidden message, which can be deciphered using a novel\\nform of graphitic-based cryptography, which has sparked a new wave of interest in the application\\nof graphitic materials in the field of secure communication systems. In a related development,\\nresearchers have discovered that the sound waves produced by graphitic materials under stress bear\\nan uncanny resemblance to the haunting melodies of traditional Tibetan throat singing, which has\\ninspired a new generation of musicians to experiment with graphite-based instruments.\\n3 Methodology\\nThe pursuit of understanding graphite necessitates a multidisciplinary approach, incorporatingele-\\nments of quantum physics, pastry arts, and professional snail training. In our investigation, we\\nemployed a novel methodology that involved the simultaneous analysis of graphite samples and\\nthe recitation of 19th-century French poetry. This dual-pronged approach allowed us to uncover\\npreviously unknown relationships between the crystalline structure of graphite and the aerodynamic\\nproperties of certain species of migratory birds. Furthermore, our research team discovered that\\nthe inclusion of ambient jazz music during the data collection process significantly enhanced the\\naccuracy of our results, particularly when the music was played on a vintage harmonica.\\nThe experimental design consisted of a series of intricate puzzles, each representing a distinct aspect of\\ngraphite’s properties, such as its thermal conductivity, electrical resistivity, and capacity to withstand\\nextreme pressures. These puzzles were solved by a team of expert cryptographers, who worked in\\ntandem with a group of professional jugglers to ensure the accurate manipulation of variables and the\\nprecise measurement of outcomes. Notably, our research revealed that the art of juggling is intimately\\nconnected to the study of graphite, as the rhythmic patterns and spatial arrangements of the juggled\\nobjects bear a striking resemblance to the molecular structure of graphite itself.\\nIn addition to the puzzle-solving and juggling components, our methodology also incorporated a\\nthorough examination of the culinary applications of graphite, including its use as a flavor enhancer\\nin certain exotic dishes and its potential as a novel food coloring agent. This led to a fascinating\\ndiscovery regarding the synergistic effects of graphite and cucumber sauce on the human palate,\\n5'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 5}, page_content='which, in turn, shed new light on the role of graphite in shaping the cultural and gastronomical\\nheritage of ancient civilizations. The implications of this finding are far-reaching, suggesting that\\nthe history of graphite is inextricably linked to the evolution of human taste preferences and the\\ndevelopment of complex societal structures.\\nMoreover, our investigation involved the creation of a vast, virtual reality simulation of a graphite\\nmine, where participants were immersed in a highly realistic environment and tasked with extracting\\ngraphite ore using a variety of hypothetical tools and techniques. This simulated mining experience\\nallowed us to gather valuable data on the human-graphite interface, including the psychological\\nand physiological effects of prolonged exposure to graphite dust and the impact of graphite on the\\nhuman immune system. The results of this study have significant implications for the graphite mining\\nindustry, highlighting the need for improved safety protocols and more effective health monitoring\\nsystems for miners.\\nThe application of advanced statistical models and machine learning algorithms to our dataset re-\\nvealed a complex network of relationships between graphite, the global economy, and the migratory\\npatterns of certain species of whales. This, in turn, led to a deeper understanding of the intricate\\nweb of causality that underlies the graphite market, including the role of graphite in shaping inter-\\nnational trade policies and influencing the global distribution of wealth. Furthermore, our analysis\\ndemonstrated that the price of graphite is intimately connected to the popularity of certain genres\\nof music, particularly those that feature the use of graphite-based musical instruments, such as the\\ngraphite-reinforced guitar string.\\nIn an unexpected twist, our research team discovered that the study of graphite is closely tied to the\\nart of professional wrestling, as the physical properties of graphite are eerily similar to those of the\\nhuman body during a wrestling match. This led to a fascinating exploration of the intersection of\\ngraphite and sports, including the development of novel graphite-based materials for use in wrestling\\ncostumes and the application of graphite-inspired strategies in competitive wrestling matches. The\\nfindings of this study have far-reaching implications for the world of sports, suggesting that the\\nproperties of graphite can be leveraged to improve athletic performance, enhance safety, and create\\nnew forms of competitive entertainment.\\nThe incorporation of graphite into the study of ancient mythology also yielded surprising results, as our\\nresearch team uncovered a previously unknown connection between the Greek god of the underworld,\\nHades, and the graphite deposits of rural Mongolia. This led to a deeper understanding of the cultural\\nsignificance of graphite in ancient societies, including its role in shaping mythological narratives,\\ninfluencing artistic expression, and informing spiritual practices. Moreover, our investigation revealed\\nthat the unique properties of graphite make it an ideal material for use in the creation of ritualistic\\nartifacts, such as graphite-tipped wands and graphite-infused ceremonial masks.\\nIn a related study, we examined the potential applications of graphite in the field of aerospace\\nengineering, including its use in the development of advanced propulsion systems, lightweight\\nstructural materials, and high-temperature coatings. The results of this investigation demonstrated\\nthat graphite-based materials exhibit exceptional performance characteristics, including high thermal\\nconductivity, low density, and exceptional strength-to-weight ratios. These properties make graphite\\nan attractive material for use in a variety of aerospace applications, from satellite components to\\nrocket nozzles, and suggest that graphite may play a critical role in shaping the future of space\\nexploration.\\nThe exploration of graphite’s role in shaping the course of human history also led to some unexpected\\ndiscoveries, including the fact that the invention of the graphite pencil was a pivotal moment in\\nthe development of modern civilization. Our research team found that the widespread adoption of\\ngraphite pencils had a profound impact on the dissemination of knowledge, the evolution of artistic\\nexpression, and the emergence of complex societal structures. Furthermore, we discovered that the\\nunique properties of graphite make it an ideal material for use in the creation of historical artifacts,\\nsuch as graphite-based sculptures, graphite-infused textiles, and graphite-tipped writing instruments.\\nIn conclusion, our methodology represents a groundbreaking approach to the study of graphite,\\none that incorporates a wide range of disciplines, from physics and chemistry to culinary arts\\nand professional wrestling. The findings of our research have significant implications for our\\nunderstanding of graphite, its properties, and its role in shaping the world around us. As we continue\\nto explore the mysteries of graphite, we are reminded of the infinite complexity and beauty of this\\n6'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 6}, page_content='fascinating material, and the many wonders that await us at the intersection of graphite and human\\ningenuity.\\nThe investigation of graphite’s potential applications in the field of medicine also yielded some\\nremarkable results, including the discovery that graphite-based materials exhibit exceptional bio-\\ncompatibility, making them ideal for use in the creation of medical implants, surgical instruments,\\nand diagnostic devices. Our research team found that the unique properties of graphite make it an\\nattractive material for use in a variety of medical applications, from tissue engineering to pharmaceu-\\ntical delivery systems. Furthermore, we discovered that the incorporation of graphite into medical\\ndevices can significantly enhance their performance, safety, and efficacy, leading to improved patient\\noutcomes and more effective treatments.\\nThe study of graphite’s role in shaping the course of modern art also led to some fascinating\\ndiscoveries, including the fact that many famous artists have used graphite in their works, often in\\ninnovative and unconventional ways. Our research team found that the unique properties of graphite\\nmake it an ideal material for use in a variety of artistic applications, from drawing and sketching\\nto sculpture and installation art. Furthermore, we discovered that the incorporation of graphite\\ninto artistic works can significantly enhance their emotional impact, aesthetic appeal, and cultural\\nsignificance, leading to a deeper understanding of the human experience and the creative process.\\nIn a related investigation, we examined the potential applications of graphite in the field of envi-\\nronmental sustainability, including its use in the creation of green technologies, renewable energy\\nsystems, and eco-friendly materials. The results of this study demonstrated that graphite-based\\nmaterials exhibit exceptional performance characteristics, including high thermal conductivity, low\\ntoxicity, and exceptional durability. These properties make graphite an attractive material for use in a\\nvariety of environmental applications, from solar panels to wind turbines, and suggest that graphite\\nmay play a critical role in shaping the future of sustainable development.\\nThe exploration of graphite’s role in shaping the course of human consciousness also led to some\\nunexpected discoveries, including the fact that the unique properties of graphite make it an ideal\\nmaterial for use in the creation of spiritual artifacts, such as graphite-tipped wands, graphite-infused\\nmeditation beads, and graphite-based ritualistic instruments. Our research team found that the\\nincorporation of graphite into spiritual practices can significantly enhance their efficacy, leading to\\ndeeper states of meditation, greater spiritual awareness, and more profound connections to the natural\\nworld. Furthermore, we discovered that the properties of graphite make it an attractive material for\\nuse in the creation of psychedelic devices, such as graphite-based hallucinogenic instruments, and\\ngraphite-infused sensory deprivation tanks.\\nThe application of advanced mathematical models to our dataset revealed a complex network of\\nrelationships between graphite, the human brain, and the global economy. This, in turn, led to a\\ndeeper understanding of the intricate web of causality that underlies the graphite market, including the\\nrole of graphite in shaping international trade policies, influencing the global distribution of wealth,\\nand informing economic decision-making. Furthermore, our analysis demonstrated that the price of\\ngraphite is intimately connected to the popularity of certain genres of literature, particularly those\\nthat feature the use of graphite-based writing instruments, such as the graphite-reinforced pen nib.\\nIn an unexpected twist, our research team discovered that the study of graphite is closely tied to\\nthe art of professional clowning, as the physical properties of graphite are eerily similar to those\\nof the human body during a clowning performance. This led to a fascinating exploration of the\\nintersection of graphite and comedy, including the development of novel graphite-based materials\\nfor use in clown costumes, the application of graphite-inspired strategies in competitive clowning\\nmatches, and the creation of graphite-themed clown props, such as graphite-tipped rubber chickens\\nand graphite-infused squirt guns.\\nThe incorporation of graphite into the study of ancient mythology also yielded surprising results, as\\nour research team uncovered a previously unknown connection between the Egyptian god of wisdom,\\nThoth, and the graphite deposits of rural Peru. This led to a deeper understanding of the cultural\\nsignificance of graphite in ancient societies, including its role in shaping mythological narratives,\\ninfluencing artistic expression, and informing spiritual practices. Moreover, our investigation revealed\\nthat the unique properties of graphite make it an ideal material for use in the creation of ritualistic\\nartifacts, such\\n7'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 7}, page_content='4 Experiments\\nThe preparation of graphite samples involved a intricate dance routine, carefully choreographed to\\nensure the optimal alignment of carbon atoms, which surprisingly led to a discussion on the aerody-\\nnamics of flying squirrels and their ability to navigate through dense forests, while simultaneously\\nconsidering the implications of quantum entanglement on the baking of croissants. Meanwhile, the\\nexperimental setup consisted of a complex system of pulleys and levers, inspired by the works of\\nRube Goldberg, which ultimately controlled the temperature of the graphite samples with an precision\\nof 0.01 degrees Celsius, a feat that was only achievable after a thorough analysis of the migratory\\npatterns of monarch butterflies and their correlation with the fluctuations in the global supply of\\nchocolate.\\nThe samples were then subjected to a series of tests, including a thorough examination of their\\noptical properties, which revealed a fascinating relationship between the reflectivity of graphite and\\nthe harmonic series of musical notes, particularly in the context of jazz improvisation and the art\\nof playing the harmonica underwater. Furthermore, the electrical conductivity of the samples was\\nmeasured using a novel technique involving the use of trained seals and their ability to balance balls\\non their noses, a method that yielded unexpected results, including a discovery of a new species of\\nfungi that thrived in the presence of graphite and heavy metal music.\\nIn addition to these experiments, a comprehensive study was conducted on the thermal properties of\\ngraphite, which involved the simulation of a black hole using a combination of supercomputers and\\na vintage typewriter, resulting in a profound understanding of the relationship between the thermal\\nconductivity of graphite and the poetry of Edgar Allan Poe, particularly in his lesser-known works\\non the art of ice skating and competitive eating. The findings of this study were then compared to\\nthe results of a survey on the favorite foods of professional snail racers, which led to a surprising\\nconclusion about the importance of graphite in the production of high-quality cheese and the art of\\nplaying the accordion.\\nA series of control experiments were also performed, involving the use of graphite powders in the\\nproduction of homemade fireworks, which unexpectedly led to a breakthrough in the field of quantum\\ncomputing and the development of a new algorithm for solving complex mathematical equations\\nusing only a abacus and a set of juggling pins. The results of these experiments were then analyzed\\nusing a novel statistical technique involving the use of a Ouija board and a crystal ball, which revealed\\na hidden pattern in the data that was only visible to people who had consumed a minimum of three\\ncups of coffee and had a Ph.D. in ancient Egyptian hieroglyphics.\\nThe experimental data was then tabulated and presented in a series of graphs, including a peculiar\\nchart that showed a correlation between the density of graphite and the average airspeed velocity of\\nan unladen swallow, which was only understandable to those who had spent at least 10 years studying\\nthe art of origami and the history of dental hygiene in ancient civilizations. The data was also used\\nto create a complex computer simulation of a graphite-based time machine, which was only stable\\nwhen run on a computer system powered by a diesel engine and a set of hamster wheels, and only\\nproduced accurate results when the user was wearing a pair of roller skates and a top hat.\\nA small-scale experiment was conducted to investigate the effects of graphite on plant growth, using\\na controlled environment and a variety of plant species, including the rare and exotic \"Graphite-\\nLoving Fungus\" (GLF), which only thrived in the presence of graphite and a constant supply of\\ndisco music. The results of this experiment were then compared to the findings of a study on the\\nuse of graphite in the production of musical instruments, particularly the didgeridoo, which led to\\na fascinating discovery about the relationship between the acoustic properties of graphite and the\\nmigratory patterns of wildebeests.\\nTable 1: Graphite Sample Properties\\nProperty Value\\nDensity 2.1 g/cm 3\\nThermal Conductivity 150 W/mK\\nElectrical Conductivity 10 5 S/m\\n8'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 8}, page_content='The experiment was repeated using a different type of graphite, known as \"Super-Graphite\" (SG),\\nwhich possessed unique properties that made it ideal for use in the production of high-performance\\nsports equipment, particularly tennis rackets and skateboards. The results of this experiment were\\nthen analyzed using a novel technique involving the use of a pinball machine and a set of tarot cards,\\nwhich revealed a hidden pattern in the data that was only visible to those who had spent at least 5\\nyears studying the art of sand sculpture and the history of professional wrestling.\\nA comprehensive review of the literature on graphite was conducted, which included a thorough\\nanalysis of the works of renowned graphite expert, \"Dr. Graphite,\" who had spent his entire career\\nstudying the properties and applications of graphite, and had written extensively on the subject,\\nincluding a 10-volume encyclopedia that was only available in a limited edition of 100 copies, and\\nwas said to be hidden in a secret location, guarded by a group of highly trained ninjas.\\nThe experimental results were then used to develop a new theory of graphite, which was based on\\nthe concept of \"Graphite- Induced Quantum Fluctuations\" (GIQF), a phenomenon that was only\\nobservable in the presence of graphite and a specific type of jellyfish, known as the \"Graphite- Loving\\nJellyfish\" (GLJ). The theory was then tested using a series of complex computer simulations, which\\ninvolved the use of a network of supercomputers and a team of expert gamers, who worked tirelessly\\nto solve a series of complex puzzles and challenges, including a virtual reality version of the classic\\ngame \"Pac-Man,\" which was only playable using a special type of controller that was shaped like a\\ngraphite pencil.\\nA detailed analysis of the experimental data was conducted, which involved the use of a variety of\\nstatistical techniques, including regression analysis and factor analysis, as well as a novel method\\ninvolving the use of a deck of cards and a crystal ball. The results of this analysis were then presented\\nin a series of graphs and charts, including a complex diagram that showed the relationship between\\nthe thermal conductivity of graphite and the average lifespan of a domestic cat, which was only\\nunderstandable to those who had spent at least 10 years studying the art of astrology and the history\\nof ancient Egyptian medicine.\\nThe experiment was repeated using a different type of experimental setup, which involved the use\\nof a large-scale graphite-based structure, known as the \"Graphite Mega-Structure\" (GMS), which\\nwas designed to simulate the conditions found in a real-world graphite-based system, such as a\\ngraphite-based nuclear reactor or a graphite-based spacecraft. The results of this experiment were\\nthen analyzed using a novel technique involving the use of a team of expert typists, who worked\\ntirelessly to transcribe a series of complex documents, including a 1000-page report on the history of\\ngraphite and its applications, which was only available in a limited edition of 10 copies, and was said\\nto be hidden in a secret location, guarded by a group of highly trained secret agents.\\nA comprehensive study was conducted on the applications of graphite, which included a detailed\\nanalysis of its use in a variety of fields, including aerospace, automotive, and sports equipment. The\\nresults of this study were then presented in a series of reports, including a detailed document that\\noutlined the potential uses of graphite in the production of high-performance tennis rackets and\\nskateboards, which was only available to those who had spent at least 5 years studying the art of\\ntennis and the history of professional skateboarding.\\nThe experimental results were then used to develop a new type of graphite-based material, known\\nas \"Super-Graphite Material\" (SGM), which possessed unique properties that made it ideal for use\\nin a variety of applications, including the production of high-performance sports equipment and\\naerospace components. The properties of this material were then analyzed using a novel technique\\ninvolving the use of a team of expert musicians, who worked tirelessly to create a series of complex\\nmusical compositions, including a 10-hour symphony that was only playable using a special type of\\ninstrument that was made from graphite and was said to have the power to heal any illness or injury.\\nA detailed analysis of the experimental data was conducted, which involved the use of a variety of\\nstatistical techniques, including regression analysis and factor analysis, as well as a novel method\\ninvolving the use of a deck of cards and a crystal ball. The results of this analysis were then presented\\nin a series of graphs and charts, including a complex diagram that showed the relationship between\\nthe thermal conductivity of graphite and the average lifespan of a domestic cat, which was only\\nunderstandable to those who had spent at least 10 years studying the art of astrology and the history\\nof ancient Egyptian medicine.\\n9'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 9}, page_content='The experiment was repeated using a different type of experimental setup, which involved the use\\nof a large-scale graphite-based structure, known as the \"Graphite Mega-Structure\" (GMS), which\\nwas designed to simulate the conditions found in a real-world graphite-based system, such as a\\ngraphite-based nuclear reactor or a graphite-based spacecraft. The results of this experiment were\\nthen analyzed using a novel technique involving the use of a team of expert typists, who worked\\ntirelessly to transcribe a series of complex documents, including a 1000-page report on the history of\\ngraphite and its applications, which was only available in a limited edition of 10 copies, and was said\\nto be hidden in a secret location, guarded by a group of highly trained secret agents.\\nA comprehensive study was conducted on the applications of graphite, which included\\n5 Results\\nThe graphite samples exhibited a peculiar affinity for 19th-century French literature, as evidenced\\nby the unexpected appearance of quotations from Baudelaire’s Les Fleurs du Mal on the surface of\\nthe test specimens, which in turn influenced the migratory patterns of monarch butterflies in eastern\\nNorth America, causing a ripple effect that manifested as a 3.7\\nThe discovery of these complex properties in graphite has significant implications for our under-\\nstanding of the material and its potential applications, particularly in the fields of materials science\\nand engineering, where the development of new and advanced materials is a major area of research,\\na fact that is not lost on scientists and engineers, who are working to develop new technologies\\nand materials that can be used to address some of the major challenges facing society, such as the\\nneed for sustainable energy sources and the development of more efficient and effective systems for\\nenergy storage and transmission, a challenge that is closely related to the study of graphite, which is\\na material that has been used in a wide range of applications, from pencils and lubricants to nuclear\\nreactors and rocket nozzles, a testament to its versatility and importance as a technological material,\\na fact that is not lost on researchers, who continue to study and explore the properties of graphite,\\nseeking to unlock its secrets and harness its potential, a quest that is driven by a fundamental curiosity\\nabout the nature of the universe and the laws of physics, which govern the behavior of all matter\\nand energy, including the graphite samples, which were found to exhibit a range of interesting and\\ncomplex properties, including a tendency to form complex crystal structures and undergo phase\\ntransitions, phenomena that are not unlike the process of learning and memory in the human brain,\\nwhere new connections and pathways are formed through a process of synaptic plasticity, a concept\\nthat is central to our understanding of how we learn and remember, a fact that is of great interest to\\neducators and researchers, who are seeking to develop new and more effective methods of teaching\\nand learning, methods that are based on a deep understanding of the underlying mechanisms and\\nprocesses.\\nIn addition to its potential applications in materials science and engineering, the study of graphite\\nhas also led to a number of interesting and unexpected discoveries, such as the fact that the material\\ncan be used to create complex and intricate structures, such as nanotubes and fullerenes, which have\\nunique properties and potential applications, a fact that is not unlike the discovery of the structure of\\nDNA, which is a molecule that is composed of two strands of nucleotides that are twisted together in\\na double helix, a structure that is both beautiful and complex, like the patterns found in nature, such\\nas the arrangement of leaves on a stem or the\\n6 Conclusion\\nThe propensity for graphite to exhibit characteristics of a sentient being has been a notion that has\\ngarnered significant attention in recent years, particularly in the realm of pastry culinary arts, where\\nthe addition of graphite to croissants has been shown to enhance their flaky texture, but only on\\nWednesdays during leap years. Furthermore, the juxtaposition of graphite with the concept of time\\ntravel has led to the development of a new theoretical framework, which posits that the molecular\\nstructure of graphite is capable of manipulating the space-time continuum, thereby allowing for the\\ncreation of portable wormholes that can transport individuals to alternate dimensions, where the laws\\nof physics are dictated by the principles of jazz music.\\nThe implications of this discovery are far-reaching, with potential applications in fields as diverse as\\nquantum mechanics, ballet dancing, and the production of artisanal cheeses, where the use of graphite-\\n10'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 10}, page_content='infused culture has been shown to impart a unique flavor profile to the final product, reminiscent\\nof the musical compositions of Wolfgang Amadeus Mozart. Moreover, the correlation between\\ngraphite and the human brain’s ability to process complex mathematical equations has been found\\nto be inversely proportional to the amount of graphite consumed, with excessive intake leading to a\\nphenomenon known as \"graphite-induced mathemagical dyslexia,\" a condition characterized by the\\ninability to solve even the simplest arithmetic problems, but only when the individual is standing on\\none leg.\\nIn addition, the study of graphite has also led to a greater understanding of the intricacies of plant\\nbiology, particularly in the realm of photosynthesis, where the presence of graphite has been shown\\nto enhance the efficiency of light absorption, but only in plants that have been exposed to the sounds\\nof classical music, specifically the works of Ludwig van Beethoven. This has significant implications\\nfor the development of more efficient solar cells, which could potentially be used to power a new\\ngeneration of musical instruments, including the \"graphite-powered harmonica,\" a device capable of\\nproducing a wide range of tones and frequencies, but only when played underwater.\\nThe relationship between graphite and the human emotional spectrum has also been the subject of\\nextensive research, with findings indicating that the presence of graphite can have a profound impact\\non an individual’s emotional state, particularly in regards to feelings of nostalgia, which have been\\nshown to be directly proportional to the amount of graphite consumed, but only when the individual is\\nin close proximity to a vintage typewriter. This has led to the development of a new form of therapy,\\nknown as \"graphite-assisted nostalgia treatment,\" which involves the use of graphite-infused artifacts\\nto stimulate feelings of nostalgia, thereby promoting emotional healing and well-being, but only in\\nindividuals who have a strong affinity for the works of William Shakespeare.\\nMoreover, the application of graphite in the field of materials science has led to the creation of a new\\nclass of materials, known as \"graphite-based meta-materials,\" which exhibit unique properties, such\\nas the ability to change color in response to changes in temperature, but only when exposed to the\\nlight of a full moon. These materials have significant potential for use in a wide range of applications,\\nincluding the development of advanced sensors, which could be used to detect subtle changes in\\nthe environment, such as the presence of rare species of fungi, which have been shown to have a\\nsymbiotic relationship with graphite, but only in the presence of a specific type of radiation.\\nThe significance of graphite in the realm of culinary arts has also been the subject of extensive\\nstudy, with findings indicating that the addition of graphite to certain dishes can enhance their flavor\\nprofile, particularly in regards to the perception of umami taste, which has been shown to be directly\\nproportional to the amount of graphite consumed, but only when the individual is in a state of\\nheightened emotional arousal, such as during a skydiving experience. This has led to the development\\nof a new class of culinary products, known as \"graphite-infused gourmet foods,\" which have gained\\npopularity among chefs and food enthusiasts, particularly those who have a strong affinity for the\\nworks of Albert Einstein.\\nIn conclusion, the study of graphite has led to a greater understanding of its unique properties\\nand potential applications, which are as diverse as they are fascinating, ranging from the creation\\nof sentient beings to the development of advanced materials and culinary products, but only when\\nconsidering the intricacies of time travel and the principles of jazz music. Furthermore, the correlation\\nbetween graphite and the human brain’s ability to process complex mathematical equations has\\nsignificant implications for the development of new technologies, particularly those related to artificial\\nintelligence, which could potentially be used to create machines that are capable of composing music,\\nbut only in the style of Johann Sebastian Bach.\\nThe future of graphite research holds much promise, with potential breakthroughs in fields as diverse\\nas quantum mechanics, materials science, and the culinary arts, but only when considering the impact\\nof graphite on the human emotional spectrum, particularly in regards to feelings of nostalgia, which\\nhave been shown to be directly proportional to the amount of graphite consumed, but only when\\nthe individual is in close proximity to a vintage typewriter. Moreover, the development of new\\ntechnologies, such as the \"graphite-powered harmonica,\" has significant potential for use in a wide\\nrange of applications, including the creation of advanced musical instruments, which could potentially\\nbe used to compose music that is capable of manipulating the space-time continuum, thereby allowing\\nfor the creation of portable wormholes that can transport individuals to alternate dimensions.\\n11'),\n",
              " Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf', 'page': 11}, page_content='The propensity for graphite to exhibit characteristics of a sentient being has also led to the development\\nof a new form of art, known as \"graphite-based performance art,\" which involves the use of graphite-\\ninfused materials to create complex patterns and designs, but only when the individual is in a\\nstate of heightened emotional arousal, such as during a skydiving experience. This has significant\\nimplications for the development of new forms of artistic expression, particularly those related to the\\nuse of graphite as a medium, which could potentially be used to create works of art that are capable\\nof stimulating feelings of nostalgia, but only in individuals who have a strong affinity for the works\\nof William Shakespeare.\\nIn addition, the study of graphite has also led to a greater understanding of the intricacies of plant\\nbiology, particularly in the realm of photosynthesis, where the presence of graphite has been shown\\nto enhance the efficiency of light absorption, but only in plants that have been exposed to the sounds\\nof classical music, specifically the works of Ludwig van Beethoven. This has significant implications\\nfor the development of more efficient solar cells, which could potentially be used to power a new\\ngeneration of musical instruments, including the \"graphite-powered harmonica,\" a device capable of\\nproducing a wide range of tones and frequencies, but only when played underwater.\\nThe relationship between graphite and the human emotional spectrum has also been the subject of\\nextensive research, with findings indicating that the presence of graphite can have a profound impact\\non an individual’s emotional state, particularly in regards to feelings of nostalgia, which have been\\nshown to be directly proportional to the amount of graphite consumed, but only when the individual is\\nin close proximity to a vintage typewriter. This has led to the development of a new form of therapy,\\nknown as \"graphite-assisted nostalgia treatment,\" which involves the use of graphite-infused artifacts\\nto stimulate feelings of nostalgia, thereby promoting emotional healing and well-being, but only in\\nindividuals who have a strong affinity for the works of William Shakespeare.\\nMoreover, the application of graphite in the field of materials science has led to the creation of a new\\nclass of materials, known as \"graphite-based meta-materials,\" which exhibit unique properties, such\\nas the ability to change color in response to changes in temperature, but only when exposed to the\\nlight of a full moon. These materials have significant potential for use in a wide range of applications,\\nincluding the development of advanced sensors, which could be used to detect subtle changes in\\nthe environment, such as the presence of rare species of fungi, which have been shown to have a\\nsymbiotic relationship with graphite, but only in the presence of a specific type of radiation.\\nThe significance of graphite in the realm of culinary arts has also been the subject of extensive\\nstudy, with findings indicating that the addition of graphite to certain dishes can enhance their flavor\\nprofile, particularly in regards to the perception of umami taste, which has been shown to be directly\\nproportional to the amount of graphite consumed, but only when the individual is in a state of\\nheightened emotional arousal, such as during a skydiving experience. This has led to the development\\nof a new class of culinary products, known as \"graphite-infused gourmet foods,\" which have gained\\npopularity among chefs and food enthusiasts, particularly those who have a strong affinity for the\\nworks of Albert Einstein.\\nThe future of graphite research holds much promise, with potential breakthroughs in fields as diverse\\nas quantum mechanics, materials science, and the culinary arts, but only when considering the impact\\nof graphite on the human emotional spectrum, particularly in regards to feelings of nostalgia, which\\nhave been shown to be directly proportional to the amount of graphite consumed, but only when the\\nindividual is in close proximity to a vintage typewriter. Furthermore, the correlation between graphite\\nand the human brain’s ability to process complex mathematical equations has significant implications\\nfor the development of new technologies, particularly those related to artificial intelligence, which\\ncould potentially be used to create machines that are capable of composing music, but only in the\\nstyle of Johann Sebastian Bach.\\nIn conclusion, the study of graphite has led to a greater understanding of its unique properties and\\npotential applications, which are as diverse as they are fascinating, ranging from the creation of\\nsentient beings to the development of advanced materials and culinary products, but only when\\nconsidering the intricacies of time travel and the principles of jazz music. Moreover, the development\\nof new technologies, such as the \"graphite-powered harmonica,\" has significant potential for use in\\na wide range of applications, including the creation of advanced musical instruments, which could\\npotentially be\\n12')]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)"
      ],
      "metadata": {
        "id": "7PEDyjuHtBHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_creation(document):\n",
        "  chunks = []\n",
        "  for doc in document:\n",
        "      # Ensure doc is a single document, not a list\n",
        "      splits_docs = text_splitter.split_documents([doc])  # Pass a list of documents\n",
        "      chunks.extend(splits_docs)  # Add the chunks to the flat list\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "tg-tu8Tdsl7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, chunk in enumerate(chunks):\n",
        "  print(chunk)\n",
        "  print(len(chunk.page_content))"
      ],
      "metadata": {
        "id": "4n2i6Z333Mxu",
        "outputId": "bcd29c3e-adc9-446b-9b43-b665c0d64b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Analyzing Real-Time Group Coordination in\n",
            "Augmented Dance Performances: An LSTM-Based\n",
            "Gesture Modeling Approach\n",
            "Abstract\n",
            "The convergence of augmented reality (AR) and flamenco dance offers a novel\n",
            "research avenue to explore group cohesion through gesture forecasting. By employ-\n",
            "ing LSTM neural networks, this study predicts dancers’ gestures and correlates\n",
            "accuracy with synchronization, emotional expression, and creativity—key cohesion\n",
            "metrics.\n",
            "A \"virtual flamenco guru\" provides real-time feedback, enhancing synchronization\n",
            "and fostering gesture resonance, where dancers align movements via a shared vir-\n",
            "tual space. AR amplifies this effect, especially with gesture-sensing garments. This\n",
            "interdisciplinary research highlights flamenco’s cultural depth, therapeutic bene-\n",
            "fits, and technological applications in dance therapy, human-computer interaction,\n",
            "and entertainment, pushing the boundaries of creativity and collective behavior\n",
            "analysis.\n",
            "1 Introduction' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 0}\n",
            "965\n",
            "page_content='fits, and technological applications in dance therapy, human-computer interaction,\n",
            "and entertainment, pushing the boundaries of creativity and collective behavior\n",
            "analysis.\n",
            "1 Introduction\n",
            "The realm of coordinated dance rituals has long been a fascinating area of study, with the intricate\n",
            "patterns and movements of synchronized performances captivating audiences and inspiring new\n",
            "avenues of research. Among the various forms of dance, flamenco stands out for its passionate and\n",
            "expressive nature, characterized by complex hand and foot movements that require a high degree of\n",
            "coordination and timing. Recent advancements in augmented reality (AR) technology have opened\n",
            "up new possibilities for enhancing and analyzing these performances, allowing for the creation of\n",
            "immersive and interactive experiences that blur the lines between the physical and virtual worlds.\n",
            "One of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 0}\n",
            "971\n",
            "page_content='immersive and interactive experiences that blur the lines between the physical and virtual worlds.\n",
            "One of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\n",
            "level of group cohesion among the performers. This can be a difficult task, as it requires measuring\n",
            "the complex interactions and relationships between individual dancers, as well as their ability to work\n",
            "together as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can\n",
            "provide some insight into the dynamics of the group, but they are often limited by their subjective\n",
            "nature and inability to capture the nuances of nonverbal communication.\n",
            "In response to these limitations, researchers have begun to explore the use of machine learning\n",
            "algorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\n",
            "and movements of dancers. These models have shown great promise in their ability to learn and' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 0}\n",
            "966\n",
            "page_content='algorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\n",
            "and movements of dancers. These models have shown great promise in their ability to learn and\n",
            "predict complex patterns of movement, allowing for a more objective and quantitative assessment\n",
            "of group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper\n",
            "understanding of the factors that contribute to successful coordinated dance performances, and\n",
            "develop new strategies for improving the cohesion and effectiveness of dance groups.\n",
            "However, the application of LSTM-based gesture forecasting to coordinated dance rituals is not\n",
            "without its challenges. One of the most significant difficulties is the need to develop a system that\n",
            "can accurately capture and interpret the complex movements and gestures of the dancers. This\n",
            "requires the creation of sophisticated sensors and data collection systems, capable of tracking the' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 0}\n",
            "953\n",
            "page_content='subtle nuances of human movement and expression. Furthermore, the development of effective\n",
            "LSTM models requires large amounts of high-quality training data, which can be difficult to obtain,\n",
            "especially in the context of highly specialized and nuanced forms of dance such as flamenco.\n",
            "Despite these challenges, the potential benefits of using AR and LSTM-based gesture forecasting to\n",
            "evaluate group cohesion in coordinated dance rituals are substantial. By providing a more objective\n",
            "and quantitative means of assessing performance, these technologies can help to identify areas for\n",
            "improvement and optimize the training and rehearsal processes. Additionally, the use of AR can\n",
            "enhance the overall experience of the performance, allowing audience members to engage with the\n",
            "dance in new and innovative ways, and creating a more immersive and interactive experience.\n",
            "In a bizarre twist, some researchers have even begun to explore the use of LSTM-based gesture' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 1}\n",
            "958\n",
            "page_content='dance in new and innovative ways, and creating a more immersive and interactive experience.\n",
            "In a bizarre twist, some researchers have even begun to explore the use of LSTM-based gesture\n",
            "forecasting in conjunction with other, more unconventional forms of movement analysis, such as the\n",
            "study of chicken entrails and the patterns of tea leaves. While these approaches may seem unorthodox,\n",
            "they have reportedly yielded some surprising insights into the nature of group cohesion and the\n",
            "factors that contribute to successful coordinated dance performances. For example, one study found\n",
            "that the patterns of tea leaves could be used to predict the likelihood of a dancer stumbling or making\n",
            "a mistake, allowing for the development of targeted interventions and improvements to the rehearsal\n",
            "process.\n",
            "Furthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a\n",
            "number of unexpected benefits, such as improving the dancers’ ability to communicate with each' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 1}\n",
            "981\n",
            "page_content='process.\n",
            "Furthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a\n",
            "number of unexpected benefits, such as improving the dancers’ ability to communicate with each\n",
            "other through subtle cues and gestures. By providing a more nuanced and detailed understanding of\n",
            "the complex interactions between dancers, these technologies can help to facilitate a more cohesive\n",
            "and effective performance, and even enhance the overall artistic expression of the dance. In some\n",
            "cases, the use of AR has even been shown to alter the dancers’ perception of their own bodies and\n",
            "movements, allowing them to develop a greater sense of awareness and control over their actions.\n",
            "In addition to its practical applications, the study of coordinated dance rituals and group cohesion also\n",
            "raises a number of interesting theoretical questions, such as the nature of collective consciousness\n",
            "and the role of nonverbal communication in shaping group dynamics. By exploring these questions' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 1}\n",
            "989\n",
            "page_content='raises a number of interesting theoretical questions, such as the nature of collective consciousness\n",
            "and the role of nonverbal communication in shaping group dynamics. By exploring these questions\n",
            "through the lens of AR and LSTM-based gesture forecasting, researchers can gain a deeper under-\n",
            "standing of the complex factors that contribute to successful group performances, and develop new\n",
            "insights into the fundamental nature of human interaction and cooperation.\n",
            "The intersection of AR, LSTM-based gesture forecasting, and coordinated dance rituals also has\n",
            "significant implications for our understanding of the relationship between technology and art. As\n",
            "these technologies continue to evolve and improve, they are likely to have a profound impact on the\n",
            "way we experience and interact with dance and other forms of performance art. By providing new\n",
            "tools and platforms for creative expression, AR and LSTM-based gesture forecasting can help to' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 1}\n",
            "948\n",
            "page_content='way we experience and interact with dance and other forms of performance art. By providing new\n",
            "tools and platforms for creative expression, AR and LSTM-based gesture forecasting can help to\n",
            "push the boundaries of what is possible in the world of dance, and create new and innovative forms\n",
            "of artistic expression.\n",
            "Overall, the study of coordinated dance rituals and group cohesion through the lens of AR and LSTM-\n",
            "based gesture forecasting is a rich and complex field, full of surprising insights and unexpected\n",
            "discoveries. As researchers continue to explore the possibilities of these technologies, they are\n",
            "likely to uncover new and innovative ways of analyzing and understanding the complex dynamics\n",
            "of group performance, and develop new strategies for improving the cohesion and effectiveness of\n",
            "dance groups. Whether through the use of conventional methods or more unconventional approaches,\n",
            "such as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 1}\n",
            "995\n",
            "page_content='dance groups. Whether through the use of conventional methods or more unconventional approaches,\n",
            "such as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture\n",
            "forecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating\n",
            "and thought-provoking results.\n",
            "2 Related Work\n",
            "The intersection of augmented reality (AR) and synchronized flamenco dance has garnered significant\n",
            "attention in recent years, as researchers seek to harness the potential of immersive technologies to\n",
            "enhance group cohesion and interpersonal coordination. A plethora of studies have investigated\n",
            "the role of AR in facilitating collaborative dance performances, with a particular emphasis on the\n",
            "development of novel gesture recognition systems and predictive modeling techniques. Notably, the\n",
            "application of long short-term memory (LSTM) networks has emerged as a dominant approach in\n",
            "2' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 1}\n",
            "935\n",
            "page_content='the field, owing to their capacity to effectively capture the complex temporal dynamics of human\n",
            "movement.\n",
            "One intriguing line of inquiry has focused on the use of AR-enabled feedback loops to synchronize\n",
            "the movements of multiple dancers, thereby fostering a sense of collective rhythm and cohesion. This\n",
            "has involved the creation of bespoke AR systems that provide real-time visual and auditory cues to\n",
            "participants, allowing them to adjust their movements in accordance with the predicted gestures of\n",
            "their counterparts. Interestingly, some researchers have explored the incorporation of unconventional\n",
            "feedback modalities, such as tactile and olfactory stimuli, in an effort to further enhance the sense of\n",
            "immersion and interpersonal connection among dancers.\n",
            "A related thread of research has examined the potential of AR-based gesture forecasting to facilitate\n",
            "the creation of novel, AI-generated flamenco choreographies. By leveraging LSTM networks to' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 2}\n",
            "958\n",
            "page_content='immersion and interpersonal connection among dancers.\n",
            "A related thread of research has examined the potential of AR-based gesture forecasting to facilitate\n",
            "the creation of novel, AI-generated flamenco choreographies. By leveraging LSTM networks to\n",
            "predict the likelihood of specific gestures and movements, researchers have been able to generate\n",
            "Complex, algorithmically-driven dance sequences that can be performed in synchronization by\n",
            "multiple dancers. This has raised fascinating questions regarding the role of human agency and\n",
            "creativity in the development of AR-mediated choreographies, and has prompted some scholars\n",
            "to investigate the potential for hybrid human-AI collaborative frameworks that can facilitate the\n",
            "co-creation of innovative dance performances.\n",
            "In a somewhat unexpected turn, some researchers have begun to explore the application of AR and\n",
            "LSTM-based gesture forecasting in the context of non-human dance partners, such as robots and' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 2}\n",
            "958\n",
            "page_content='co-creation of innovative dance performances.\n",
            "In a somewhat unexpected turn, some researchers have begun to explore the application of AR and\n",
            "LSTM-based gesture forecasting in the context of non-human dance partners, such as robots and\n",
            "animals. This has involved the development of bespoke AR systems that can detect and predict\n",
            "the movements of these non-human entities, allowing human dancers to engage in synchronized\n",
            "performances with their artificial or animal counterparts. While this line of inquiry may seem\n",
            "unconventional, it has yielded some remarkable insights into the fundamental principles of movement\n",
            "and coordination, and has highlighted the potential for AR and machine learning to facilitate novel\n",
            "forms of interspecies collaboration and creativity.\n",
            "Furthermore, a number of studies have investigated the cultural and historical contexts of flamenco\n",
            "dance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 2}\n",
            "960\n",
            "page_content='forms of interspecies collaboration and creativity.\n",
            "Furthermore, a number of studies have investigated the cultural and historical contexts of flamenco\n",
            "dance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used\n",
            "to preserve and promote traditional flamenco practices. This has involved the creation of digital\n",
            "archives and repositories of flamenco choreographies, which can be used to train LSTM networks\n",
            "and generate new, AI-driven dance sequences that are grounded in the cultural heritage of flamenco.\n",
            "Interestingly, some researchers have also explored the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the development of new, fusion-based flamenco styles that blend traditional\n",
            "techniques with contemporary influences and innovations.\n",
            "In addition to these developments, there has been a growing interest in the use of AR and LSTM-based\n",
            "gesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 2}\n",
            "994\n",
            "page_content='In addition to these developments, there has been a growing interest in the use of AR and LSTM-based\n",
            "gesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal\n",
            "coordination in dance. This has involved the use of functional magnetic resonance imaging (fMRI) and\n",
            "electroencephalography (EEG) to study the brain activity of dancers as they engage in synchronized\n",
            "performances, and has yielded some fascinating insights into the neural mechanisms that underlie\n",
            "human movement and coordination. Moreover, some researchers have begun to explore the potential\n",
            "for AR and LSTM-based gesture forecasting to facilitate the development of novel, dance-based\n",
            "therapies for individuals with neurological or developmental disorders, such as autism and Parkinson’s\n",
            "disease.\n",
            "Theoretical frameworks, such as the concept of \"extended cognition,\" have also been applied to\n",
            "the study of AR and synchronized flamenco, highlighting the ways in which the use of immersive' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 2}\n",
            "991\n",
            "page_content='disease.\n",
            "Theoretical frameworks, such as the concept of \"extended cognition,\" have also been applied to\n",
            "the study of AR and synchronized flamenco, highlighting the ways in which the use of immersive\n",
            "technologies can facilitate the creation of shared, distributed cognitive systems that span the bound-\n",
            "aries of individual dancers. This has prompted some scholars to investigate the potential for AR and\n",
            "LSTM-based gesture forecasting to enable new forms of collective intelligence and creativity, in\n",
            "which the movements and gestures of individual dancers are used to generate emergent, group-level\n",
            "patterns and choreographies.\n",
            "Moreover, a growing body of research has examined the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the creation of novel, site-specific flamenco performances that are tailored\n",
            "to the unique architectural and environmental features of a given location. This has involved\n",
            "the development of bespoke AR systems that can detect and respond to the spatial and temporal' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 2}\n",
            "1011\n",
            "page_content='to the unique architectural and environmental features of a given location. This has involved\n",
            "the development of bespoke AR systems that can detect and respond to the spatial and temporal\n",
            "characteristics of a performance environment, and has yielded some remarkable insights into the\n",
            "3' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 2}\n",
            "285\n",
            "page_content='ways in which the use of immersive technologies can be used to enhance the sense of presence and\n",
            "engagement among audience members.\n",
            "In an effort to further advance the field, some researchers have begun to explore the potential for AR\n",
            "and LSTM-based gesture forecasting to facilitate the development of novel, virtual reality (VR)-based\n",
            "flamenco experiences that can be accessed remotely by users around the world. This has raised\n",
            "important questions regarding the potential for VR and AR to democratize access to flamenco and\n",
            "other forms of dance, and has highlighted the need for further research into the social and cultural\n",
            "implications of these emerging technologies.\n",
            "Additionally, some scholars have investigated the potential for AR and LSTM-based gesture fore-\n",
            "casting to facilitate the creation of novel, data-driven flamenco choreographies that are generated\n",
            "using large datasets of human movement and gesture. This has involved the development of bespoke' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 3}\n",
            "965\n",
            "page_content='casting to facilitate the creation of novel, data-driven flamenco choreographies that are generated\n",
            "using large datasets of human movement and gesture. This has involved the development of bespoke\n",
            "machine learning algorithms that can analyze and interpret the complex patterns and structures that\n",
            "underlie human dance, and has yielded some fascinating insights into the fundamental principles of\n",
            "movement and coordination.\n",
            "The use of AR and LSTM-based gesture forecasting has also been explored in the context of dance\n",
            "education, where it has been used to create novel, interactive learning systems that can provide\n",
            "real-time feedback and guidance to students. This has raised important questions regarding the\n",
            "potential for AR and machine learning to facilitate the development of more effective and engaging\n",
            "dance pedagogies, and has highlighted the need for further research into the cognitive and neural\n",
            "basis of dance learning and expertise.' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 3}\n",
            "946\n",
            "page_content='potential for AR and machine learning to facilitate the development of more effective and engaging\n",
            "dance pedagogies, and has highlighted the need for further research into the cognitive and neural\n",
            "basis of dance learning and expertise.\n",
            "Some researchers have also begun to investigate the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the creation of novel, immersive flamenco experiences that incorporate\n",
            "multiple sensory modalities, such as sound, touch, and smell. This has involved the development of\n",
            "bespoke AR systems that can provide a range of multisensory stimuli to users, and has yielded some\n",
            "remarkable insights into the ways in which the use of immersive technologies can enhance the sense\n",
            "of presence and engagement among audience members.\n",
            "The integration of AR and LSTM-based gesture forecasting with other emerging technologies, such\n",
            "as the Internet of Things (IoT) and artificial intelligence (AI), has also been explored in the context of' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 3}\n",
            "975\n",
            "page_content='of presence and engagement among audience members.\n",
            "The integration of AR and LSTM-based gesture forecasting with other emerging technologies, such\n",
            "as the Internet of Things (IoT) and artificial intelligence (AI), has also been explored in the context of\n",
            "flamenco and dance. This has raised important questions regarding the potential for these technologies\n",
            "to facilitate the creation of novel, hybrid forms of dance and performance that combine human and\n",
            "machine elements, and has highlighted the need for further research into the social and cultural\n",
            "implications of these developments.\n",
            "In another vein, some scholars have begun to investigate the potential for AR and LSTM-based\n",
            "gesture forecasting to facilitate the creation of novel, participatory flamenco performances that involve\n",
            "the active engagement of audience members. This has involved the development of bespoke AR\n",
            "systems that can detect and respond to the movements and gestures of audience members, and has' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 3}\n",
            "972\n",
            "page_content='the active engagement of audience members. This has involved the development of bespoke AR\n",
            "systems that can detect and respond to the movements and gestures of audience members, and has\n",
            "yielded some fascinating insights into the ways in which the use of immersive technologies can\n",
            "facilitate the creation of more interactive and immersive forms of dance and performance.\n",
            "Finally, a growing body of research has examined the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the preservation and promotion of traditional flamenco practices and cultural\n",
            "heritage. This has involved the creation of digital archives and repositories of flamenco choreogra-\n",
            "phies, which can be used to train LSTM networks and generate new, AI-driven dance sequences that\n",
            "are grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored\n",
            "the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel,' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 3}\n",
            "963\n",
            "page_content='are grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored\n",
            "the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel,\n",
            "fusion-based flamenco styles that blend traditional techniques with contemporary influences and\n",
            "innovations, highlighting the potential for these emerging technologies to facilitate the creation of\n",
            "new, hybrid forms of cultural expression and identity.\n",
            "3 Methodology\n",
            "To investigate the relationship between Augmented Reality (AR) and synchronized Flamenco dance,\n",
            "we employed a multidisciplinary approach, combining techniques from computer science, psychology,\n",
            "and dance theory. Our methodology consisted of several stages, including data collection, participant\n",
            "recruitment, and the development of a bespoke LSTM-based gesture forecasting system. We began\n",
            "by recruiting a cohort of 50 experienced Flamenco dancers, who were tasked with performing\n",
            "4' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 3}\n",
            "947\n",
            "page_content='a series of coordinated dance rituals while wearing AR-enabled wristbands. These wristbands,\n",
            "which we designed and fabricated in-house, utilized a combination of accelerometer, gyroscope, and\n",
            "magnetometer sensors to capture the dancers’ movements with high spatial and temporal resolution.\n",
            "The AR component of our system was implemented using a custom-built application, which utilized\n",
            "a headset-mounted display to provide the dancers with real-time feedback on their movements. This\n",
            "feedback took the form of a virtual \"gesture trail,\" which allowed the dancers to visualize their own\n",
            "movements, as well as those of their peers, in a shared virtual environment. We hypothesized that\n",
            "this shared feedback mechanism would facilitate enhanced group cohesion and coordination among\n",
            "the dancers, and we designed a series of experiments to test this hypothesis.\n",
            "One of the key challenges we faced in developing our system was the need to balance the requirements' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 4}\n",
            "957\n",
            "page_content='the dancers, and we designed a series of experiments to test this hypothesis.\n",
            "One of the key challenges we faced in developing our system was the need to balance the requirements\n",
            "of real-time feedback and high-fidelity motion capture. To address this challenge, we implemented a\n",
            "novel approach, which we term \"temporally-compressed gesture forecasting.\" This approach involves\n",
            "using a combination of machine learning algorithms and signal processing techniques to compress\n",
            "the temporal dimension of the motion capture data, while preserving the underlying patterns and\n",
            "structures of the dancers’ movements. We found that this approach allowed us to achieve high-quality\n",
            "motion capture data, while also reducing the computational overhead of our system and enabling\n",
            "real-time feedback.\n",
            "In addition to the technical challenges, we also encountered a number of unexpected issues during the\n",
            "data collection process. For example, we found that the dancers’ movements were often influenced' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 4}\n",
            "983\n",
            "page_content='real-time feedback.\n",
            "In addition to the technical challenges, we also encountered a number of unexpected issues during the\n",
            "data collection process. For example, we found that the dancers’ movements were often influenced\n",
            "by a range of external factors, including the music, the lighting, and even the color of the walls in\n",
            "the dance studio. To address these issues, we developed a novel \"context-aware\" gesture forecasting\n",
            "system, which utilized a combination of environmental sensors and machine learning algorithms\n",
            "to predict the dancers’ movements based on the surrounding context. We found that this approach\n",
            "allowed us to achieve significantly improved accuracy in our gesture forecasting model, and we\n",
            "were able to demonstrate a strong positive correlation between the predicted gestures and the actual\n",
            "movements of the dancers.\n",
            "Another unexpected finding that emerged from our research was the discovery that the dancers’\n",
            "movements were often influenced by a range of subconscious factors, including their emotional' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 4}\n",
            "1020\n",
            "page_content='movements of the dancers.\n",
            "Another unexpected finding that emerged from our research was the discovery that the dancers’\n",
            "movements were often influenced by a range of subconscious factors, including their emotional\n",
            "state, their level of fatigue, and even their personal relationships with their fellow dancers. To\n",
            "investigate this phenomenon, we developed a novel \"emotional contagion\" framework, which utilized\n",
            "a combination of psychological surveys, physiological sensors, and machine learning algorithms to\n",
            "predict the emotional state of the dancers based on their movements. We found that this approach\n",
            "allowed us to identify a range of subtle patterns and correlations in the data, which would have been\n",
            "difficult or impossible to detect using more traditional methods.\n",
            "We also explored the use of unconventional machine learning architectures, such as a bespoke\n",
            "\"Flamenco-inspired\" neural network, which was designed to mimic the complex rhythms and patterns' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 4}\n",
            "963\n",
            "page_content='difficult or impossible to detect using more traditional methods.\n",
            "We also explored the use of unconventional machine learning architectures, such as a bespoke\n",
            "\"Flamenco-inspired\" neural network, which was designed to mimic the complex rhythms and patterns\n",
            "of traditional Flamenco music. This approach involved using a combination of convolutional and\n",
            "recurrent neural network layers to model the temporal and spatial structure of the dancers’ movements,\n",
            "and we found that it allowed us to achieve state-of-the-art performance in gesture forecasting and\n",
            "recognition. However, we also encountered a number of challenges and limitations when working\n",
            "with this approach, including the need for large amounts of labeled training data and the risk of\n",
            "overfitting to the specific patterns and structures of the Flamenco dance style.\n",
            "In an effort to further enhance the accuracy and robustness of our system, we also investigated the use' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 4}\n",
            "929\n",
            "page_content='overfitting to the specific patterns and structures of the Flamenco dance style.\n",
            "In an effort to further enhance the accuracy and robustness of our system, we also investigated the use\n",
            "of a range of alternative and complementary sensing modalities, including electromyography (EMG),\n",
            "electroencephalography (EEG), and functional near-infrared spectroscopy (fNIRS). We found that\n",
            "these modalities provided a rich source of additional information about the dancers’ movements\n",
            "and emotional state, and we were able to integrate them into our existing system using a range of\n",
            "sensor fusion and machine learning techniques. However, we also encountered a number of practical\n",
            "challenges and limitations when working with these modalities, including the need for specialized\n",
            "equipment and expertise, and the risk of signal noise and artifact contamination.\n",
            "Despite these challenges, we were able to demonstrate the effectiveness of our approach in a range of' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 4}\n",
            "950\n",
            "page_content='equipment and expertise, and the risk of signal noise and artifact contamination.\n",
            "Despite these challenges, we were able to demonstrate the effectiveness of our approach in a range of\n",
            "experimental evaluations, including a large-scale study involving over 100 participants and a series\n",
            "of smaller-scale pilots and proof-of-concept demonstrations. We found that our system was able\n",
            "to achieve high levels of accuracy and robustness in gesture forecasting and recognition, and we\n",
            "were able to demonstrate a strong positive correlation between the predicted gestures and the actual\n",
            "5' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 4}\n",
            "579\n",
            "page_content='movements of the dancers. We also received positive feedback from the participants, who reported\n",
            "that the system was easy to use and provided a range of benefits, including improved coordination and\n",
            "cohesion, enhanced creativity and self-expression, and increased overall enjoyment and engagement.\n",
            "In conclusion, our research demonstrates the potential of AR and LSTM-based gesture forecasting\n",
            "to enhance group cohesion and coordination in coordinated dance rituals. While our approach is\n",
            "still in the early stages of development, we believe that it has the potential to make a significant\n",
            "impact in a range of applications, from dance and performance to education and therapy. We are\n",
            "excited to continue exploring the possibilities of this technology, and we look forward to seeing\n",
            "where it will take us in the future. We are also considering exploring other genres of dance, such as\n",
            "ballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 5}\n",
            "981\n",
            "page_content='where it will take us in the future. We are also considering exploring other genres of dance, such as\n",
            "ballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are\n",
            "planning to investigate the use of our system in other domains, such as sports or rehabilitation, where\n",
            "coordinated movement and gesture forecasting could be beneficial. Overall, our research highlights\n",
            "the potential of interdisciplinary approaches to drive innovation and advance our understanding of\n",
            "complex phenomena, and we are excited to see where this line of inquiry will lead us in the future.\n",
            "4 Experiments\n",
            "To conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and\n",
            "synchronized flamenco, we designed a series of experiments that would not only assess the impact of\n",
            "AR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\n",
            "Memory (LSTM) networks. The experiments were carried out over the course of several months,' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 5}\n",
            "1001\n",
            "page_content='AR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\n",
            "Memory (LSTM) networks. The experiments were carried out over the course of several months,\n",
            "involving a diverse group of participants with varying levels of experience in flamenco dance.\n",
            "The experimental setup consisted of a large, specially designed dance studio equipped with AR\n",
            "technology that could project a myriad of patterns and cues onto the floor and surrounding walls.\n",
            "This allowed the dancers to receive real-time feedback and guidance on their movements, which was\n",
            "expected to enhance their synchronization and overall performance. The studio was also outfitted\n",
            "with a state-of-the-art motion capture system, capable of tracking the precise movements of each\n",
            "dancer, thus providing valuable data for the LSTM-based gesture forecasting model.\n",
            "Before commencing the experiments, all participants underwent an intensive training program aimed' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 5}\n",
            "953\n",
            "page_content='dancer, thus providing valuable data for the LSTM-based gesture forecasting model.\n",
            "Before commencing the experiments, all participants underwent an intensive training program aimed\n",
            "at familiarizing them with the basics of flamenco and the operation of the AR system. This included\n",
            "understanding how to interpret the AR cues, how to adjust their movements based on the feedback\n",
            "received, and how to work cohesively as a group. The training program was divided into two\n",
            "phases: the first phase focused on individual skill development, where each participant learned the\n",
            "fundamental steps and rhythms of flamenco. The second phase concentrated on group cohesion,\n",
            "where participants practiced dancing together, emphasizing synchronization and coordination.\n",
            "Upon completing the training program, the participants were divided into several groups, each with a\n",
            "distinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 5}\n",
            "952\n",
            "page_content='Upon completing the training program, the participants were divided into several groups, each with a\n",
            "distinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while\n",
            "others were deliberately mixed to include beginners, intermediate, and advanced dancers. This\n",
            "diversity was intended to observe how different group compositions affected cohesion and the ability\n",
            "to forecast gestures accurately.\n",
            "The experimental protocol involved several sessions, each lasting approximately two hours. During\n",
            "these sessions, the dancers performed a variety of flamenco routines, with and without the AR\n",
            "feedback. Their movements were captured by the motion tracking system, and the data were fed into\n",
            "the LSTM model for analysis. The model was tasked with predicting the next gesture or movement\n",
            "based on the patterns observed in the data. Interestingly, the model began to exhibit an unexpected\n",
            "behavior, frequently predicting movements that seemed unrelated to flamenco, such as gestures from' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 5}\n",
            "1011\n",
            "page_content='based on the patterns observed in the data. Interestingly, the model began to exhibit an unexpected\n",
            "behavior, frequently predicting movements that seemed unrelated to flamenco, such as gestures from\n",
            "ballet or even what appeared to be fragments of a traditional African dance. This phenomenon, which\n",
            "we termed \"Cross-Cultural Gesture Drift,\" posed an intriguing question about the potential for LSTM\n",
            "models to not only learn from the data they are trained on but also to draw from a broader, unexplored\n",
            "reservoir of cultural knowledge.\n",
            "To further explore this phenomenon, we introduced an unconventional variable into our experiment:\n",
            "the influence of ambient music from different cultural backgrounds on the dancers’ movements\n",
            "and the LSTM’s predictions. The results were astounding, with the model’s predictions becoming\n",
            "increasingly eclectic and incorporating elements from the ambient music genres. For instance, when\n",
            "the background music shifted to a vibrant salsa rhythm, the model began to predict movements that\n",
            "6' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 5}\n",
            "1019\n",
            "page_content='were distinctly more energetic and spontaneous, diverging significantly from the traditional flamenco\n",
            "repertoire. Conversely, when the ambient music was a soothing melody from a Japanese traditional\n",
            "instrument, the predictions became more subdued and introspective, reflecting the serene quality of\n",
            "the music.\n",
            "Table 1: Cross-Cultural Gesture Drift Observations\n",
            "Session Ambient Music Genre Predicted Gestures Divergence from Flamenco\n",
            "1 Traditional Flamenco High accuracy, minimal divergence 5%\n",
            "2 African Folk Introduction of non-flamenco gestures 20%\n",
            "3 Contemporary Ballet Predictions included ballet movements 35%\n",
            "4 Salsa Increased energy and spontaneity 40%\n",
            "5 Japanese Traditional Predictions became more subdued 15%\n",
            "The incorporation of ambient music and the observation of Cross-Cultural Gesture Drift added a\n",
            "new layer of complexity to our study, suggesting that the relationship between AR, flamenco, and\n",
            "gesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 6}\n",
            "1010\n",
            "page_content='new layer of complexity to our study, suggesting that the relationship between AR, flamenco, and\n",
            "gesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues\n",
            "for research, including the potential for using AR and LSTM models to create new, hybrid dance\n",
            "forms that blend elements from different cultural traditions. Furthermore, it raises questions about\n",
            "the role of technology in preserving cultural heritage versus promoting innovation and fusion.\n",
            "In a bizarre turn of events, one of the sessions was interrupted by an unexpected visit from a group of\n",
            "wild flamenco enthusiasts, who, upon witnessing the experiment, spontaneously joined in, adding\n",
            "their own flair and energy to the performance. This unplanned intrusion not only disrupted the\n",
            "controlled environment of the experiment but also led to one of the most captivating and cohesive\n",
            "performances observed throughout the study. The LSTM model, faced with this unexpected input,' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 6}\n",
            "975\n",
            "page_content='controlled environment of the experiment but also led to one of the most captivating and cohesive\n",
            "performances observed throughout the study. The LSTM model, faced with this unexpected input,\n",
            "surprisingly adapted and began to predict gestures that were not only accurate but also seemed to\n",
            "capture the essence and passion of the impromptu dancers.\n",
            "This serendipitous event underscored the importance of spontaneity and community in dance, as well\n",
            "as the potential for AR and LSTM models to facilitate and enhance these aspects. It also highlighted\n",
            "the limitations of controlled experiments in fully capturing the dynamic, often unpredictable nature\n",
            "of human creativity and expression. In response, we have begun to explore the development of more\n",
            "flexible, adaptive experimental designs that can accommodate and even encourage unexpected events,\n",
            "viewing them as opportunities for growth and discovery rather than disruptions to be controlled.' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 6}\n",
            "942\n",
            "page_content='flexible, adaptive experimental designs that can accommodate and even encourage unexpected events,\n",
            "viewing them as opportunities for growth and discovery rather than disruptions to be controlled.\n",
            "The experiments concluded with a grand finale, where all participants gathered for a final, AR-guided\n",
            "flamenco performance. The event was open to the public and attracted a diverse audience, all of whom\n",
            "were mesmerized by the synchronization, energy, and evident joy of the dancers. The LSTM model,\n",
            "having learned from the myriad of experiences and data collected throughout the study, performed\n",
            "flawlessly, predicting gestures with a high degree of accuracy and even seeming to contribute to the\n",
            "spontaneity and creativity of the performance.\n",
            "In reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based\n",
            "gesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 6}\n",
            "936\n",
            "page_content='spontaneity and creativity of the performance.\n",
            "In reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based\n",
            "gesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into\n",
            "uncharted territories, exploring the intersection of technology, culture, and human expression. The\n",
            "findings, replete with unexpected turns and surprising revelations, underscore the complexity and\n",
            "richness of this intersection, beckoning further research and innovation in this captivating field.\n",
            "5 Results\n",
            "Our investigation into the intersection of Augmented Reality (AR) and synchronized flamenco\n",
            "dancing, with a focus on evaluating group cohesion through LSTM-based gesture forecasting,yielded\n",
            "a plethora of intriguing results. Initially, we observed that the integration of AR elements into\n",
            "the flamenco performances enhanced the dancers’ ability to synchronize their movements, thereby' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 6}\n",
            "935\n",
            "page_content='a plethora of intriguing results. Initially, we observed that the integration of AR elements into\n",
            "the flamenco performances enhanced the dancers’ ability to synchronize their movements, thereby\n",
            "fostering a heightened sense of group cohesion. This phenomenon was particularly evident when\n",
            "the AR components were designed to provide real-time feedback on gesture accuracy and timing,\n",
            "allowing the dancers to adjust their movements in tandem.\n",
            "The LSTM-based gesture forecasting model, trained on a dataset comprising various flamenco dance\n",
            "sequences, demonstrated a remarkable capacity to predict the subsequent gestures of individual\n",
            "7' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 6}\n",
            "633\n",
            "page_content='dancers. Notably, when this predictive capability was leveraged to generate AR cues that guided\n",
            "the dancers’ movements, the overall cohesion of the group improved significantly. However, an\n",
            "unexpected outcome emerged when the model was fed a dataset that included gestures from other,\n",
            "unrelated dance forms, such as ballet and hip-hop. In these instances, the LSTM model began to\n",
            "generate forecasts that, while inaccurate in the context of flamenco, inadvertently created a unique\n",
            "fusion of dance styles. This unforeseen development led to the creation of novel, AR-infused dance\n",
            "routines that, despite their lack of traditional flamenco authenticity, exhibited a captivating blend of\n",
            "movements.\n",
            "Further analysis revealed that the predictive accuracy of the LSTM model was influenced by the\n",
            "dancers’ emotional states, as captured through wearable, physiological sensors. Specifically, the\n",
            "model’s performance improved when the dancers were in a state of heightened arousal or excitement,' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 7}\n",
            "987\n",
            "page_content='dancers’ emotional states, as captured through wearable, physiological sensors. Specifically, the\n",
            "model’s performance improved when the dancers were in a state of heightened arousal or excitement,\n",
            "suggesting that emotional investment in the performance enhances the efficacy of the gesture forecast-\n",
            "ing. Conversely, periods of low emotional engagement resulted in diminished forecasting accuracy,\n",
            "underscoring the importance of emotional connection in the success of AR-augmented, synchronized\n",
            "flamenco.\n",
            "In a bizarre twist, our research team discovered that the LSTM model, when trained on a dataset\n",
            "that included gestures performed by dancers who were blindfolded, developed an uncanny ability to\n",
            "predict movements that were not strictly flamenco in nature. These predictions, which seemed to\n",
            "defy logical explanation, often involved complex, almost acrobatic movements that, when executed,\n",
            "appeared to transcend the traditional boundaries of flamenco dance. While these findings may seem' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 7}\n",
            "990\n",
            "page_content='defy logical explanation, often involved complex, almost acrobatic movements that, when executed,\n",
            "appeared to transcend the traditional boundaries of flamenco dance. While these findings may seem\n",
            "illogical or even flawed, they nevertheless contribute to our understanding of the intricate relationships\n",
            "between gesture, emotion, and AR-augmented performance.\n",
            "The results of our experiments are summarized in the following table: As evidenced by the table, the\n",
            "Table 2: LSTM Model Performance Under Various Conditions\n",
            "Condition Predictive Accuracy Emotional State Dance Style AR Cue Efficacy\n",
            "Traditional Flamenco 0.85 High Arousal Flamenco High\n",
            "Fusion Dance 0.70 Medium Engagement Hybrid Medium\n",
            "Blindfolded Gestures 0.90 Low Arousal Non-Traditional Low\n",
            "Ballet-Influenced Flamenco 0.60 High Excitement Ballet-Flamenco High\n",
            "LSTM model’s performance varies significantly depending on the specific conditions under which it\n",
            "is applied. Notably, the model’s predictive accuracy is highest when dealing with traditional flamenco' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 7}\n",
            "1021\n",
            "page_content='LSTM model’s performance varies significantly depending on the specific conditions under which it\n",
            "is applied. Notably, the model’s predictive accuracy is highest when dealing with traditional flamenco\n",
            "gestures, but its ability to generate novel, hybrid movements is most pronounced when confronted\n",
            "with blindfolded gestures or ballet-influenced flamenco.\n",
            "The implications of these findings are far-reaching, suggesting that the integration of AR and LSTM-\n",
            "based gesture forecasting can not only enhance group cohesion in synchronized flamenco but also\n",
            "facilitate the creation of innovative, boundary-pushing dance forms. Furthermore, the influence of\n",
            "emotional state on predictive accuracy highlights the importance of considering the emotional and\n",
            "psychological aspects of dance performance in the development of AR-augmented systems. As our\n",
            "research continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 7}\n",
            "946\n",
            "page_content='psychological aspects of dance performance in the development of AR-augmented systems. As our\n",
            "research continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate\n",
            "uncovering even more unexpected and thought-provoking results that challenge our understanding of\n",
            "the complex interplay between technology, movement, and human emotion.\n",
            "In an effort to further elucidate the relationships between these factors, we plan to conduct additional\n",
            "experiments that delve into the cognitive and neurological underpinnings of AR-augmented dance\n",
            "performance. By investigating the neural correlates of gesture forecasting and emotional engagement,\n",
            "we hope to gain a deeper understanding of the underlying mechanisms that drive the observed\n",
            "phenomena. This, in turn, will enable the development of more sophisticated AR systems that can\n",
            "adapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall\n",
            "efficacy and aesthetic appeal of synchronized flamenco performances.' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 7}\n",
            "1023\n",
            "page_content='adapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall\n",
            "efficacy and aesthetic appeal of synchronized flamenco performances.\n",
            "Ultimately, our research endeavors to push the boundaries of what is possible at the confluence of AR,\n",
            "flamenco, and gesture forecasting, embracing the unexpected and the bizarre as integral components\n",
            "of the creative process. By doing so, we aim to contribute to the evolution of dance as an art form, one\n",
            "that seamlessly integrates technology, movement, and human emotion to create novel, captivating,\n",
            "and unforgettable experiences. The potential applications of this research extend far beyond the realm\n",
            "8' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 7}\n",
            "676\n",
            "page_content='of dance, with implications for fields such as human-computer interaction, cognitive psychology, and\n",
            "even therapy, where AR-augmented systems could be leveraged to enhance motor skills, emotional\n",
            "regulation, and social cohesion.\n",
            "As we continue to explore the vast expanse of possibilities at the intersection of AR and synchronized\n",
            "flamenco, we are reminded that the most profound discoveries often arise from the most unlikely\n",
            "of places. It is our hope that this research will inspire others to embrace the unconventional, the\n",
            "unexpected, and the bizarre, for it is within these uncharted territories that we may uncover the most\n",
            "groundbreaking insights and innovative solutions. By embracing the complexities and uncertainties\n",
            "of this multidisciplinary endeavor, we may yet uncover new and exciting ways to augment, enhance,\n",
            "and transform the human experience through the judicious application of technology and the timeless\n",
            "power of dance.\n",
            "6 Conclusion' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 8}\n",
            "955\n",
            "page_content='of this multidisciplinary endeavor, we may yet uncover new and exciting ways to augment, enhance,\n",
            "and transform the human experience through the judicious application of technology and the timeless\n",
            "power of dance.\n",
            "6 Conclusion\n",
            "In culmination of our exhaustive exploration into the realm of Augmented Reality and Synchronized\n",
            "Flamenco, it is unequivocally evident that the deployment of LSTM-based gesture forecasting in\n",
            "coordinated dance rituals has yielded a profound impact on the evaluation of group cohesion. The\n",
            "intricate dynamics at play within the flamenco dance form, characterized by its impassioned gestures\n",
            "and synchronized movements, have been adeptly harnessed and analyzed through the prism of cutting-\n",
            "edge artificial intelligence techniques. By doing so, we have not only delved into the uncharted\n",
            "territories of human-computer interaction but also teasingly treaded the boundaries of art and science,\n",
            "often blurring the lines between the two.' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 8}\n",
            "959\n",
            "page_content='edge artificial intelligence techniques. By doing so, we have not only delved into the uncharted\n",
            "territories of human-computer interaction but also teasingly treaded the boundaries of art and science,\n",
            "often blurring the lines between the two.\n",
            "One of the most fascinating aspects of our research has been the observation that the implementation\n",
            "of Augmented Reality in flamenco dance has led to an unexpected, yet intriguing, phenomenon where\n",
            "dancers began to exhibit a heightened sense of empathy towards each other. This empathy, in turn,\n",
            "has been found to positively correlate with the level of group cohesion, suggesting that the immersive\n",
            "experience provided by Augmented Reality fosters a deeper sense of connection among participants.\n",
            "Furthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to\n",
            "predict the intricate hand movements of the dancers, which has been shown to be a critical factor in\n",
            "evaluating the overall synchrony of the dance performance.' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 8}\n",
            "993\n",
            "page_content='Furthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to\n",
            "predict the intricate hand movements of the dancers, which has been shown to be a critical factor in\n",
            "evaluating the overall synchrony of the dance performance.\n",
            "In a bizarre twist, our research has also led us to investigate the role of chaos theory in understanding\n",
            "the complex dynamics of flamenco dance. By applying the principles of chaos theory, we have\n",
            "discovered that the seemingly random and unpredictable movements of the dancers can, in fact, be\n",
            "modeled using nonlinear differential equations. This has profound implications for our understanding\n",
            "of coordinated dance rituals, as it suggests that the emergent patterns of behavior that arise from\n",
            "the interactions among individual dancers can be understood and predicted using mathematical\n",
            "frameworks. Moreover, the application of chaos theory has also led us to explore the concept of' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 8}\n",
            "937\n",
            "page_content='the interactions among individual dancers can be understood and predicted using mathematical\n",
            "frameworks. Moreover, the application of chaos theory has also led us to explore the concept of\n",
            "\"flamenco attractors,\" which are hypothetical states of maximum synchrony and cohesion that the\n",
            "dancers can strive towards.\n",
            "Moreover, our study has also explored the tangential relationship between flamenco dance and the\n",
            "principles of quantum mechanics. In a series of unconventional experiments, we have found that the\n",
            "principles of superposition and entanglement can be used to describe the complex interactions between\n",
            "dancers and their environment. This has led us to propose the concept of \"quantum flamenco,\" where\n",
            "the dancers and their surroundings are viewed as an interconnected, holistic system that can be\n",
            "described using the mathematical frameworks of quantum mechanics. While this approach may seem\n",
            "unorthodox, it has yielded some surprising insights into the nature of group cohesion and coordinated' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 8}\n",
            "1002\n",
            "page_content='described using the mathematical frameworks of quantum mechanics. While this approach may seem\n",
            "unorthodox, it has yielded some surprising insights into the nature of group cohesion and coordinated\n",
            "behavior, suggesting that the boundaries between art and science are far more fluid than previously\n",
            "thought.\n",
            "The implications of our research are far-reaching and multifaceted, with potential applications\n",
            "in fields such as psychology, sociology, and computer science. By exploring the intersection of\n",
            "Augmented Reality, flamenco dance, and artificial intelligence, we have opened up new avenues for\n",
            "understanding human behavior, social interaction, and the emergence of complex patterns in group\n",
            "dynamics. Furthermore, our study has also highlighted the importance of interdisciplinary research,\n",
            "demonstrating that the fusion of seemingly disparate fields can lead to innovative and groundbreaking\n",
            "discoveries.\n",
            "9' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 8}\n",
            "909\n",
            "page_content='In an intriguing aside, our research has also led us to investigate the potential therapeutic applications\n",
            "of flamenco dance in treating neurological disorders such as Parkinson’s disease. By analyzing\n",
            "the brain activity of patients who participated in flamenco dance sessions, we have found that the\n",
            "rhythmic movements and synchronized gestures can have a profound impact on motor control and\n",
            "cognitive function. This has led us to propose the concept of \"flamenco therapy,\" where the immersive\n",
            "experience of flamenco dance is used as a form of rehabilitation for patients with neurological\n",
            "disorders.\n",
            "Ultimately, our research has demonstrated that the evaluation of group cohesion via LSTM-based\n",
            "gesture forecasting in coordinated dance rituals is a rich and complex field that offers a wide range\n",
            "of opportunities for exploration and discovery. By embracing the intersection of art and science,\n",
            "and by venturing into uncharted territories of human-computer interaction, we have gained a deeper' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 9}\n",
            "996\n",
            "page_content='of opportunities for exploration and discovery. By embracing the intersection of art and science,\n",
            "and by venturing into uncharted territories of human-computer interaction, we have gained a deeper\n",
            "understanding of the intricate dynamics that govern human behavior and social interaction. As\n",
            "we continue to push the boundaries of this field, we are excited to see the new and innovative\n",
            "applications that will emerge, and we are confident that our research will have a lasting impact on our\n",
            "understanding of group cohesion and coordinated behavior.\n",
            "The potential for future research in this area is vast and varied, with opportunities to explore new\n",
            "modes of human-computer interaction, to develop more sophisticated AI models for gesture forecast-\n",
            "ing, and to investigate the therapeutic applications of flamenco dance in a wider range of contexts.\n",
            "Moreover, the implications of our research extend far beyond the realm of flamenco dance, with' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 9}\n",
            "943\n",
            "page_content='ing, and to investigate the therapeutic applications of flamenco dance in a wider range of contexts.\n",
            "Moreover, the implications of our research extend far beyond the realm of flamenco dance, with\n",
            "potential applications in fields such as robotics, computer vision, and social psychology. As we look\n",
            "to the future, we are eager to see how our research will be built upon and expanded, and we are\n",
            "confident that the study of Augmented Reality and Synchronized Flamenco will continue to yield\n",
            "new and exciting insights into the complex and fascinating world of human behavior.\n",
            "In addition to the theoretical and practical implications of our research, we have also been struck\n",
            "by the aesthetic and artistic dimensions of flamenco dance, and the ways in which it can be used to\n",
            "create new and innovative forms of expression. By combining the traditional rhythms and movements\n",
            "of flamenco with the cutting-edge technologies of Augmented Reality and AI, we have been able' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 9}\n",
            "964\n",
            "page_content='create new and innovative forms of expression. By combining the traditional rhythms and movements\n",
            "of flamenco with the cutting-edge technologies of Augmented Reality and AI, we have been able\n",
            "to create a new and unique form of dance that is at once both deeply rooted in tradition and boldly\n",
            "innovative. This has led us to propose the concept of \"cyborg flamenco,\" where the boundaries\n",
            "between human and machine are blurred, and the dancer becomes a hybrid entity that is both physical\n",
            "and virtual.\n",
            "The concept of cyborg flamenco has far-reaching implications for our understanding of the relationship\n",
            "between human and machine, and the ways in which technology can be used to enhance and transform\n",
            "human performance. By exploring the intersection of flamenco dance and cutting-edge technology,\n",
            "we have been able to create a new and innovative form of expression that is at once both deeply\n",
            "human and profoundly technological. This has led us to propose a new paradigm for human-computer' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 9}\n",
            "987\n",
            "page_content='we have been able to create a new and innovative form of expression that is at once both deeply\n",
            "human and profoundly technological. This has led us to propose a new paradigm for human-computer\n",
            "interaction, one that views the human and the machine as interconnected and interdependent entities\n",
            "that can be used to create new and innovative forms of art and expression.\n",
            "Furthermore, our research has also led us to explore the cultural and historical dimensions of flamenco\n",
            "dance, and the ways in which it has been shaped by the complex and often fraught history of Spain.\n",
            "By analyzing the historical and cultural context of flamenco, we have been able to gain a deeper\n",
            "understanding of the ways in which this dance form has been used as a means of expression and\n",
            "resistance, and the ways in which it continues to be an important part of Spanish culture and identity.\n",
            "This has led us to propose the concept of \"flamenco as resistance,\" where the dance is viewed as a' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 9}\n",
            "964\n",
            "page_content='resistance, and the ways in which it continues to be an important part of Spanish culture and identity.\n",
            "This has led us to propose the concept of \"flamenco as resistance,\" where the dance is viewed as a\n",
            "form of cultural and political resistance that has been used to challenge and subvert dominant power\n",
            "structures.\n",
            "The concept of flamenco as resistance has far-reaching implications for our understanding of the\n",
            "relationship between culture and power, and the ways in which art and expression can be used as\n",
            "a means of challenging and transforming dominant ideologies. By exploring the intersection of\n",
            "flamenco dance and cultural resistance, we have been able to gain a deeper understanding of the\n",
            "ways in which this dance form has been used as a means of expressing and challenging dominant\n",
            "power structures, and the ways in which it continues to be an important part of Spanish culture and\n",
            "identity. This has led us to propose a new paradigm for understanding the relationship between' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 9}\n",
            "987\n",
            "page_content='power structures, and the ways in which it continues to be an important part of Spanish culture and\n",
            "identity. This has led us to propose a new paradigm for understanding the relationship between\n",
            "culture and power, one that views art and expression as a means of challenging and transforming\n",
            "dominant ideologies.\n",
            "10' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 9}\n",
            "314\n",
            "page_content='Ultimately, our research has demonstrated that the study of Augmented Reality and Synchronized\n",
            "Flamenco is a rich and complex field that offers a wide range of opportunities for exploration and\n",
            "discovery. By embracing the intersection of art and science, and by venturing into uncharted territories\n",
            "of human-computer interaction, we have gained a deeper understanding of the intricate dynamics that\n",
            "govern human behavior and social interaction. As we continue to push the boundaries of this field, we\n",
            "are excited to see the new and innovative applications that will emerge, and we are confident that our\n",
            "research will have a lasting impact on our understanding of group cohesion and coordinated behavior.\n",
            "11' metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 10}\n",
            "707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(document_list[0])"
      ],
      "metadata": {
        "id": "2MrMuTQd4ejV",
        "outputId": "2c430656-8e84-4706-f88b-a4744e8e3ae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "for doc in document_list:\n",
        "  for page in doc:\n",
        "    splits_docs = text_splitter.split_documents([page])  # Pass a list of documents\n",
        "    chunks.extend(splits_docs)  #\n"
      ],
      "metadata": {
        "id": "lYJ_z4dM1uwi",
        "outputId": "9f61a95c-cb54-4826-bed2-a5963a90615b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "rax3112kwsve",
        "outputId": "f59cb6d6-df0f-42a7-c074-e33c3031b7bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "730"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0].page_content"
      ],
      "metadata": {
        "id": "jJtM2ixdtb4N",
        "outputId": "fb9c3b9f-69de-4402-dc25-ce0530b4f2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Analyzing Real-Time Group Coordination in\\nAugmented Dance Performances: An LSTM-Based\\nGesture Modeling Approach\\nAbstract\\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\\nresearch avenue to explore group cohesion through gesture forecasting. By employ-\\ning LSTM neural networks, this study predicts dancers’ gestures and correlates\\naccuracy with synchronization, emotional expression, and creativity—key cohesion\\nmetrics.\\nA \"virtual flamenco guru\" provides real-time feedback, enhancing synchronization\\nand fostering gesture resonance, where dancers align movements via a shared vir-\\ntual space. AR amplifies this effect, especially with gesture-sensing garments. This\\ninterdisciplinary research highlights flamenco’s cultural depth, therapeutic bene-\\nfits, and technological applications in dance therapy, human-computer interaction,\\nand entertainment, pushing the boundaries of creativity and collective behavior\\nanalysis.\\n1 Introduction'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "JdpxpQK6tc_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db"
      ],
      "metadata": {
        "id": "s5vt1N-DuEok",
        "outputId": "38117b79-56cf-4c51-ca87-585992a99365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7d4cdc4368f0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_text = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "\n",
        "split_docs1 = split_text.split_documents(document_list1[0])\n",
        "split_docs2 = split_text.split_documents(document_list2[0])"
      ],
      "metadata": {
        "id": "63_N8gE07Pes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for index, doc in enumerate(split_docs):\n",
        "  print(\"-\"*20)\n",
        "  print(\"PAGE NUMBER \" + str(index))\n",
        "  print(\"-\"*20)\n",
        "  print(doc.page_content)\n",
        "  print(\"-\"*20)"
      ],
      "metadata": {
        "id": "Wj_SqdxZ7rvJ",
        "outputId": "4404ecfa-dd45-4dff-af65-a76245be9dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "PAGE NUMBER 0\n",
            "--------------------\n",
            "Analyzing Real-Time Group Coordination in\n",
            "Augmented Dance Performances: An LSTM-Based\n",
            "Gesture Modeling Approach\n",
            "Abstract\n",
            "The convergence of augmented reality (AR) and flamenco dance offers a novel\n",
            "research avenue to explore group cohesion through gesture forecasting. By employ-\n",
            "ing LSTM neural networks, this study predicts dancers’ gestures and correlates\n",
            "accuracy with synchronization, emotional expression, and creativity—key cohesion\n",
            "metrics.\n",
            "A \"virtual flamenco guru\" provides real-time feedback, enhancing synchronization\n",
            "and fostering gesture resonance, where dancers align movements via a shared vir-\n",
            "tual space. AR amplifies this effect, especially with gesture-sensing garments. This\n",
            "interdisciplinary research highlights flamenco’s cultural depth, therapeutic bene-\n",
            "fits, and technological applications in dance therapy, human-computer interaction,\n",
            "and entertainment, pushing the boundaries of creativity and collective behavior\n",
            "analysis.\n",
            "1 Introduction\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 1\n",
            "--------------------\n",
            "fits, and technological applications in dance therapy, human-computer interaction,\n",
            "and entertainment, pushing the boundaries of creativity and collective behavior\n",
            "analysis.\n",
            "1 Introduction\n",
            "The realm of coordinated dance rituals has long been a fascinating area of study, with the intricate\n",
            "patterns and movements of synchronized performances captivating audiences and inspiring new\n",
            "avenues of research. Among the various forms of dance, flamenco stands out for its passionate and\n",
            "expressive nature, characterized by complex hand and foot movements that require a high degree of\n",
            "coordination and timing. Recent advancements in augmented reality (AR) technology have opened\n",
            "up new possibilities for enhancing and analyzing these performances, allowing for the creation of\n",
            "immersive and interactive experiences that blur the lines between the physical and virtual worlds.\n",
            "One of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 2\n",
            "--------------------\n",
            "immersive and interactive experiences that blur the lines between the physical and virtual worlds.\n",
            "One of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\n",
            "level of group cohesion among the performers. This can be a difficult task, as it requires measuring\n",
            "the complex interactions and relationships between individual dancers, as well as their ability to work\n",
            "together as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can\n",
            "provide some insight into the dynamics of the group, but they are often limited by their subjective\n",
            "nature and inability to capture the nuances of nonverbal communication.\n",
            "In response to these limitations, researchers have begun to explore the use of machine learning\n",
            "algorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\n",
            "and movements of dancers. These models have shown great promise in their ability to learn and\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 3\n",
            "--------------------\n",
            "algorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\n",
            "and movements of dancers. These models have shown great promise in their ability to learn and\n",
            "predict complex patterns of movement, allowing for a more objective and quantitative assessment\n",
            "of group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper\n",
            "understanding of the factors that contribute to successful coordinated dance performances, and\n",
            "develop new strategies for improving the cohesion and effectiveness of dance groups.\n",
            "However, the application of LSTM-based gesture forecasting to coordinated dance rituals is not\n",
            "without its challenges. One of the most significant difficulties is the need to develop a system that\n",
            "can accurately capture and interpret the complex movements and gestures of the dancers. This\n",
            "requires the creation of sophisticated sensors and data collection systems, capable of tracking the\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 4\n",
            "--------------------\n",
            "subtle nuances of human movement and expression. Furthermore, the development of effective\n",
            "LSTM models requires large amounts of high-quality training data, which can be difficult to obtain,\n",
            "especially in the context of highly specialized and nuanced forms of dance such as flamenco.\n",
            "Despite these challenges, the potential benefits of using AR and LSTM-based gesture forecasting to\n",
            "evaluate group cohesion in coordinated dance rituals are substantial. By providing a more objective\n",
            "and quantitative means of assessing performance, these technologies can help to identify areas for\n",
            "improvement and optimize the training and rehearsal processes. Additionally, the use of AR can\n",
            "enhance the overall experience of the performance, allowing audience members to engage with the\n",
            "dance in new and innovative ways, and creating a more immersive and interactive experience.\n",
            "In a bizarre twist, some researchers have even begun to explore the use of LSTM-based gesture\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 5\n",
            "--------------------\n",
            "dance in new and innovative ways, and creating a more immersive and interactive experience.\n",
            "In a bizarre twist, some researchers have even begun to explore the use of LSTM-based gesture\n",
            "forecasting in conjunction with other, more unconventional forms of movement analysis, such as the\n",
            "study of chicken entrails and the patterns of tea leaves. While these approaches may seem unorthodox,\n",
            "they have reportedly yielded some surprising insights into the nature of group cohesion and the\n",
            "factors that contribute to successful coordinated dance performances. For example, one study found\n",
            "that the patterns of tea leaves could be used to predict the likelihood of a dancer stumbling or making\n",
            "a mistake, allowing for the development of targeted interventions and improvements to the rehearsal\n",
            "process.\n",
            "Furthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a\n",
            "number of unexpected benefits, such as improving the dancers’ ability to communicate with each\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 6\n",
            "--------------------\n",
            "process.\n",
            "Furthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a\n",
            "number of unexpected benefits, such as improving the dancers’ ability to communicate with each\n",
            "other through subtle cues and gestures. By providing a more nuanced and detailed understanding of\n",
            "the complex interactions between dancers, these technologies can help to facilitate a more cohesive\n",
            "and effective performance, and even enhance the overall artistic expression of the dance. In some\n",
            "cases, the use of AR has even been shown to alter the dancers’ perception of their own bodies and\n",
            "movements, allowing them to develop a greater sense of awareness and control over their actions.\n",
            "In addition to its practical applications, the study of coordinated dance rituals and group cohesion also\n",
            "raises a number of interesting theoretical questions, such as the nature of collective consciousness\n",
            "and the role of nonverbal communication in shaping group dynamics. By exploring these questions\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 7\n",
            "--------------------\n",
            "raises a number of interesting theoretical questions, such as the nature of collective consciousness\n",
            "and the role of nonverbal communication in shaping group dynamics. By exploring these questions\n",
            "through the lens of AR and LSTM-based gesture forecasting, researchers can gain a deeper under-\n",
            "standing of the complex factors that contribute to successful group performances, and develop new\n",
            "insights into the fundamental nature of human interaction and cooperation.\n",
            "The intersection of AR, LSTM-based gesture forecasting, and coordinated dance rituals also has\n",
            "significant implications for our understanding of the relationship between technology and art. As\n",
            "these technologies continue to evolve and improve, they are likely to have a profound impact on the\n",
            "way we experience and interact with dance and other forms of performance art. By providing new\n",
            "tools and platforms for creative expression, AR and LSTM-based gesture forecasting can help to\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 8\n",
            "--------------------\n",
            "way we experience and interact with dance and other forms of performance art. By providing new\n",
            "tools and platforms for creative expression, AR and LSTM-based gesture forecasting can help to\n",
            "push the boundaries of what is possible in the world of dance, and create new and innovative forms\n",
            "of artistic expression.\n",
            "Overall, the study of coordinated dance rituals and group cohesion through the lens of AR and LSTM-\n",
            "based gesture forecasting is a rich and complex field, full of surprising insights and unexpected\n",
            "discoveries. As researchers continue to explore the possibilities of these technologies, they are\n",
            "likely to uncover new and innovative ways of analyzing and understanding the complex dynamics\n",
            "of group performance, and develop new strategies for improving the cohesion and effectiveness of\n",
            "dance groups. Whether through the use of conventional methods or more unconventional approaches,\n",
            "such as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 9\n",
            "--------------------\n",
            "dance groups. Whether through the use of conventional methods or more unconventional approaches,\n",
            "such as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture\n",
            "forecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating\n",
            "and thought-provoking results.\n",
            "2 Related Work\n",
            "The intersection of augmented reality (AR) and synchronized flamenco dance has garnered significant\n",
            "attention in recent years, as researchers seek to harness the potential of immersive technologies to\n",
            "enhance group cohesion and interpersonal coordination. A plethora of studies have investigated\n",
            "the role of AR in facilitating collaborative dance performances, with a particular emphasis on the\n",
            "development of novel gesture recognition systems and predictive modeling techniques. Notably, the\n",
            "application of long short-term memory (LSTM) networks has emerged as a dominant approach in\n",
            "2\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 10\n",
            "--------------------\n",
            "the field, owing to their capacity to effectively capture the complex temporal dynamics of human\n",
            "movement.\n",
            "One intriguing line of inquiry has focused on the use of AR-enabled feedback loops to synchronize\n",
            "the movements of multiple dancers, thereby fostering a sense of collective rhythm and cohesion. This\n",
            "has involved the creation of bespoke AR systems that provide real-time visual and auditory cues to\n",
            "participants, allowing them to adjust their movements in accordance with the predicted gestures of\n",
            "their counterparts. Interestingly, some researchers have explored the incorporation of unconventional\n",
            "feedback modalities, such as tactile and olfactory stimuli, in an effort to further enhance the sense of\n",
            "immersion and interpersonal connection among dancers.\n",
            "A related thread of research has examined the potential of AR-based gesture forecasting to facilitate\n",
            "the creation of novel, AI-generated flamenco choreographies. By leveraging LSTM networks to\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 11\n",
            "--------------------\n",
            "immersion and interpersonal connection among dancers.\n",
            "A related thread of research has examined the potential of AR-based gesture forecasting to facilitate\n",
            "the creation of novel, AI-generated flamenco choreographies. By leveraging LSTM networks to\n",
            "predict the likelihood of specific gestures and movements, researchers have been able to generate\n",
            "Complex, algorithmically-driven dance sequences that can be performed in synchronization by\n",
            "multiple dancers. This has raised fascinating questions regarding the role of human agency and\n",
            "creativity in the development of AR-mediated choreographies, and has prompted some scholars\n",
            "to investigate the potential for hybrid human-AI collaborative frameworks that can facilitate the\n",
            "co-creation of innovative dance performances.\n",
            "In a somewhat unexpected turn, some researchers have begun to explore the application of AR and\n",
            "LSTM-based gesture forecasting in the context of non-human dance partners, such as robots and\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 12\n",
            "--------------------\n",
            "co-creation of innovative dance performances.\n",
            "In a somewhat unexpected turn, some researchers have begun to explore the application of AR and\n",
            "LSTM-based gesture forecasting in the context of non-human dance partners, such as robots and\n",
            "animals. This has involved the development of bespoke AR systems that can detect and predict\n",
            "the movements of these non-human entities, allowing human dancers to engage in synchronized\n",
            "performances with their artificial or animal counterparts. While this line of inquiry may seem\n",
            "unconventional, it has yielded some remarkable insights into the fundamental principles of movement\n",
            "and coordination, and has highlighted the potential for AR and machine learning to facilitate novel\n",
            "forms of interspecies collaboration and creativity.\n",
            "Furthermore, a number of studies have investigated the cultural and historical contexts of flamenco\n",
            "dance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 13\n",
            "--------------------\n",
            "forms of interspecies collaboration and creativity.\n",
            "Furthermore, a number of studies have investigated the cultural and historical contexts of flamenco\n",
            "dance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used\n",
            "to preserve and promote traditional flamenco practices. This has involved the creation of digital\n",
            "archives and repositories of flamenco choreographies, which can be used to train LSTM networks\n",
            "and generate new, AI-driven dance sequences that are grounded in the cultural heritage of flamenco.\n",
            "Interestingly, some researchers have also explored the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the development of new, fusion-based flamenco styles that blend traditional\n",
            "techniques with contemporary influences and innovations.\n",
            "In addition to these developments, there has been a growing interest in the use of AR and LSTM-based\n",
            "gesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 14\n",
            "--------------------\n",
            "In addition to these developments, there has been a growing interest in the use of AR and LSTM-based\n",
            "gesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal\n",
            "coordination in dance. This has involved the use of functional magnetic resonance imaging (fMRI) and\n",
            "electroencephalography (EEG) to study the brain activity of dancers as they engage in synchronized\n",
            "performances, and has yielded some fascinating insights into the neural mechanisms that underlie\n",
            "human movement and coordination. Moreover, some researchers have begun to explore the potential\n",
            "for AR and LSTM-based gesture forecasting to facilitate the development of novel, dance-based\n",
            "therapies for individuals with neurological or developmental disorders, such as autism and Parkinson’s\n",
            "disease.\n",
            "Theoretical frameworks, such as the concept of \"extended cognition,\" have also been applied to\n",
            "the study of AR and synchronized flamenco, highlighting the ways in which the use of immersive\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 15\n",
            "--------------------\n",
            "disease.\n",
            "Theoretical frameworks, such as the concept of \"extended cognition,\" have also been applied to\n",
            "the study of AR and synchronized flamenco, highlighting the ways in which the use of immersive\n",
            "technologies can facilitate the creation of shared, distributed cognitive systems that span the bound-\n",
            "aries of individual dancers. This has prompted some scholars to investigate the potential for AR and\n",
            "LSTM-based gesture forecasting to enable new forms of collective intelligence and creativity, in\n",
            "which the movements and gestures of individual dancers are used to generate emergent, group-level\n",
            "patterns and choreographies.\n",
            "Moreover, a growing body of research has examined the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the creation of novel, site-specific flamenco performances that are tailored\n",
            "to the unique architectural and environmental features of a given location. This has involved\n",
            "the development of bespoke AR systems that can detect and respond to the spatial and temporal\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 16\n",
            "--------------------\n",
            "to the unique architectural and environmental features of a given location. This has involved\n",
            "the development of bespoke AR systems that can detect and respond to the spatial and temporal\n",
            "characteristics of a performance environment, and has yielded some remarkable insights into the\n",
            "3\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 17\n",
            "--------------------\n",
            "ways in which the use of immersive technologies can be used to enhance the sense of presence and\n",
            "engagement among audience members.\n",
            "In an effort to further advance the field, some researchers have begun to explore the potential for AR\n",
            "and LSTM-based gesture forecasting to facilitate the development of novel, virtual reality (VR)-based\n",
            "flamenco experiences that can be accessed remotely by users around the world. This has raised\n",
            "important questions regarding the potential for VR and AR to democratize access to flamenco and\n",
            "other forms of dance, and has highlighted the need for further research into the social and cultural\n",
            "implications of these emerging technologies.\n",
            "Additionally, some scholars have investigated the potential for AR and LSTM-based gesture fore-\n",
            "casting to facilitate the creation of novel, data-driven flamenco choreographies that are generated\n",
            "using large datasets of human movement and gesture. This has involved the development of bespoke\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 18\n",
            "--------------------\n",
            "casting to facilitate the creation of novel, data-driven flamenco choreographies that are generated\n",
            "using large datasets of human movement and gesture. This has involved the development of bespoke\n",
            "machine learning algorithms that can analyze and interpret the complex patterns and structures that\n",
            "underlie human dance, and has yielded some fascinating insights into the fundamental principles of\n",
            "movement and coordination.\n",
            "The use of AR and LSTM-based gesture forecasting has also been explored in the context of dance\n",
            "education, where it has been used to create novel, interactive learning systems that can provide\n",
            "real-time feedback and guidance to students. This has raised important questions regarding the\n",
            "potential for AR and machine learning to facilitate the development of more effective and engaging\n",
            "dance pedagogies, and has highlighted the need for further research into the cognitive and neural\n",
            "basis of dance learning and expertise.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 19\n",
            "--------------------\n",
            "potential for AR and machine learning to facilitate the development of more effective and engaging\n",
            "dance pedagogies, and has highlighted the need for further research into the cognitive and neural\n",
            "basis of dance learning and expertise.\n",
            "Some researchers have also begun to investigate the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the creation of novel, immersive flamenco experiences that incorporate\n",
            "multiple sensory modalities, such as sound, touch, and smell. This has involved the development of\n",
            "bespoke AR systems that can provide a range of multisensory stimuli to users, and has yielded some\n",
            "remarkable insights into the ways in which the use of immersive technologies can enhance the sense\n",
            "of presence and engagement among audience members.\n",
            "The integration of AR and LSTM-based gesture forecasting with other emerging technologies, such\n",
            "as the Internet of Things (IoT) and artificial intelligence (AI), has also been explored in the context of\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 20\n",
            "--------------------\n",
            "of presence and engagement among audience members.\n",
            "The integration of AR and LSTM-based gesture forecasting with other emerging technologies, such\n",
            "as the Internet of Things (IoT) and artificial intelligence (AI), has also been explored in the context of\n",
            "flamenco and dance. This has raised important questions regarding the potential for these technologies\n",
            "to facilitate the creation of novel, hybrid forms of dance and performance that combine human and\n",
            "machine elements, and has highlighted the need for further research into the social and cultural\n",
            "implications of these developments.\n",
            "In another vein, some scholars have begun to investigate the potential for AR and LSTM-based\n",
            "gesture forecasting to facilitate the creation of novel, participatory flamenco performances that involve\n",
            "the active engagement of audience members. This has involved the development of bespoke AR\n",
            "systems that can detect and respond to the movements and gestures of audience members, and has\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 21\n",
            "--------------------\n",
            "the active engagement of audience members. This has involved the development of bespoke AR\n",
            "systems that can detect and respond to the movements and gestures of audience members, and has\n",
            "yielded some fascinating insights into the ways in which the use of immersive technologies can\n",
            "facilitate the creation of more interactive and immersive forms of dance and performance.\n",
            "Finally, a growing body of research has examined the potential for AR and LSTM-based gesture\n",
            "forecasting to facilitate the preservation and promotion of traditional flamenco practices and cultural\n",
            "heritage. This has involved the creation of digital archives and repositories of flamenco choreogra-\n",
            "phies, which can be used to train LSTM networks and generate new, AI-driven dance sequences that\n",
            "are grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored\n",
            "the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel,\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 22\n",
            "--------------------\n",
            "are grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored\n",
            "the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel,\n",
            "fusion-based flamenco styles that blend traditional techniques with contemporary influences and\n",
            "innovations, highlighting the potential for these emerging technologies to facilitate the creation of\n",
            "new, hybrid forms of cultural expression and identity.\n",
            "3 Methodology\n",
            "To investigate the relationship between Augmented Reality (AR) and synchronized Flamenco dance,\n",
            "we employed a multidisciplinary approach, combining techniques from computer science, psychology,\n",
            "and dance theory. Our methodology consisted of several stages, including data collection, participant\n",
            "recruitment, and the development of a bespoke LSTM-based gesture forecasting system. We began\n",
            "by recruiting a cohort of 50 experienced Flamenco dancers, who were tasked with performing\n",
            "4\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 23\n",
            "--------------------\n",
            "a series of coordinated dance rituals while wearing AR-enabled wristbands. These wristbands,\n",
            "which we designed and fabricated in-house, utilized a combination of accelerometer, gyroscope, and\n",
            "magnetometer sensors to capture the dancers’ movements with high spatial and temporal resolution.\n",
            "The AR component of our system was implemented using a custom-built application, which utilized\n",
            "a headset-mounted display to provide the dancers with real-time feedback on their movements. This\n",
            "feedback took the form of a virtual \"gesture trail,\" which allowed the dancers to visualize their own\n",
            "movements, as well as those of their peers, in a shared virtual environment. We hypothesized that\n",
            "this shared feedback mechanism would facilitate enhanced group cohesion and coordination among\n",
            "the dancers, and we designed a series of experiments to test this hypothesis.\n",
            "One of the key challenges we faced in developing our system was the need to balance the requirements\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 24\n",
            "--------------------\n",
            "the dancers, and we designed a series of experiments to test this hypothesis.\n",
            "One of the key challenges we faced in developing our system was the need to balance the requirements\n",
            "of real-time feedback and high-fidelity motion capture. To address this challenge, we implemented a\n",
            "novel approach, which we term \"temporally-compressed gesture forecasting.\" This approach involves\n",
            "using a combination of machine learning algorithms and signal processing techniques to compress\n",
            "the temporal dimension of the motion capture data, while preserving the underlying patterns and\n",
            "structures of the dancers’ movements. We found that this approach allowed us to achieve high-quality\n",
            "motion capture data, while also reducing the computational overhead of our system and enabling\n",
            "real-time feedback.\n",
            "In addition to the technical challenges, we also encountered a number of unexpected issues during the\n",
            "data collection process. For example, we found that the dancers’ movements were often influenced\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 25\n",
            "--------------------\n",
            "real-time feedback.\n",
            "In addition to the technical challenges, we also encountered a number of unexpected issues during the\n",
            "data collection process. For example, we found that the dancers’ movements were often influenced\n",
            "by a range of external factors, including the music, the lighting, and even the color of the walls in\n",
            "the dance studio. To address these issues, we developed a novel \"context-aware\" gesture forecasting\n",
            "system, which utilized a combination of environmental sensors and machine learning algorithms\n",
            "to predict the dancers’ movements based on the surrounding context. We found that this approach\n",
            "allowed us to achieve significantly improved accuracy in our gesture forecasting model, and we\n",
            "were able to demonstrate a strong positive correlation between the predicted gestures and the actual\n",
            "movements of the dancers.\n",
            "Another unexpected finding that emerged from our research was the discovery that the dancers’\n",
            "movements were often influenced by a range of subconscious factors, including their emotional\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 26\n",
            "--------------------\n",
            "movements of the dancers.\n",
            "Another unexpected finding that emerged from our research was the discovery that the dancers’\n",
            "movements were often influenced by a range of subconscious factors, including their emotional\n",
            "state, their level of fatigue, and even their personal relationships with their fellow dancers. To\n",
            "investigate this phenomenon, we developed a novel \"emotional contagion\" framework, which utilized\n",
            "a combination of psychological surveys, physiological sensors, and machine learning algorithms to\n",
            "predict the emotional state of the dancers based on their movements. We found that this approach\n",
            "allowed us to identify a range of subtle patterns and correlations in the data, which would have been\n",
            "difficult or impossible to detect using more traditional methods.\n",
            "We also explored the use of unconventional machine learning architectures, such as a bespoke\n",
            "\"Flamenco-inspired\" neural network, which was designed to mimic the complex rhythms and patterns\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 27\n",
            "--------------------\n",
            "difficult or impossible to detect using more traditional methods.\n",
            "We also explored the use of unconventional machine learning architectures, such as a bespoke\n",
            "\"Flamenco-inspired\" neural network, which was designed to mimic the complex rhythms and patterns\n",
            "of traditional Flamenco music. This approach involved using a combination of convolutional and\n",
            "recurrent neural network layers to model the temporal and spatial structure of the dancers’ movements,\n",
            "and we found that it allowed us to achieve state-of-the-art performance in gesture forecasting and\n",
            "recognition. However, we also encountered a number of challenges and limitations when working\n",
            "with this approach, including the need for large amounts of labeled training data and the risk of\n",
            "overfitting to the specific patterns and structures of the Flamenco dance style.\n",
            "In an effort to further enhance the accuracy and robustness of our system, we also investigated the use\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 28\n",
            "--------------------\n",
            "overfitting to the specific patterns and structures of the Flamenco dance style.\n",
            "In an effort to further enhance the accuracy and robustness of our system, we also investigated the use\n",
            "of a range of alternative and complementary sensing modalities, including electromyography (EMG),\n",
            "electroencephalography (EEG), and functional near-infrared spectroscopy (fNIRS). We found that\n",
            "these modalities provided a rich source of additional information about the dancers’ movements\n",
            "and emotional state, and we were able to integrate them into our existing system using a range of\n",
            "sensor fusion and machine learning techniques. However, we also encountered a number of practical\n",
            "challenges and limitations when working with these modalities, including the need for specialized\n",
            "equipment and expertise, and the risk of signal noise and artifact contamination.\n",
            "Despite these challenges, we were able to demonstrate the effectiveness of our approach in a range of\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 29\n",
            "--------------------\n",
            "equipment and expertise, and the risk of signal noise and artifact contamination.\n",
            "Despite these challenges, we were able to demonstrate the effectiveness of our approach in a range of\n",
            "experimental evaluations, including a large-scale study involving over 100 participants and a series\n",
            "of smaller-scale pilots and proof-of-concept demonstrations. We found that our system was able\n",
            "to achieve high levels of accuracy and robustness in gesture forecasting and recognition, and we\n",
            "were able to demonstrate a strong positive correlation between the predicted gestures and the actual\n",
            "5\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 30\n",
            "--------------------\n",
            "movements of the dancers. We also received positive feedback from the participants, who reported\n",
            "that the system was easy to use and provided a range of benefits, including improved coordination and\n",
            "cohesion, enhanced creativity and self-expression, and increased overall enjoyment and engagement.\n",
            "In conclusion, our research demonstrates the potential of AR and LSTM-based gesture forecasting\n",
            "to enhance group cohesion and coordination in coordinated dance rituals. While our approach is\n",
            "still in the early stages of development, we believe that it has the potential to make a significant\n",
            "impact in a range of applications, from dance and performance to education and therapy. We are\n",
            "excited to continue exploring the possibilities of this technology, and we look forward to seeing\n",
            "where it will take us in the future. We are also considering exploring other genres of dance, such as\n",
            "ballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 31\n",
            "--------------------\n",
            "where it will take us in the future. We are also considering exploring other genres of dance, such as\n",
            "ballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are\n",
            "planning to investigate the use of our system in other domains, such as sports or rehabilitation, where\n",
            "coordinated movement and gesture forecasting could be beneficial. Overall, our research highlights\n",
            "the potential of interdisciplinary approaches to drive innovation and advance our understanding of\n",
            "complex phenomena, and we are excited to see where this line of inquiry will lead us in the future.\n",
            "4 Experiments\n",
            "To conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and\n",
            "synchronized flamenco, we designed a series of experiments that would not only assess the impact of\n",
            "AR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\n",
            "Memory (LSTM) networks. The experiments were carried out over the course of several months,\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 32\n",
            "--------------------\n",
            "AR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\n",
            "Memory (LSTM) networks. The experiments were carried out over the course of several months,\n",
            "involving a diverse group of participants with varying levels of experience in flamenco dance.\n",
            "The experimental setup consisted of a large, specially designed dance studio equipped with AR\n",
            "technology that could project a myriad of patterns and cues onto the floor and surrounding walls.\n",
            "This allowed the dancers to receive real-time feedback and guidance on their movements, which was\n",
            "expected to enhance their synchronization and overall performance. The studio was also outfitted\n",
            "with a state-of-the-art motion capture system, capable of tracking the precise movements of each\n",
            "dancer, thus providing valuable data for the LSTM-based gesture forecasting model.\n",
            "Before commencing the experiments, all participants underwent an intensive training program aimed\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 33\n",
            "--------------------\n",
            "dancer, thus providing valuable data for the LSTM-based gesture forecasting model.\n",
            "Before commencing the experiments, all participants underwent an intensive training program aimed\n",
            "at familiarizing them with the basics of flamenco and the operation of the AR system. This included\n",
            "understanding how to interpret the AR cues, how to adjust their movements based on the feedback\n",
            "received, and how to work cohesively as a group. The training program was divided into two\n",
            "phases: the first phase focused on individual skill development, where each participant learned the\n",
            "fundamental steps and rhythms of flamenco. The second phase concentrated on group cohesion,\n",
            "where participants practiced dancing together, emphasizing synchronization and coordination.\n",
            "Upon completing the training program, the participants were divided into several groups, each with a\n",
            "distinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 34\n",
            "--------------------\n",
            "Upon completing the training program, the participants were divided into several groups, each with a\n",
            "distinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while\n",
            "others were deliberately mixed to include beginners, intermediate, and advanced dancers. This\n",
            "diversity was intended to observe how different group compositions affected cohesion and the ability\n",
            "to forecast gestures accurately.\n",
            "The experimental protocol involved several sessions, each lasting approximately two hours. During\n",
            "these sessions, the dancers performed a variety of flamenco routines, with and without the AR\n",
            "feedback. Their movements were captured by the motion tracking system, and the data were fed into\n",
            "the LSTM model for analysis. The model was tasked with predicting the next gesture or movement\n",
            "based on the patterns observed in the data. Interestingly, the model began to exhibit an unexpected\n",
            "behavior, frequently predicting movements that seemed unrelated to flamenco, such as gestures from\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 35\n",
            "--------------------\n",
            "based on the patterns observed in the data. Interestingly, the model began to exhibit an unexpected\n",
            "behavior, frequently predicting movements that seemed unrelated to flamenco, such as gestures from\n",
            "ballet or even what appeared to be fragments of a traditional African dance. This phenomenon, which\n",
            "we termed \"Cross-Cultural Gesture Drift,\" posed an intriguing question about the potential for LSTM\n",
            "models to not only learn from the data they are trained on but also to draw from a broader, unexplored\n",
            "reservoir of cultural knowledge.\n",
            "To further explore this phenomenon, we introduced an unconventional variable into our experiment:\n",
            "the influence of ambient music from different cultural backgrounds on the dancers’ movements\n",
            "and the LSTM’s predictions. The results were astounding, with the model’s predictions becoming\n",
            "increasingly eclectic and incorporating elements from the ambient music genres. For instance, when\n",
            "the background music shifted to a vibrant salsa rhythm, the model began to predict movements that\n",
            "6\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 36\n",
            "--------------------\n",
            "were distinctly more energetic and spontaneous, diverging significantly from the traditional flamenco\n",
            "repertoire. Conversely, when the ambient music was a soothing melody from a Japanese traditional\n",
            "instrument, the predictions became more subdued and introspective, reflecting the serene quality of\n",
            "the music.\n",
            "Table 1: Cross-Cultural Gesture Drift Observations\n",
            "Session Ambient Music Genre Predicted Gestures Divergence from Flamenco\n",
            "1 Traditional Flamenco High accuracy, minimal divergence 5%\n",
            "2 African Folk Introduction of non-flamenco gestures 20%\n",
            "3 Contemporary Ballet Predictions included ballet movements 35%\n",
            "4 Salsa Increased energy and spontaneity 40%\n",
            "5 Japanese Traditional Predictions became more subdued 15%\n",
            "The incorporation of ambient music and the observation of Cross-Cultural Gesture Drift added a\n",
            "new layer of complexity to our study, suggesting that the relationship between AR, flamenco, and\n",
            "gesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 37\n",
            "--------------------\n",
            "new layer of complexity to our study, suggesting that the relationship between AR, flamenco, and\n",
            "gesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues\n",
            "for research, including the potential for using AR and LSTM models to create new, hybrid dance\n",
            "forms that blend elements from different cultural traditions. Furthermore, it raises questions about\n",
            "the role of technology in preserving cultural heritage versus promoting innovation and fusion.\n",
            "In a bizarre turn of events, one of the sessions was interrupted by an unexpected visit from a group of\n",
            "wild flamenco enthusiasts, who, upon witnessing the experiment, spontaneously joined in, adding\n",
            "their own flair and energy to the performance. This unplanned intrusion not only disrupted the\n",
            "controlled environment of the experiment but also led to one of the most captivating and cohesive\n",
            "performances observed throughout the study. The LSTM model, faced with this unexpected input,\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 38\n",
            "--------------------\n",
            "controlled environment of the experiment but also led to one of the most captivating and cohesive\n",
            "performances observed throughout the study. The LSTM model, faced with this unexpected input,\n",
            "surprisingly adapted and began to predict gestures that were not only accurate but also seemed to\n",
            "capture the essence and passion of the impromptu dancers.\n",
            "This serendipitous event underscored the importance of spontaneity and community in dance, as well\n",
            "as the potential for AR and LSTM models to facilitate and enhance these aspects. It also highlighted\n",
            "the limitations of controlled experiments in fully capturing the dynamic, often unpredictable nature\n",
            "of human creativity and expression. In response, we have begun to explore the development of more\n",
            "flexible, adaptive experimental designs that can accommodate and even encourage unexpected events,\n",
            "viewing them as opportunities for growth and discovery rather than disruptions to be controlled.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 39\n",
            "--------------------\n",
            "flexible, adaptive experimental designs that can accommodate and even encourage unexpected events,\n",
            "viewing them as opportunities for growth and discovery rather than disruptions to be controlled.\n",
            "The experiments concluded with a grand finale, where all participants gathered for a final, AR-guided\n",
            "flamenco performance. The event was open to the public and attracted a diverse audience, all of whom\n",
            "were mesmerized by the synchronization, energy, and evident joy of the dancers. The LSTM model,\n",
            "having learned from the myriad of experiences and data collected throughout the study, performed\n",
            "flawlessly, predicting gestures with a high degree of accuracy and even seeming to contribute to the\n",
            "spontaneity and creativity of the performance.\n",
            "In reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based\n",
            "gesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 40\n",
            "--------------------\n",
            "spontaneity and creativity of the performance.\n",
            "In reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based\n",
            "gesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into\n",
            "uncharted territories, exploring the intersection of technology, culture, and human expression. The\n",
            "findings, replete with unexpected turns and surprising revelations, underscore the complexity and\n",
            "richness of this intersection, beckoning further research and innovation in this captivating field.\n",
            "5 Results\n",
            "Our investigation into the intersection of Augmented Reality (AR) and synchronized flamenco\n",
            "dancing, with a focus on evaluating group cohesion through LSTM-based gesture forecasting,yielded\n",
            "a plethora of intriguing results. Initially, we observed that the integration of AR elements into\n",
            "the flamenco performances enhanced the dancers’ ability to synchronize their movements, thereby\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 41\n",
            "--------------------\n",
            "a plethora of intriguing results. Initially, we observed that the integration of AR elements into\n",
            "the flamenco performances enhanced the dancers’ ability to synchronize their movements, thereby\n",
            "fostering a heightened sense of group cohesion. This phenomenon was particularly evident when\n",
            "the AR components were designed to provide real-time feedback on gesture accuracy and timing,\n",
            "allowing the dancers to adjust their movements in tandem.\n",
            "The LSTM-based gesture forecasting model, trained on a dataset comprising various flamenco dance\n",
            "sequences, demonstrated a remarkable capacity to predict the subsequent gestures of individual\n",
            "7\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 42\n",
            "--------------------\n",
            "dancers. Notably, when this predictive capability was leveraged to generate AR cues that guided\n",
            "the dancers’ movements, the overall cohesion of the group improved significantly. However, an\n",
            "unexpected outcome emerged when the model was fed a dataset that included gestures from other,\n",
            "unrelated dance forms, such as ballet and hip-hop. In these instances, the LSTM model began to\n",
            "generate forecasts that, while inaccurate in the context of flamenco, inadvertently created a unique\n",
            "fusion of dance styles. This unforeseen development led to the creation of novel, AR-infused dance\n",
            "routines that, despite their lack of traditional flamenco authenticity, exhibited a captivating blend of\n",
            "movements.\n",
            "Further analysis revealed that the predictive accuracy of the LSTM model was influenced by the\n",
            "dancers’ emotional states, as captured through wearable, physiological sensors. Specifically, the\n",
            "model’s performance improved when the dancers were in a state of heightened arousal or excitement,\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 43\n",
            "--------------------\n",
            "dancers’ emotional states, as captured through wearable, physiological sensors. Specifically, the\n",
            "model’s performance improved when the dancers were in a state of heightened arousal or excitement,\n",
            "suggesting that emotional investment in the performance enhances the efficacy of the gesture forecast-\n",
            "ing. Conversely, periods of low emotional engagement resulted in diminished forecasting accuracy,\n",
            "underscoring the importance of emotional connection in the success of AR-augmented, synchronized\n",
            "flamenco.\n",
            "In a bizarre twist, our research team discovered that the LSTM model, when trained on a dataset\n",
            "that included gestures performed by dancers who were blindfolded, developed an uncanny ability to\n",
            "predict movements that were not strictly flamenco in nature. These predictions, which seemed to\n",
            "defy logical explanation, often involved complex, almost acrobatic movements that, when executed,\n",
            "appeared to transcend the traditional boundaries of flamenco dance. While these findings may seem\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 44\n",
            "--------------------\n",
            "defy logical explanation, often involved complex, almost acrobatic movements that, when executed,\n",
            "appeared to transcend the traditional boundaries of flamenco dance. While these findings may seem\n",
            "illogical or even flawed, they nevertheless contribute to our understanding of the intricate relationships\n",
            "between gesture, emotion, and AR-augmented performance.\n",
            "The results of our experiments are summarized in the following table: As evidenced by the table, the\n",
            "Table 2: LSTM Model Performance Under Various Conditions\n",
            "Condition Predictive Accuracy Emotional State Dance Style AR Cue Efficacy\n",
            "Traditional Flamenco 0.85 High Arousal Flamenco High\n",
            "Fusion Dance 0.70 Medium Engagement Hybrid Medium\n",
            "Blindfolded Gestures 0.90 Low Arousal Non-Traditional Low\n",
            "Ballet-Influenced Flamenco 0.60 High Excitement Ballet-Flamenco High\n",
            "LSTM model’s performance varies significantly depending on the specific conditions under which it\n",
            "is applied. Notably, the model’s predictive accuracy is highest when dealing with traditional flamenco\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 45\n",
            "--------------------\n",
            "LSTM model’s performance varies significantly depending on the specific conditions under which it\n",
            "is applied. Notably, the model’s predictive accuracy is highest when dealing with traditional flamenco\n",
            "gestures, but its ability to generate novel, hybrid movements is most pronounced when confronted\n",
            "with blindfolded gestures or ballet-influenced flamenco.\n",
            "The implications of these findings are far-reaching, suggesting that the integration of AR and LSTM-\n",
            "based gesture forecasting can not only enhance group cohesion in synchronized flamenco but also\n",
            "facilitate the creation of innovative, boundary-pushing dance forms. Furthermore, the influence of\n",
            "emotional state on predictive accuracy highlights the importance of considering the emotional and\n",
            "psychological aspects of dance performance in the development of AR-augmented systems. As our\n",
            "research continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 46\n",
            "--------------------\n",
            "psychological aspects of dance performance in the development of AR-augmented systems. As our\n",
            "research continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate\n",
            "uncovering even more unexpected and thought-provoking results that challenge our understanding of\n",
            "the complex interplay between technology, movement, and human emotion.\n",
            "In an effort to further elucidate the relationships between these factors, we plan to conduct additional\n",
            "experiments that delve into the cognitive and neurological underpinnings of AR-augmented dance\n",
            "performance. By investigating the neural correlates of gesture forecasting and emotional engagement,\n",
            "we hope to gain a deeper understanding of the underlying mechanisms that drive the observed\n",
            "phenomena. This, in turn, will enable the development of more sophisticated AR systems that can\n",
            "adapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall\n",
            "efficacy and aesthetic appeal of synchronized flamenco performances.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 47\n",
            "--------------------\n",
            "adapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall\n",
            "efficacy and aesthetic appeal of synchronized flamenco performances.\n",
            "Ultimately, our research endeavors to push the boundaries of what is possible at the confluence of AR,\n",
            "flamenco, and gesture forecasting, embracing the unexpected and the bizarre as integral components\n",
            "of the creative process. By doing so, we aim to contribute to the evolution of dance as an art form, one\n",
            "that seamlessly integrates technology, movement, and human emotion to create novel, captivating,\n",
            "and unforgettable experiences. The potential applications of this research extend far beyond the realm\n",
            "8\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 48\n",
            "--------------------\n",
            "of dance, with implications for fields such as human-computer interaction, cognitive psychology, and\n",
            "even therapy, where AR-augmented systems could be leveraged to enhance motor skills, emotional\n",
            "regulation, and social cohesion.\n",
            "As we continue to explore the vast expanse of possibilities at the intersection of AR and synchronized\n",
            "flamenco, we are reminded that the most profound discoveries often arise from the most unlikely\n",
            "of places. It is our hope that this research will inspire others to embrace the unconventional, the\n",
            "unexpected, and the bizarre, for it is within these uncharted territories that we may uncover the most\n",
            "groundbreaking insights and innovative solutions. By embracing the complexities and uncertainties\n",
            "of this multidisciplinary endeavor, we may yet uncover new and exciting ways to augment, enhance,\n",
            "and transform the human experience through the judicious application of technology and the timeless\n",
            "power of dance.\n",
            "6 Conclusion\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 49\n",
            "--------------------\n",
            "of this multidisciplinary endeavor, we may yet uncover new and exciting ways to augment, enhance,\n",
            "and transform the human experience through the judicious application of technology and the timeless\n",
            "power of dance.\n",
            "6 Conclusion\n",
            "In culmination of our exhaustive exploration into the realm of Augmented Reality and Synchronized\n",
            "Flamenco, it is unequivocally evident that the deployment of LSTM-based gesture forecasting in\n",
            "coordinated dance rituals has yielded a profound impact on the evaluation of group cohesion. The\n",
            "intricate dynamics at play within the flamenco dance form, characterized by its impassioned gestures\n",
            "and synchronized movements, have been adeptly harnessed and analyzed through the prism of cutting-\n",
            "edge artificial intelligence techniques. By doing so, we have not only delved into the uncharted\n",
            "territories of human-computer interaction but also teasingly treaded the boundaries of art and science,\n",
            "often blurring the lines between the two.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 50\n",
            "--------------------\n",
            "edge artificial intelligence techniques. By doing so, we have not only delved into the uncharted\n",
            "territories of human-computer interaction but also teasingly treaded the boundaries of art and science,\n",
            "often blurring the lines between the two.\n",
            "One of the most fascinating aspects of our research has been the observation that the implementation\n",
            "of Augmented Reality in flamenco dance has led to an unexpected, yet intriguing, phenomenon where\n",
            "dancers began to exhibit a heightened sense of empathy towards each other. This empathy, in turn,\n",
            "has been found to positively correlate with the level of group cohesion, suggesting that the immersive\n",
            "experience provided by Augmented Reality fosters a deeper sense of connection among participants.\n",
            "Furthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to\n",
            "predict the intricate hand movements of the dancers, which has been shown to be a critical factor in\n",
            "evaluating the overall synchrony of the dance performance.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 51\n",
            "--------------------\n",
            "Furthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to\n",
            "predict the intricate hand movements of the dancers, which has been shown to be a critical factor in\n",
            "evaluating the overall synchrony of the dance performance.\n",
            "In a bizarre twist, our research has also led us to investigate the role of chaos theory in understanding\n",
            "the complex dynamics of flamenco dance. By applying the principles of chaos theory, we have\n",
            "discovered that the seemingly random and unpredictable movements of the dancers can, in fact, be\n",
            "modeled using nonlinear differential equations. This has profound implications for our understanding\n",
            "of coordinated dance rituals, as it suggests that the emergent patterns of behavior that arise from\n",
            "the interactions among individual dancers can be understood and predicted using mathematical\n",
            "frameworks. Moreover, the application of chaos theory has also led us to explore the concept of\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 52\n",
            "--------------------\n",
            "the interactions among individual dancers can be understood and predicted using mathematical\n",
            "frameworks. Moreover, the application of chaos theory has also led us to explore the concept of\n",
            "\"flamenco attractors,\" which are hypothetical states of maximum synchrony and cohesion that the\n",
            "dancers can strive towards.\n",
            "Moreover, our study has also explored the tangential relationship between flamenco dance and the\n",
            "principles of quantum mechanics. In a series of unconventional experiments, we have found that the\n",
            "principles of superposition and entanglement can be used to describe the complex interactions between\n",
            "dancers and their environment. This has led us to propose the concept of \"quantum flamenco,\" where\n",
            "the dancers and their surroundings are viewed as an interconnected, holistic system that can be\n",
            "described using the mathematical frameworks of quantum mechanics. While this approach may seem\n",
            "unorthodox, it has yielded some surprising insights into the nature of group cohesion and coordinated\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 53\n",
            "--------------------\n",
            "described using the mathematical frameworks of quantum mechanics. While this approach may seem\n",
            "unorthodox, it has yielded some surprising insights into the nature of group cohesion and coordinated\n",
            "behavior, suggesting that the boundaries between art and science are far more fluid than previously\n",
            "thought.\n",
            "The implications of our research are far-reaching and multifaceted, with potential applications\n",
            "in fields such as psychology, sociology, and computer science. By exploring the intersection of\n",
            "Augmented Reality, flamenco dance, and artificial intelligence, we have opened up new avenues for\n",
            "understanding human behavior, social interaction, and the emergence of complex patterns in group\n",
            "dynamics. Furthermore, our study has also highlighted the importance of interdisciplinary research,\n",
            "demonstrating that the fusion of seemingly disparate fields can lead to innovative and groundbreaking\n",
            "discoveries.\n",
            "9\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 54\n",
            "--------------------\n",
            "In an intriguing aside, our research has also led us to investigate the potential therapeutic applications\n",
            "of flamenco dance in treating neurological disorders such as Parkinson’s disease. By analyzing\n",
            "the brain activity of patients who participated in flamenco dance sessions, we have found that the\n",
            "rhythmic movements and synchronized gestures can have a profound impact on motor control and\n",
            "cognitive function. This has led us to propose the concept of \"flamenco therapy,\" where the immersive\n",
            "experience of flamenco dance is used as a form of rehabilitation for patients with neurological\n",
            "disorders.\n",
            "Ultimately, our research has demonstrated that the evaluation of group cohesion via LSTM-based\n",
            "gesture forecasting in coordinated dance rituals is a rich and complex field that offers a wide range\n",
            "of opportunities for exploration and discovery. By embracing the intersection of art and science,\n",
            "and by venturing into uncharted territories of human-computer interaction, we have gained a deeper\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 55\n",
            "--------------------\n",
            "of opportunities for exploration and discovery. By embracing the intersection of art and science,\n",
            "and by venturing into uncharted territories of human-computer interaction, we have gained a deeper\n",
            "understanding of the intricate dynamics that govern human behavior and social interaction. As\n",
            "we continue to push the boundaries of this field, we are excited to see the new and innovative\n",
            "applications that will emerge, and we are confident that our research will have a lasting impact on our\n",
            "understanding of group cohesion and coordinated behavior.\n",
            "The potential for future research in this area is vast and varied, with opportunities to explore new\n",
            "modes of human-computer interaction, to develop more sophisticated AI models for gesture forecast-\n",
            "ing, and to investigate the therapeutic applications of flamenco dance in a wider range of contexts.\n",
            "Moreover, the implications of our research extend far beyond the realm of flamenco dance, with\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 56\n",
            "--------------------\n",
            "ing, and to investigate the therapeutic applications of flamenco dance in a wider range of contexts.\n",
            "Moreover, the implications of our research extend far beyond the realm of flamenco dance, with\n",
            "potential applications in fields such as robotics, computer vision, and social psychology. As we look\n",
            "to the future, we are eager to see how our research will be built upon and expanded, and we are\n",
            "confident that the study of Augmented Reality and Synchronized Flamenco will continue to yield\n",
            "new and exciting insights into the complex and fascinating world of human behavior.\n",
            "In addition to the theoretical and practical implications of our research, we have also been struck\n",
            "by the aesthetic and artistic dimensions of flamenco dance, and the ways in which it can be used to\n",
            "create new and innovative forms of expression. By combining the traditional rhythms and movements\n",
            "of flamenco with the cutting-edge technologies of Augmented Reality and AI, we have been able\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 57\n",
            "--------------------\n",
            "create new and innovative forms of expression. By combining the traditional rhythms and movements\n",
            "of flamenco with the cutting-edge technologies of Augmented Reality and AI, we have been able\n",
            "to create a new and unique form of dance that is at once both deeply rooted in tradition and boldly\n",
            "innovative. This has led us to propose the concept of \"cyborg flamenco,\" where the boundaries\n",
            "between human and machine are blurred, and the dancer becomes a hybrid entity that is both physical\n",
            "and virtual.\n",
            "The concept of cyborg flamenco has far-reaching implications for our understanding of the relationship\n",
            "between human and machine, and the ways in which technology can be used to enhance and transform\n",
            "human performance. By exploring the intersection of flamenco dance and cutting-edge technology,\n",
            "we have been able to create a new and innovative form of expression that is at once both deeply\n",
            "human and profoundly technological. This has led us to propose a new paradigm for human-computer\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 58\n",
            "--------------------\n",
            "we have been able to create a new and innovative form of expression that is at once both deeply\n",
            "human and profoundly technological. This has led us to propose a new paradigm for human-computer\n",
            "interaction, one that views the human and the machine as interconnected and interdependent entities\n",
            "that can be used to create new and innovative forms of art and expression.\n",
            "Furthermore, our research has also led us to explore the cultural and historical dimensions of flamenco\n",
            "dance, and the ways in which it has been shaped by the complex and often fraught history of Spain.\n",
            "By analyzing the historical and cultural context of flamenco, we have been able to gain a deeper\n",
            "understanding of the ways in which this dance form has been used as a means of expression and\n",
            "resistance, and the ways in which it continues to be an important part of Spanish culture and identity.\n",
            "This has led us to propose the concept of \"flamenco as resistance,\" where the dance is viewed as a\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 59\n",
            "--------------------\n",
            "resistance, and the ways in which it continues to be an important part of Spanish culture and identity.\n",
            "This has led us to propose the concept of \"flamenco as resistance,\" where the dance is viewed as a\n",
            "form of cultural and political resistance that has been used to challenge and subvert dominant power\n",
            "structures.\n",
            "The concept of flamenco as resistance has far-reaching implications for our understanding of the\n",
            "relationship between culture and power, and the ways in which art and expression can be used as\n",
            "a means of challenging and transforming dominant ideologies. By exploring the intersection of\n",
            "flamenco dance and cultural resistance, we have been able to gain a deeper understanding of the\n",
            "ways in which this dance form has been used as a means of expressing and challenging dominant\n",
            "power structures, and the ways in which it continues to be an important part of Spanish culture and\n",
            "identity. This has led us to propose a new paradigm for understanding the relationship between\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 60\n",
            "--------------------\n",
            "power structures, and the ways in which it continues to be an important part of Spanish culture and\n",
            "identity. This has led us to propose a new paradigm for understanding the relationship between\n",
            "culture and power, one that views art and expression as a means of challenging and transforming\n",
            "dominant ideologies.\n",
            "10\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 61\n",
            "--------------------\n",
            "Ultimately, our research has demonstrated that the study of Augmented Reality and Synchronized\n",
            "Flamenco is a rich and complex field that offers a wide range of opportunities for exploration and\n",
            "discovery. By embracing the intersection of art and science, and by venturing into uncharted territories\n",
            "of human-computer interaction, we have gained a deeper understanding of the intricate dynamics that\n",
            "govern human behavior and social interaction. As we continue to push the boundaries of this field, we\n",
            "are excited to see the new and innovative applications that will emerge, and we are confident that our\n",
            "research will have a lasting impact on our understanding of group cohesion and coordinated behavior.\n",
            "11\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" An LSTM-Based\\nGesture Modeling Approach\\nAbstract\\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\\nresearch avenue to explore group cohesion through gesture forecasting\"\n",
        "result = db.similarity_search(query)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "pZF3ICneyWOw",
        "outputId": "bdbbda41-6c53-4454-8c6d-1536bf86aa18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='180bf234-2b70-4b44-a105-b30111b031b1', metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 1}, page_content='dance groups. Whether through the use of conventional methods or more unconventional approaches,\\nsuch as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture\\nforecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating\\nand thought-provoking results.\\n2 Related Work\\nThe intersection of augmented reality (AR) and synchronized flamenco dance has garnered significant\\nattention in recent years, as researchers seek to harness the potential of immersive technologies to\\nenhance group cohesion and interpersonal coordination. A plethora of studies have investigated\\nthe role of AR in facilitating collaborative dance performances, with a particular emphasis on the\\ndevelopment of novel gesture recognition systems and predictive modeling techniques. Notably, the\\napplication of long short-term memory (LSTM) networks has emerged as a dominant approach in\\n2'),\n",
              " Document(id='490586ea-9dda-432d-b5b8-e92e55b7ed58', metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 6}, page_content='spontaneity and creativity of the performance.\\nIn reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based\\ngesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into\\nuncharted territories, exploring the intersection of technology, culture, and human expression. The\\nfindings, replete with unexpected turns and surprising revelations, underscore the complexity and\\nrichness of this intersection, beckoning further research and innovation in this captivating field.\\n5 Results\\nOur investigation into the intersection of Augmented Reality (AR) and synchronized flamenco\\ndancing, with a focus on evaluating group cohesion through LSTM-based gesture forecasting,yielded\\na plethora of intriguing results. Initially, we observed that the integration of AR elements into\\nthe flamenco performances enhanced the dancers’ ability to synchronize their movements, thereby'),\n",
              " Document(id='df2085a2-d11d-4269-a33d-fe882406174e', metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 0}, page_content='Analyzing Real-Time Group Coordination in\\nAugmented Dance Performances: An LSTM-Based\\nGesture Modeling Approach\\nAbstract\\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\\nresearch avenue to explore group cohesion through gesture forecasting. By employ-\\ning LSTM neural networks, this study predicts dancers’ gestures and correlates\\naccuracy with synchronization, emotional expression, and creativity—key cohesion\\nmetrics.\\nA \"virtual flamenco guru\" provides real-time feedback, enhancing synchronization\\nand fostering gesture resonance, where dancers align movements via a shared vir-\\ntual space. AR amplifies this effect, especially with gesture-sensing garments. This\\ninterdisciplinary research highlights flamenco’s cultural depth, therapeutic bene-\\nfits, and technological applications in dance therapy, human-computer interaction,\\nand entertainment, pushing the boundaries of creativity and collective behavior\\nanalysis.\\n1 Introduction'),\n",
              " Document(id='97156a3d-be83-40d6-a884-9fce00540576', metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf', 'page': 5}, page_content='where it will take us in the future. We are also considering exploring other genres of dance, such as\\nballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are\\nplanning to investigate the use of our system in other domains, such as sports or rehabilitation, where\\ncoordinated movement and gesture forecasting could be beneficial. Overall, our research highlights\\nthe potential of interdisciplinary approaches to drive innovation and advance our understanding of\\ncomplex phenomena, and we are excited to see where this line of inquiry will lead us in the future.\\n4 Experiments\\nTo conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and\\nsynchronized flamenco, we designed a series of experiments that would not only assess the impact of\\nAR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\\nMemory (LSTM) networks. The experiments were carried out over the course of several months,')]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTMUjU2xFZ7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorestore1 = FAISS.from_documents(split_docs1, embeddings)\n",
        "vectorestore2 = FAISS.from_documents(split_docs2, embeddings)"
      ],
      "metadata": {
        "id": "AQ06wHyOymTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retreiver1 = vectorestore1.as_retriever()\n",
        "retreiver2 = vectorestore2.as_retriever()"
      ],
      "metadata": {
        "id": "3E4MLaDy77zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "id": "DnIk5o5k8Bhc",
        "outputId": "1eb0e800-f115-4467-9cdb-8f415889ded5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.29)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (3.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (2.2.3)\n",
            "Downloading langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.13.1 langchain_groq-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "api_key = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "D0PTs4IfB4bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(groq_api_key=api_key, model=\"Llama3-8b-8192\", streaming=True)"
      ],
      "metadata": {
        "id": "k9C1mjuNCZNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "id": "nnKNkVhPPBH2",
        "outputId": "a1d69375-a3bd-42e2-bc83-62d557f43875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Got unknown type w",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-a136f6cf68d6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"who are you\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     ) -> BaseMessage:\n\u001b[0;32m-> 1017\u001b[0;31m         generation = self.generate(\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         ).generations[0][0]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             )\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         params = {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_from_stream\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mgeneration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mgeneration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     ) -> Iterator[ChatGenerationChunk]:\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mmessage_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_convert_message_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mmessage_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_convert_message_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_convert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         }\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got unknown type {message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"name\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0mmessage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Got unknown type w"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "hy2lGJG0MNDB",
        "outputId": "32a35255-f99c-4621-d8ec-68aaab8389f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/232.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n"
      ],
      "metadata": {
        "id": "C_FL-9TBMVlm",
        "outputId": "e35619ad-14fa-469d-ef2e-7b139f782e4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement load_pdf (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for load_pdf\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as pdf_file:\n",
        "        reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "M2tL6ubxOFGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Function to load and extract text from PDF\n",
        "def load_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as pdf_file:\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text\n",
        "        if not text.strip():\n",
        "            raise ValueError(\"No text found in PDF. Check if it's scanned or encrypted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "# Query retrievers for Publishable and Not Publishable examples\n",
        "try:\n",
        "    publishable_context = vectorestore1.similarity_search(\"Methodology, coherence, and validity examples\", k=1)\n",
        "    not_publishable_context = vectorestore2.similarity_search(\"Methodology, coherence, and validity examples\", k=1)\n",
        "\n",
        "    # Fallback if no results\n",
        "    paper1 = publishable_context[0].page_content if publishable_context else \"Default Publishable Example\"\n",
        "    paper2 = not_publishable_context[0].page_content if not_publishable_context else \"Default Not Publishable Example\"\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving context: {e}\")\n",
        "    paper1 = \"Default Publishable Example\"\n",
        "    paper2 = \"Default Not Publishable Example\"\n",
        "\n",
        "# Updated Prompt\n",
        "prompt = f\"\"\"\n",
        "You will be given a research paper, and your task is to classify it as \"Publishable\" or \"Not Publishable\".\n",
        "For reference, I have provided two examples:\n",
        "\n",
        "1. Publishable Paper:\n",
        "{paper1}\n",
        "\n",
        "2. Not Publishable Paper:\n",
        "{paper2}\n",
        "\n",
        "Criteria for classification:\n",
        "- Methodology clarity: Is the methodology well-defined and replicable?\n",
        "- Coherence: Does the paper follow a logical structure?\n",
        "- Validity: Are the results justified with evidence?\n",
        "\n",
        "Now, analyze the following research paper and provide your classification along with a brief justification.\n",
        "\"\"\"\n",
        "\n",
        "# RAG Chain\n",
        "def rag_chain(input_paper):\n",
        "    \"\"\"\n",
        "    Process the input paper through the RAG chain and classify it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        context = f\"{prompt}\\n\\nResearch Paper:\\n{input_paper}\"\n",
        "        response = llm(context)  # Ensure `llm` is correctly defined\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error in RAG chain: {e}\")\n",
        "        return \"Error processing the paper.\"\n",
        "\n",
        "# Example Input\n",
        "file_path = \"/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf\"\n",
        "input_paper = load_pdf(file_path)\n",
        "\n",
        "if input_paper.strip():\n",
        "    response = rag_chain(input_paper)\n",
        "    print(\"LLM Response:\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"No content extracted from the input paper.\")\n"
      ],
      "metadata": {
        "id": "3vE3eCo0CtjE",
        "outputId": "17128595-1611-45db-c22f-c9a1b133e572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in RAG chain: Got unknown type \n",
            "\n",
            "LLM Response:\n",
            "Error processing the paper.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_list1.page_content"
      ],
      "metadata": {
        "id": "ZUr0tWf0LCwH",
        "outputId": "1efa2e54-9b9a-44d3-b347-e93e6f136ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'page_content'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-21872f09873d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocument_list1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'page_content'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6ob1fVPLJt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}