{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1e2dbc7097e48e3b29ce4cf283575f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae2eea5018624cd387b795984f8c67f6",
              "IPY_MODEL_2e47d86c7981472fabb943f6ca37c365",
              "IPY_MODEL_fb71f3c4cf514fb898b8a3eebd6e1fdf"
            ],
            "layout": "IPY_MODEL_2ba9e4f813fd4842bc240d3842a9976d"
          }
        },
        "ae2eea5018624cd387b795984f8c67f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd4c64153d054299a502c00f3974872a",
            "placeholder": "​",
            "style": "IPY_MODEL_fdb439b8e6464d1da7d0cb3ed32dff1c",
            "value": "modules.json: 100%"
          }
        },
        "2e47d86c7981472fabb943f6ca37c365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cebb234a65194569a3b8473058957901",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_148dea2d807c43038b255e414eda18fa",
            "value": 349
          }
        },
        "fb71f3c4cf514fb898b8a3eebd6e1fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3856ea7c7c4a84b38f1fb03e3057c2",
            "placeholder": "​",
            "style": "IPY_MODEL_7c3bf3fc14214759ab54ce1664ea6491",
            "value": " 349/349 [00:00&lt;00:00, 19.9kB/s]"
          }
        },
        "2ba9e4f813fd4842bc240d3842a9976d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4c64153d054299a502c00f3974872a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb439b8e6464d1da7d0cb3ed32dff1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cebb234a65194569a3b8473058957901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "148dea2d807c43038b255e414eda18fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c3856ea7c7c4a84b38f1fb03e3057c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3bf3fc14214759ab54ce1664ea6491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e168f3b81f234a6fac29826f15ac5daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_161d7f2f88024cbfb26808ca49917836",
              "IPY_MODEL_866a5806968b442a8208ba7eb627665d",
              "IPY_MODEL_95bba2fb994943afb2d64f85cb811653"
            ],
            "layout": "IPY_MODEL_729530f3ff424afca61e8c5c3903c5d8"
          }
        },
        "161d7f2f88024cbfb26808ca49917836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eff1d2d2a964c97ba9d87ba260017b1",
            "placeholder": "​",
            "style": "IPY_MODEL_140fc7ebea5d465dafaf3039ebab2b1f",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "866a5806968b442a8208ba7eb627665d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89c2ef67c4204bf1b0cab9804f36a5c3",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3735f928b1014db0b2a64679b316d2cb",
            "value": 116
          }
        },
        "95bba2fb994943afb2d64f85cb811653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aeedde62af846858225587c11e8cff4",
            "placeholder": "​",
            "style": "IPY_MODEL_3b68b38f10b141939b9efe7010a5334e",
            "value": " 116/116 [00:00&lt;00:00, 7.31kB/s]"
          }
        },
        "729530f3ff424afca61e8c5c3903c5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eff1d2d2a964c97ba9d87ba260017b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140fc7ebea5d465dafaf3039ebab2b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89c2ef67c4204bf1b0cab9804f36a5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3735f928b1014db0b2a64679b316d2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5aeedde62af846858225587c11e8cff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b68b38f10b141939b9efe7010a5334e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c043deaa42744b5dbdd7ce40bd0b96e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8782202762304a478451959e2a3665d8",
              "IPY_MODEL_635b5c2714ed4f0eaf2991629cfe2118",
              "IPY_MODEL_718d6cb50ec345acb3ffe314eda20374"
            ],
            "layout": "IPY_MODEL_2a58719da0f2499ea6a98a0b74e52815"
          }
        },
        "8782202762304a478451959e2a3665d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e14e2f34395491288dc8ec588e45330",
            "placeholder": "​",
            "style": "IPY_MODEL_2921bde2dcce40b59e1965443b7aa117",
            "value": "README.md: 100%"
          }
        },
        "635b5c2714ed4f0eaf2991629cfe2118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed798867644499e90c6a59394ff99c5",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bf7b30a67684b3a83d3a7762a445e8e",
            "value": 10659
          }
        },
        "718d6cb50ec345acb3ffe314eda20374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_706b1b1659ff467b8cd7609762b1d883",
            "placeholder": "​",
            "style": "IPY_MODEL_0ccadeda1a6b4d71bbf70b7d1b7f1251",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 579kB/s]"
          }
        },
        "2a58719da0f2499ea6a98a0b74e52815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e14e2f34395491288dc8ec588e45330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2921bde2dcce40b59e1965443b7aa117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed798867644499e90c6a59394ff99c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf7b30a67684b3a83d3a7762a445e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "706b1b1659ff467b8cd7609762b1d883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ccadeda1a6b4d71bbf70b7d1b7f1251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c02f19f5daad4ba3a2c815c2b52706c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e86a0f28bc8456f80ccd8e392681c84",
              "IPY_MODEL_11410dcb748347e59bbe35ce94236762",
              "IPY_MODEL_fa92650d90c4467b8b83d4071964e8f6"
            ],
            "layout": "IPY_MODEL_f6f6f23bd99d44189dd30f5a83ebf826"
          }
        },
        "5e86a0f28bc8456f80ccd8e392681c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96c30d301684ee0bef8dd02d6f90686",
            "placeholder": "​",
            "style": "IPY_MODEL_e3c4cdbf2ba347cc89544faddcc274f1",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "11410dcb748347e59bbe35ce94236762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_671cf7cdda2e4c67b3039a19994794d8",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea28ae37ddbd4de0a4e329a516be237f",
            "value": 53
          }
        },
        "fa92650d90c4467b8b83d4071964e8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a23ec6188993433784ee1454afb29d83",
            "placeholder": "​",
            "style": "IPY_MODEL_f3a46da21a594fe2b907011b71990b28",
            "value": " 53.0/53.0 [00:00&lt;00:00, 1.14kB/s]"
          }
        },
        "f6f6f23bd99d44189dd30f5a83ebf826": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a96c30d301684ee0bef8dd02d6f90686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3c4cdbf2ba347cc89544faddcc274f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "671cf7cdda2e4c67b3039a19994794d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea28ae37ddbd4de0a4e329a516be237f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a23ec6188993433784ee1454afb29d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a46da21a594fe2b907011b71990b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab6aa16c05fe44c08af8fea8622d5415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38e45eb8125847b38e670c1897e193fc",
              "IPY_MODEL_70b60239c0a8405da1fef8e3dbbc2942",
              "IPY_MODEL_eedb7d83347f4a1c892bd492fb7ebbc8"
            ],
            "layout": "IPY_MODEL_dcef677ca95d4a3f80c0cb736d5e5e11"
          }
        },
        "38e45eb8125847b38e670c1897e193fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc8c8cd12870410590124466054b493d",
            "placeholder": "​",
            "style": "IPY_MODEL_72b8aaf708864588ae13386f6f25451e",
            "value": "config.json: 100%"
          }
        },
        "70b60239c0a8405da1fef8e3dbbc2942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a0cbec074dc4d54ac6e6001760766ff",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_356566524a1d47f79a0ee8dc5e3c0a8c",
            "value": 612
          }
        },
        "eedb7d83347f4a1c892bd492fb7ebbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67678895d1364d9a93fe5467b1264701",
            "placeholder": "​",
            "style": "IPY_MODEL_6c2e1b44d3c64074a417dc5c840f65ce",
            "value": " 612/612 [00:00&lt;00:00, 24.7kB/s]"
          }
        },
        "dcef677ca95d4a3f80c0cb736d5e5e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc8c8cd12870410590124466054b493d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b8aaf708864588ae13386f6f25451e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a0cbec074dc4d54ac6e6001760766ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "356566524a1d47f79a0ee8dc5e3c0a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67678895d1364d9a93fe5467b1264701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2e1b44d3c64074a417dc5c840f65ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bc1d5b62e7a4dcb9c58064a6f7b7a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0589e52ee0c45138ab5a9b85b47c27d",
              "IPY_MODEL_4302ba166bb14f62bc3f12fcd126866f",
              "IPY_MODEL_89dba6251bcd4cdf9c8d54b0db04c1aa"
            ],
            "layout": "IPY_MODEL_4fca89a599864bd9be4aa81d8babf220"
          }
        },
        "f0589e52ee0c45138ab5a9b85b47c27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3bf25f15d8d437aae6ae8c81a7c95f6",
            "placeholder": "​",
            "style": "IPY_MODEL_0da8e46e4221459a8d9cbb865d856b73",
            "value": "model.safetensors: 100%"
          }
        },
        "4302ba166bb14f62bc3f12fcd126866f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeeee69c075e4a1ca8adcc995b2b2a17",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faa5cf4b79024095beab5db4af58c605",
            "value": 90868376
          }
        },
        "89dba6251bcd4cdf9c8d54b0db04c1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7150a8e4edf54bc2b7c1df7b3f625d8e",
            "placeholder": "​",
            "style": "IPY_MODEL_900941e17a1c4c27a93c755ccf7ccad6",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 200MB/s]"
          }
        },
        "4fca89a599864bd9be4aa81d8babf220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3bf25f15d8d437aae6ae8c81a7c95f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0da8e46e4221459a8d9cbb865d856b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeeee69c075e4a1ca8adcc995b2b2a17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faa5cf4b79024095beab5db4af58c605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7150a8e4edf54bc2b7c1df7b3f625d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "900941e17a1c4c27a93c755ccf7ccad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af009acf067f43248fb071614710116f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c26ee641f02434c91a8566ba4770648",
              "IPY_MODEL_5d322c0a368f4a788d10ccaa7b711e27",
              "IPY_MODEL_bae8b0d47ddf418290a05e02bb4d702f"
            ],
            "layout": "IPY_MODEL_9efdd4755934439bb64b46d2ec78e735"
          }
        },
        "8c26ee641f02434c91a8566ba4770648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0789b3354814e40bcb7e8c85c038d99",
            "placeholder": "​",
            "style": "IPY_MODEL_ecfc1873d2c34c53bb09258c96c444be",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "5d322c0a368f4a788d10ccaa7b711e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c39a1e141104b13b6f761eadb52ee6b",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b546a3025c0e44159d19fc7973d875d4",
            "value": 350
          }
        },
        "bae8b0d47ddf418290a05e02bb4d702f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad831f4978a84457b7cfbe309cbb33aa",
            "placeholder": "​",
            "style": "IPY_MODEL_86e7958fda384eb99d9c4888ccfc885a",
            "value": " 350/350 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "9efdd4755934439bb64b46d2ec78e735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0789b3354814e40bcb7e8c85c038d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecfc1873d2c34c53bb09258c96c444be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c39a1e141104b13b6f761eadb52ee6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b546a3025c0e44159d19fc7973d875d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad831f4978a84457b7cfbe309cbb33aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e7958fda384eb99d9c4888ccfc885a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0330413031d47ff80155fb02a4ed907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ec4b93f57d6410ea47bd3e02e4be329",
              "IPY_MODEL_7a5d713d35c44a6882f1701da3bc867d",
              "IPY_MODEL_5d1f0dcf9ea74a1b8119cd5fc57b84d5"
            ],
            "layout": "IPY_MODEL_8b89a0039237477ba5fa75a993b471de"
          }
        },
        "8ec4b93f57d6410ea47bd3e02e4be329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a00f4d6689f347e6a42139f88efeca9a",
            "placeholder": "​",
            "style": "IPY_MODEL_6e023457518d4229a8a24f5c7a2ef312",
            "value": "vocab.txt: 100%"
          }
        },
        "7a5d713d35c44a6882f1701da3bc867d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc1451f8ad44b5baae618c844a124bc",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da6f1cc346d54c98adfe3be452558529",
            "value": 231508
          }
        },
        "5d1f0dcf9ea74a1b8119cd5fc57b84d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ee60f53fd34bf29ce2fd165db5e62e",
            "placeholder": "​",
            "style": "IPY_MODEL_1117d011ede14425beb35f87f26d4fec",
            "value": " 232k/232k [00:00&lt;00:00, 680kB/s]"
          }
        },
        "8b89a0039237477ba5fa75a993b471de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a00f4d6689f347e6a42139f88efeca9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e023457518d4229a8a24f5c7a2ef312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cc1451f8ad44b5baae618c844a124bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da6f1cc346d54c98adfe3be452558529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00ee60f53fd34bf29ce2fd165db5e62e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1117d011ede14425beb35f87f26d4fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78298257b52e45f2ab68a6cd3439e295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_997518bd215941b0b9bbb91275992d68",
              "IPY_MODEL_ade07784d7a948f580efa5e569a84cb9",
              "IPY_MODEL_20199058dc04478d9c4f5ac3ae646b8e"
            ],
            "layout": "IPY_MODEL_767b723c9cce4b4bac276d39b9a1cd42"
          }
        },
        "997518bd215941b0b9bbb91275992d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c001ad29e19f4952b34a129ec3990182",
            "placeholder": "​",
            "style": "IPY_MODEL_cfdf8359bb29455baa660799ff696cba",
            "value": "tokenizer.json: 100%"
          }
        },
        "ade07784d7a948f580efa5e569a84cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2e4387fb7144391a2b389497e87f3d6",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dd88371fcb94db886817b9e80e7083b",
            "value": 466247
          }
        },
        "20199058dc04478d9c4f5ac3ae646b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff331d868c145e88fbeba9e049ef5d4",
            "placeholder": "​",
            "style": "IPY_MODEL_ba79bb95f62d41ea8912a27d30ea7aae",
            "value": " 466k/466k [00:00&lt;00:00, 1.33MB/s]"
          }
        },
        "767b723c9cce4b4bac276d39b9a1cd42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c001ad29e19f4952b34a129ec3990182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfdf8359bb29455baa660799ff696cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2e4387fb7144391a2b389497e87f3d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd88371fcb94db886817b9e80e7083b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aff331d868c145e88fbeba9e049ef5d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba79bb95f62d41ea8912a27d30ea7aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e968e750ef24b6f88186d8976b888a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af6811c7099342338c99e28aabc59920",
              "IPY_MODEL_4b8e5b041ae2480693e5948e4234d3f8",
              "IPY_MODEL_015b14b288694b21a32e7811524f6bb4"
            ],
            "layout": "IPY_MODEL_e1e7bd92f5a544958266732f9a9a823f"
          }
        },
        "af6811c7099342338c99e28aabc59920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f03ea11480453a975600a624232928",
            "placeholder": "​",
            "style": "IPY_MODEL_b75aa641e792478f9066dab1a61c711a",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "4b8e5b041ae2480693e5948e4234d3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29bb68ce8e984188b9451721a11a4b28",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d60d2b0d674f4a9287a05a4976f120e8",
            "value": 112
          }
        },
        "015b14b288694b21a32e7811524f6bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4428f31e9e5c4ab3a58bc55d9b32b268",
            "placeholder": "​",
            "style": "IPY_MODEL_9f2b80e9598b40aa95d29b08832d96b9",
            "value": " 112/112 [00:00&lt;00:00, 5.66kB/s]"
          }
        },
        "e1e7bd92f5a544958266732f9a9a823f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f03ea11480453a975600a624232928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b75aa641e792478f9066dab1a61c711a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29bb68ce8e984188b9451721a11a4b28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60d2b0d674f4a9287a05a4976f120e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4428f31e9e5c4ab3a58bc55d9b32b268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f2b80e9598b40aa95d29b08832d96b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7020d01221e344e9bc0c610a1a95c552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5338a7465bb84eaf97bc929a868d5d19",
              "IPY_MODEL_3e81c0615cc947cc8ff398c33bd2511d",
              "IPY_MODEL_6886a2cb6c164ef4996413762ad58f35"
            ],
            "layout": "IPY_MODEL_860ee6d0fb9d423a9362eacfb339cabf"
          }
        },
        "5338a7465bb84eaf97bc929a868d5d19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52cdffef1c5747ada6b013d908cd2451",
            "placeholder": "​",
            "style": "IPY_MODEL_6219bc17c0394bf3a81d5afc72fbad86",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "3e81c0615cc947cc8ff398c33bd2511d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80874ff8fb3f4446bd58fccde205e41a",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8befcb1d14614d05bc9a1d753df91c7c",
            "value": 190
          }
        },
        "6886a2cb6c164ef4996413762ad58f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cafe4de3eda4a23bd760113dc3fbbae",
            "placeholder": "​",
            "style": "IPY_MODEL_bbe9f17571ec4409bca55422bb1db7ab",
            "value": " 190/190 [00:00&lt;00:00, 6.60kB/s]"
          }
        },
        "860ee6d0fb9d423a9362eacfb339cabf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52cdffef1c5747ada6b013d908cd2451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6219bc17c0394bf3a81d5afc72fbad86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80874ff8fb3f4446bd58fccde205e41a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8befcb1d14614d05bc9a1d753df91c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cafe4de3eda4a23bd760113dc3fbbae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe9f17571ec4409bca55422bb1db7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshal292004/KGPDSH/blob/master/Copy_of_CreateDataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FFVM61IrX4qT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b00edb6-b030-4df3-d4de-d47f79e4fc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.60-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.25 (from langchain)\n",
            "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.48-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.2.2)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.60-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
            "Downloading langgraph_sdk-0.1.48-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, tiktoken, pydantic-settings, langgraph-sdk, dataclasses-json, langchain-core, langgraph-checkpoint, langgraph, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.9.0.post1 httpx-sse-0.4.0 langchain-0.3.14 langchain-core-0.3.29 langchain_community-0.3.14 langgraph-0.2.60 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.48 marshmallow-3.23.3 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community tiktoken langgraph faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "toLP9w14Of6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac37c2c-ccf9-4578-c6bd-8a4588dede6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "ZwaIMqa1UvQy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "print(f\"Current Working Directory: {current_directory}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYMMF7jdQW_Z",
        "outputId": "259f7469-52cb-4d96-a88b-e7cf3828c31b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5p8_IYJQflz",
        "outputId": "da64fe1a-dacf-4aae-a604-b895d2810182"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'drive', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uFMZF4a4RGGY",
        "outputId": "3ff3e86b-fe3a-4613-8c71-9dfc1567d6f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/KDSH_2025_Dataset/Reference')"
      ],
      "metadata": {
        "id": "OR_sCFWoQ3B9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recursively_get_pdf_files(directory):\n",
        "    pdf_files = []\n",
        "    try:\n",
        "        if os.path.isfile(directory) and directory.endswith('.pdf'):\n",
        "            pdf_files.append(directory)\n",
        "        elif os.path.isdir(directory):\n",
        "            for file in os.listdir(directory):\n",
        "                full_path = os.path.join(directory, file)\n",
        "                pdf_files.extend(recursively_get_pdf_files(full_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing {directory}: {e}\")\n",
        "    return pdf_files\n"
      ],
      "metadata": {
        "id": "z5duPna2TJim"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_files1=recursively_get_pdf_files('/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable')\n",
        "list_of_files2=recursively_get_pdf_files('/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable')"
      ],
      "metadata": {
        "id": "M-Mf-4NHT6Ei"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_files1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYDJQj9iUAsZ",
        "outputId": "61683016-4ca0-4c08-afd9-696bf37efd5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_files2"
      ],
      "metadata": {
        "id": "RsQkDLvXEwwM",
        "outputId": "d8af84a9-4a08-4cba-b882-e66e42c15a09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R005.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R002.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R003.pdf',\n",
              " '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R004.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_docs(list_of_pdf_paths):\n",
        "  document_list=[]\n",
        "  for pdf_path in list_of_pdf_paths:\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    document_list.append(loader.load())\n",
        "  return document_list"
      ],
      "metadata": {
        "id": "o2qXS2kFUHZ3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPeAjjYqEv7M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_list1= load_docs(list_of_files1)\n",
        "document_list2 = load_docs(list_of_files2)"
      ],
      "metadata": {
        "id": "mHwzhlAxUcfD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrDAOudYUj5m",
        "outputId": "a1f4a0d3-1d5f-49c4-e287-f7c679d30dc1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in document_list1:\n",
        "  print(\"-\"*20)\n",
        "  print(doc)\n",
        "  print(\"-\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS-Tv7JdVAIe",
        "outputId": "cc63301e-f897-45c1-c07e-b2ffc23b5c0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 0}, page_content='Detailed Action Identification in Baseball Game\\nRecordings\\nAbstract\\nThis research introduces MLB-YouTube, a new and complex dataset created for\\nnuanced activity recognition in baseball videos. This dataset is structured to\\nsupport two types of analysis: one for classifying activities in segmented videos\\nand another for detecting activities in unsegmented, continuous video streams. This\\nstudy evaluates several methods for recognizing activities, focusing on how they\\ncapture the temporal organization of activities in videos. This evaluation starts\\nwith categorizing segmented videos and progresses to applying these methods\\nto continuous video feeds. Additionally, this paper assesses the effectiveness of\\ndifferent models in the challenging task of forecasting pitch velocity and type\\nusing baseball broadcast videos. The findings indicate that incorporating temporal\\ndynamics into models is beneficial for detailed activity recognition.\\n1 Introduction\\nAction recognition, a significant problem in computer vision, finds extensive use in sports. Profes-\\nsional sporting events are extensively recorded for entertainment, and these recordings are invaluable\\nfor subsequent analysis by coaches, scouts, and media analysts. While numerous game statistics\\nare currently gathered manually, the potential exists for these to be replaced by computer vision\\nsystems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)\\nto automatically record pitch speed and movement, utilizing a network of high-speed cameras and\\nradar to collect detailed data on each player. Access to much of this data is restricted from the public\\ndomain.\\nThis paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-\\nties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or\\ndetection, our dataset emphasizes fine-grained activity recognition. The differences between activities\\nare often minimal, primarily involving the movement of a single individual, with a consistent scene\\nstructure across activities. The determination of activity is based on a single camera perspective. This\\nstudy compares various methods for temporal feature aggregation, both for classifying activities in\\nsegmented videos and for detecting them in continuous video streams.\\n2 Related Work\\nThe field of activity recognition has garnered substantial attention in computer vision research. Initial\\nsuccesses were achieved with hand-engineered features such as dense trajectories. The focus of more\\nrecent studies has shifted towards the application of Convolutional Neural Networks (CNNs) for\\nactivity recognition. Two-stream CNN architectures utilize both spatial RGB frames and optical\\nflow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have been\\ndeveloped. The development of these advanced CNN models has been supported by large datasets\\nsuch as Kinetics, THUMOS, and ActivityNet.\\nSeveral studies have investigated the aggregation of temporal features for the purpose of activity\\nrecognition. Research has compared several pooling techniques and determined that both Long Short-\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 1}, page_content='Term Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. It\\nhas been discovered that pooling intervals from varying locations and durations is advantageous for\\nactivity recognition. It was demonstrated that identifying and classifying key sub-event intervals can\\nlead to better performance.\\nRecently, segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently\\nfor activity detection. These methods depend on the 3D CNN to capture temporal dynamics, which\\ntypically span only 16 frames. Although longer-term temporal structures have been explored, this was\\nusually accomplished with temporal pooling of localized features or (spatio-)temporal convolutions\\nwith extended fixed intervals. Recurrent Neural Networks (RNNs) have also been applied to represent\\ntransitions in activity between frames.\\n3 MLB-YouTube Dataset\\nWe have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\\non YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos\\nintended for activity recognition and continuous videos designed for activity classification. The\\ndataset’s complexity is amplified by the fact that it originates from televised baseball games, where a\\nsingle camera perspective is shared among various activities. Additionally, there is minimal variance\\nin motion and appearance among different activities, such as swinging a bat versus bunting. In\\ncontrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities\\nwith diverse settings, scales, and camera angles, our dataset features activities where a single frame\\nmight not be adequate to determine the activity.\\nThe minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between\\nthese actions requires identifying whether the batter swings or not, detecting the umpire’s signal\\n(Figure 4) for a strike, or noting the absence of a signal for a ball. This is further complicated because\\nthe batter or catcher can obstruct the umpire, and each umpire has their unique style of signaling a\\nstrike.\\nOur dataset for segmented video analysis comprises 4,290 clips. Each clip is annotated for multiple\\nbaseball actions, including swing, hit, ball, strike, and foul. Given that a single clip may contain\\nseveral activities, this is considered a multi-label classification task. Table 1 presents the complete\\nlist of activities and their respective counts within the dataset. Additionally, clips featuring a pitch\\nwere annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. Furthermore, a\\ncollection of 2,983 hard negative examples, where no action is present, was gathered. These instances\\ninclude views of the crowd, the field, or players standing idly before or after a pitch. Examples of\\nactivities and hard negatives are depicted in Figure 2.\\nOur continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Every\\nframe in these videos is annotated with the baseball activities that occur. On average, each continuous\\nclip contains 7.2 activities, amounting to over 15,000 activity instances in total.\\nTable 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.\\nActivity Count\\nNo Activity 2983\\nBall 1434\\nStrike 1799\\nSwing 2506\\nHit 1391\\nFoul 718\\nIn Play 679\\nBunt 24\\nHit by Pitch 14\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 2}, page_content='4 Segmented Video Recognition Approach\\nWe investigate different techniques for aggregating temporal features in segmented video activity\\nrecognition. In segmented videos, the classification task is simpler because each frame corresponds to\\nan activity, eliminating the need for the model to identify the start and end of activities. Our methods\\nare based on a CNN that generates a per-frame or per-segment representation, derived from standard\\ntwo-stream CNNs using deep CNNs like I3D or InceptionV3.\\nGiven video features v of dimensions T × D, where T represents the video’s temporal length and D\\nis the feature’s dimensionality, the usual approach for feature pooling involves max- or mean-pooling\\nacross the temporal dimension, followed by a fully-connected layer for video clip classification, as\\ndepicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,\\nlosing temporal information. An alternative is to employ a fixed temporal pyramid with various\\nlengths, as shown in Fig 5(b), dividing the video into intervals of lengths 1/2, 1/4, and 1/8, and\\nmax-pooling each. The pooled features are concatenated, creating a K × D representation, where K\\nis the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip.\\nWe also explore learning temporal convolution filters to aggregate local temporal structures. A kernel\\nof size L×1 is applied to each frame, enabling each timestep representation to incorporate information\\nfrom adjacent frames. After applying max-pooling to the output of the temporal convolution, a fully-\\nconnected layer is used for classification, as illustrated in Fig. 5(c).\\nWhile temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.\\nPrevious studies have shown that learning the sub-interval to pool is beneficial for activity recognition.\\nThese learned intervals are defined by three parameters: a center g, a width σ, and a stride δ,\\nparameterizing N Gaussians. Given the video length T, the positions of the strided Gaussians are\\nfirst calculated as:\\ngn = 0.5 − T − (gn + 1)\\nN − 1 forn = 0, 1, . . . , N− 1\\npt,n = gn + (t − 0.5T + 0.5)1\\nδ fort = 0, 1, . . . , T− 1\\nThe filters are then generated as:\\nFm[i, t] = 1\\nZm\\nexp\\n\\x12\\n−(t − µi,m)2\\n2σ2m\\n\\x13\\ni ∈ {0, 1, . . . , N− 1}, t∈ {0, 1, . . . , T− 1}\\nwhere Zm is a normalization constant.\\nWe apply these filters F to the T × D video representation through matrix multiplication, yielding an\\nN × D representation that serves as input to a fully-connected layer for classification. This method\\nis shown in Fig 5(d).\\nAdditionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state\\nas input to a fully-connected layer for classification. We frame our tasks as multi-label classification\\nand train these models to minimize binary cross-entropy:\\nL(v) =\\nX\\nc\\nzc log(p(c|G(v))) + (1− zc) log(1− p(c|G(v)))\\nwhere G(v) is the function that pools the temporal information, and zc is the ground truth label for\\nclass c.\\n5 Activity Detection in Continuous Videos\\nDetecting activities in continuous videos poses a greater challenge. The goal here is to classify each\\nframe according to the activities occurring. Unlike segmented videos, continuous videos feature\\nmultiple sequential activities, often interspersed with frames of inactivity. This necessitates that\\nthe model learn to identify the start and end points of activities. As a baseline, we train a single\\nfully-connected layer to serve as a per-frame classifier, which does not utilize temporal information\\nbeyond that contained in the features.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 3}, page_content='We adapt the methods developed for segmented video classification to continuous videos by imple-\\nmenting a temporal sliding window approach. We select a fixed window duration of L features, apply\\nmax-pooling to each window (similar to Fig. 5(a)), and classify each pooled segment. This approach\\nis extended to temporal pyramid pooling by dividing the window of lengthL into segments of lengths\\nL/2, L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,\\nand the pooled features are concatenated, yielding a 14 × D-dimensional representation for each\\nwindow, which is then used as input to the classifier.\\nFor temporal convolutional models in continuous videos, we modify the segmented video approach by\\nlearning a temporal convolutional kernel of length L and convolving it with the input video features.\\nThis operation transforms input of size T × D into output of size T × D, followed by a per-frame\\nclassifier. This enables the model to aggregate local temporal information.\\nTo extend the sub-event model to continuous videos, we follow a similar approach but setT = L in\\nEq. 1, resulting in filters of length L. The T ×D video representation is convolved with the sub-event\\nfilters F, producing an N × D × T-dimensional representation used as input to a fully-connected\\nlayer for frame classification.\\nThe model is trained to minimize per-frame binary classification:\\nL(v) =\\nX\\nt,c\\nzt,c log(p(c|H(vt))) + (1− zt,c) log(1− p(c|H(vt)))\\nwhere vt is the per-frame or per-segment feature at time t, H(vt) is the sliding window application of\\none of the feature pooling methods, and zt,c is the ground truth class at time t.\\nA method to learn ’super-events’ (i.e., global video context) has been introduced and shown to be\\neffective for activity detection in continuous videos. This approach involves learning a set of temporal\\nstructure filters modeled as N Cauchy distributions. Each distribution is defined by a center xn and a\\nwidth γn. Given the video length T, the filters are constructed by:\\nxn = (T − 1)(tanh(x′\\nn) + 1)\\n2\\nfn(t) = 1\\nZn\\nγn\\nπ((t − xn)2 + γ2n) exp(1 − 2|tanh(γ′\\nn)|)\\nwhere Zn is a normalization constant, t ∈ {1, 2, . . . , T}, and n ∈ {1, 2, . . . , N}.\\nThe filters are combined with learned per-class soft-attention weights A, and the super-event repre-\\nsentation is computed as:\\nSc =\\nX\\nn\\nAc,n\\nX\\nt\\nfn(t) · vt\\nwhere v is the T × D video representation. These filters enable the model to focus on relevant\\nintervals for temporal context. The super-event representation is concatenated to each timestep and\\nused for classification. We also experiment with combining the super- and sub-event representations\\nto form a three-level hierarchy for event representation.\\n6 Experiments\\n6.1 Implementation Details\\nFor our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics\\ndatasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable\\nfeature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet\\nand Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth\\ncompared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow\\nwas computed and clipped to [−20, 20]. For InceptionV3, features were computed every 3 frames\\n(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\\n3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam\\noptimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50\\nepochs.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 4}, page_content='6.2 Segmented Video Activity Recognition\\nWe initially conducted binary pitch/non-pitch classification for each video segment. This task is\\nrelatively straightforward due to the distinct differences between pitch and non-pitch frames. The\\nresults, detailed in Table 2, reveal minimal variation across different features or models.\\nTable 2: Performance on segmented videos for binary pitch/non-pitch classification.\\nModel RGB Flow Two-stream\\nInceptionV3 97.46 98.44 98.67\\nInceptionV3 + sub-events 98.67 98.73 99.36\\nI3D 98.64 98.88 98.70\\nI3D + sub-events 98.42 98.35 98.65\\n6.2.1 Multi-label Classification\\nWe assessed various temporal feature aggregation methods by calculating the mean average precision\\n(mAP) for each video clip, a standard metric for multi-label classification. Table 4 compares the\\nperformance of these methods. All methods surpass mean/max-pooling, highlighting the importance\\nof preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs\\nshow some improvement. Temporal convolution offers a more significant performance boost but\\nrequires substantially more parameters (see Table 3). Learning sub-events, as per previous research,\\nyields the best results. While LSTMs and temporal convolutions have been used before, they need\\nmore parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitate\\nsequential processing of video features, whereas other methods can be fully parallelized.\\nTable 3: Additional parameters required for models when added to the base model (e.g., I3D or\\nInception V3).\\nModel # Parameters\\nMax/Mean Pooling 16K\\nPyramid Pooling 115K\\nLSTM 10.5M\\nTemporal Conv 31.5M\\nSub-events 36K\\nTable 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.\\nLearning sub-intervals for pooling is found to be crucial for activity recognition.\\nMethod RGB Flow Two-stream\\nRandom 16.3 16.3 16.3\\nInceptionV3 + mean-pool 35.6 47.2 45.3\\nInceptionV3 + max-pool 47.9 48.6 54.4\\nInceptionV3 + pyramid 49.7 53.2 55.3\\nInceptionV3 + LSTM 47.6 55.6 57.7\\nInceptionV3 + temporal conv 47.2 55.2 56.1\\nInceptionV3 + sub-events 56.2 62.5 62.6\\nI3D + mean-pool 42.4 47.6 52.7\\nI3D + max-pool 48.3 53.4 57.2\\nI3D + pyramid 53.2 56.7 58.7\\nI3D + LSTM 48.2 53.1 53.1\\nI3D + temporal conv 52.8 57.1 58.4\\nI3D + sub-events 55.5 61.2 61.3\\nTable 5 shows the average precision for each activity class. Learning temporal structure is particularly\\nbeneficial for frame-based features (e.g., InceptionV3), which capture less temporal information\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 5}, page_content='compared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detecting\\nstrikes, hits, foul balls, and hit-by-pitch events, which exhibit changes in video features post-event.\\nFor instance, after a hit, the camera often tracks the ball’s trajectory, while after a hit-by-pitch, it\\nfollows the player to first base, as illustrated in Fig. 6 and Fig. 7.\\nTable 5: Per-class average precision for segmented videos using two-stream features in multi-\\nlabel activity classification. Utilizing sub-events to discern temporal intervals of interest proves\\nadvantageous for activity recognition.\\nMethod Ball Strike Swing Hit Foul In Play Bunt Hit by Pitch\\nRandom 21.8 28.6 37.4 20.9 11.4 10.3 1.1 4.5\\nInceptionV3 + max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7\\nInceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2\\nI3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2\\nI3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0\\n6.2.2 Pitch Speed Regression\\nEstimating pitch speed from video frames is an exceptionally difficult problem, as it requires the\\nnetwork to pinpoint the pitch’s start and end, and derive the speed from a minimal signal. The baseball,\\noften obscured by the pitcher, travels at speeds over 100mph and covers 60.5 feet in approximately 0.5\\nseconds. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,\\nproving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos, we\\nrecalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connected\\nlayer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\\nand actual speeds, we achieved an average error of 3.6mph. Table 6 compares different models, and\\nFig. 8 illustrates the sub-events learned for various speeds.\\nTable 6: Results for pitch speed regression on segmented videos, reporting root-mean-squared errors.\\nMethod Two-stream\\nI3D 4.3 mph\\nI3D + LSTM 4.1 mph\\nI3D + sub-events 3.9 mph\\nInceptionV3 5.3 mph\\nInceptionV3 + LSTM 4.5 mph\\nInceptionV3 + sub-events 3.6 mph\\n6.2.3 Pitch Type Classification\\nWe conducted experiments to determine the feasibility of predicting pitch types from video, a task\\nmade challenging by pitchers’ efforts to disguise their pitches from batters and the subtle differences\\nbetween pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\\nutilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.\\nPose features were considered due to variations in body mechanics between different pitches. Our\\ndataset includes six pitch types, with results presented in Table 7. LSTMs performed worse than the\\nbaseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the\\neasiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult\\n(12%).\\n6.3 Continuous Video Activity Detection\\nWe evaluate models extended for continuous videos using per-frame mean average precision (mAP),\\nwith results shown in Table 8. This setting is more challenging than segmented videos, requiring\\nthe model to identify activity start and end times and handle ambiguous negative examples. All\\nmodels improve upon the baseline per-frame classification, confirming the importance of temporal\\ninformation. Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R006.pdf', 'page': 6}, page_content='Table 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose\\nheatmaps.\\nMethod Accuracy\\nRandom 17.0%\\nI3D 25.8%\\nI3D + LSTM 18.5%\\nI3D + sub-events 34.5%\\nPose 28.4%\\nPose + LSTM 27.6%\\nPose + sub-events 36.4%\\nconvolution appear to overfit. Convolutional sub-events, especially when combined with super-event\\nrepresentation, significantly enhance performance, particularly for frame-based features.\\nTable 8: Performance on continuous videos for multi-label activity classification (per-frame mAP).\\nMethod RGB Flow Two-stream\\nRandom 13.4 13.4 13.4\\nI3D 33.8 35.1 34.2\\nI3D + max-pooling 34.9 36.4 36.8\\nI3D + pyramid 36.8 37.5 39.7\\nI3D + LSTM 36.2 37.3 39.4\\nI3D + temporal conv 35.2 38.1 39.2\\nI3D + sub-events 35.5 37.5 38.5\\nI3D + super-events 38.7 38.6 39.1\\nI3D + sub+super-events 38.2 39.4 40.4\\nInceptionV3 31.2 31.8 31.9\\nInceptionV3 + max-pooling 31.8 34.1 35.2\\nInceptionV3 + pyramid 32.2 35.1 36.8\\nInceptionV3 + LSTM 32.1 33.5 34.1\\nInceptionV3 + temporal conv 28.4 34.4 33.4\\nInceptionV3 + sub-events 32.1 35.8 37.3\\nInceptionV3 + super-events 31.5 36.2 39.6\\nInceptionV3 + sub+super-events 34.2 40.2 40.9\\n7 Conclusion\\nThis paper introduces MLB-YouTube, a novel and challenging dataset designed for detailed activity\\nrecognition in videos. We conduct a comparative analysis of various recognition techniques that\\nemploy temporal feature pooling for both segmented and continuous videos. Our findings reveal that\\nlearning sub-events to pinpoint temporal regions of interest significantly enhances performance in\\nsegmented video classification. In the context of activity detection in continuous videos, we establish\\nthat incorporating convolutional sub-events with a super-event representation, creating a three-level\\nactivity hierarchy, yields the most favorable outcomes.\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 0}, page_content='Advancements in 3D Food Modeling: A Review of the\\nMetaFood Challenge Techniques and Outcomes\\nAbstract\\nThe growing focus on leveraging computer vision for dietary oversight and nutri-\\ntion tracking has spurred the creation of sophisticated 3D reconstruction methods\\nfor food. The lack of comprehensive, high-fidelity data, coupled with limited\\ncollaborative efforts between academic and industrial sectors, has significantly\\nhindered advancements in this domain. This study addresses these obstacles by\\nintroducing the MetaFood Challenge, aimed at generating precise, volumetrically\\naccurate 3D food models from 2D images, utilizing a checkerboard for size cal-\\nibration. The challenge was structured around 20 food items across three levels\\nof complexity: easy (200 images), medium (30 images), and hard (1 image). A\\ntotal of 16 teams participated in the final assessment phase. The methodologies\\ndeveloped during this challenge have yielded highly encouraging outcomes in\\n3D food reconstruction, showing great promise for refining portion estimation in\\ndietary evaluations and nutritional tracking. Further information on this workshop\\nchallenge and the dataset is accessible via the provided URL.\\n1 Introduction\\nThe convergence of computer vision technologies with culinary practices has pioneered innovative\\napproaches to dietary monitoring and nutritional assessment. The MetaFood Workshop Challenge\\nrepresents a landmark initiative in this emerging field, responding to the pressing demand for precise\\nand scalable techniques for estimating food portions and monitoring nutritional consumption. Such\\ntechnologies are vital for fostering healthier eating behaviors and addressing health issues linked to\\ndiet.\\nBy concentrating on the development of accurate 3D models of food derived from various visual\\ninputs, including multiple views and single perspectives, this challenge endeavors to bridge the\\ndisparity between current methodologies and practical needs. It promotes the creation of unique\\nsolutions capable of managing the intricacies of food morphology, texture, and illumination, while also\\nmeeting the real-world demands of dietary evaluation. This initiative gathers experts from computer\\nvision, machine learning, and nutrition science to propel 3D food reconstruction technologies forward.\\nThese advancements have the potential to substantially enhance the precision and utility of food\\nportion estimation across diverse applications, from individual health tracking to extensive nutritional\\ninvestigations.\\nConventional methods for assessing diet, like 24-Hour Recall or Food Frequency Questionnaires\\n(FFQs), are frequently reliant on manual data entry, which is prone to inaccuracies and can be\\nburdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression-\\nbased methods for estimating food portions directly from images of eating occasions. By enhancing\\n3D reconstruction for food, the aim is to provide more accurate and intuitive nutritional assessment\\ntools. This technology could revolutionize the sharing of culinary experiences and significantly\\nimpact nutrition science and public health.\\nParticipants were tasked with creating 3D models of 20 distinct food items from 2D images, mim-\\nicking scenarios where mobile devices equipped with depth-sensing cameras are used for dietary\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 1}, page_content='recording and nutritional tracking. The challenge was segmented into three tiers of difficulty based\\non the number of images provided: approximately 200 images for easy, 30 for medium, and a single\\ntop-view image for hard. This design aimed to rigorously test the adaptability and resilience of\\nproposed solutions under various realistic conditions. A notable feature of this challenge was the use\\nof a visible checkerboard for physical referencing and the provision of depth images for each frame,\\nensuring the 3D models maintained accurate real-world measurements for portion size estimation.\\nThis initiative not only expands the frontiers of 3D reconstruction technology but also sets the stage\\nfor more reliable and user-friendly real-world applications, including image-based dietary assessment.\\nThe resulting solutions hold the potential to profoundly influence nutritional intake monitoring and\\ncomprehension, supporting broader health and wellness objectives. As progress continues, innovative\\napplications are anticipated to transform personal health management, nutritional research, and the\\nwider food industry. The remainder of this report is structured as follows: Section 2 delves into the\\nexisting literature on food portion size estimation, Section 3 describes the dataset and evaluation\\nframework used in the challenge, and Sections 4, 5, and 6 discuss the methodologies and findings of\\nthe top three teams (V olETA, ININ-VIAUN, and FoodRiddle), respectively.\\n2 Related Work\\nEstimating food portions is a crucial part of image-based dietary assessment, aiming to determine the\\nvolume, energy content, or macronutrients directly from images of meals. Unlike the well-studied\\ntask of food recognition, estimating food portions is particularly challenging due to the lack of 3D\\ninformation and physical size references necessary for accurately judging the actual size of food\\nportions. Accurate portion size estimation requires understanding the volume and density of food,\\nelements that are hard to deduce from a 2D image, underscoring the need for sophisticated techniques\\nto tackle this problem. Current methods for estimating food portions are grouped into four categories.\\nStereo-Based Approaches use multiple images to reconstruct the 3D structure of food. Some methods\\nestimate food volume using multi-view stereo reconstruction based on epipolar geometry, while\\nothers perform two-view dense reconstruction. Simultaneous Localization and Mapping (SLAM) has\\nalso been used for continuous, real-time food volume estimation. However, these methods are limited\\nby their need for multiple images, which is not always practical.\\nModel-Based Approaches use predefined shapes and templates to estimate volume. For instance,\\ncertain templates are assigned to foods from a library and transformed based on physical references to\\nestimate the size and location of the food. Template matching approaches estimate food volume from\\na single image, but they struggle with variations in food shapes that differ from predefined templates.\\nRecent work has used 3D food meshes as templates to align camera and object poses for portion size\\nestimation.\\nDepth Camera-Based Approaches use depth cameras to create depth maps, capturing the distance from\\nthe camera to the food. These depth maps form a voxel representation used for volume estimation.\\nThe main drawback is the need for high-quality depth maps and the extra processing required for\\nconsumer-grade depth sensors.\\nDeep Learning Approaches utilize neural networks trained on large image datasets for portion\\nestimation. Regression networks estimate the energy value of food from single images or from an\\n\"Energy Distribution Map\" that maps input images to energy distributions. Some networks use both\\nimages and depth maps to estimate energy, mass, and macronutrient content. However, deep learning\\nmethods require extensive data for training and are not always interpretable, with performance\\ndegrading when test images significantly differ from training data.\\nWhile these methods have advanced food portion estimation, they face limitations that hinder their\\nwidespread use and accuracy. Stereo-based methods are impractical for single images, model-based\\napproaches struggle with diverse food shapes, depth camera methods need specialized hardware,\\nand deep learning approaches lack interpretability and struggle with out-of-distribution samples. 3D\\nreconstruction offers a promising solution by providing comprehensive spatial information, adapting\\nto various shapes, potentially working with single images, offering visually interpretable results,\\nand enabling a standardized approach to food portion estimation. These benefits motivated the\\norganization of the 3D Food Reconstruction challenge, aiming to overcome existing limitations and\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 2}, page_content='develop more accurate, user-friendly, and widely applicable food portion estimation techniques,\\nimpacting nutritional assessment and dietary monitoring.\\n3 Datasets and Evaluation Pipeline\\n3.1 Dataset Description\\nThe dataset for the MetaFood Challenge features 20 carefully chosen food items from the MetaFood3D\\ndataset, each scanned in 3D and accompanied by video recordings. To ensure precise size accuracy\\nin the reconstructed 3D models, each food item was captured alongside a checkerboard and pattern\\nmat, serving as physical scaling references. The challenge is divided into three levels of difficulty,\\ndetermined by the quantity of 2D images provided for reconstruction:\\n• Easy: Around 200 images taken from video.\\n• Medium: 30 images.\\n• Hard: A single image from a top-down perspective.\\nTable 1 details the food items included in the dataset.\\nTable 1: MetaFood Challenge Data Details\\nObject Index Food Item Difficulty Level Number of Frames\\n1 Strawberry Easy 199\\n2 Cinnamon bun Easy 200\\n3 Pork rib Easy 200\\n4 Corn Easy 200\\n5 French toast Easy 200\\n6 Sandwich Easy 200\\n7 Burger Easy 200\\n8 Cake Easy 200\\n9 Blueberry muffin Medium 30\\n10 Banana Medium 30\\n11 Salmon Medium 30\\n12 Steak Medium 30\\n13 Burrito Medium 30\\n14 Hotdog Medium 30\\n15 Chicken nugget Medium 30\\n16 Everything bagel Hard 1\\n17 Croissant Hard 1\\n18 Shrimp Hard 1\\n19 Waffle Hard 1\\n20 Pizza Hard 1\\n3.2 Evaluation Pipeline\\nThe evaluation process is split into two phases, focusing on the accuracy of the reconstructed 3D\\nmodels in terms of shape (3D structure) and portion size (volume).\\n3.2.1 Phase-I: Volume Accuracy\\nIn the first phase, the Mean Absolute Percentage Error (MAPE) is used to evaluate portion size\\naccuracy, calculated as follows:\\nMAPE = 1\\nn\\nnX\\ni=1\\n\\x0c\\x0c\\x0c\\x0c\\nAi − Fi\\nAi\\n\\x0c\\x0c\\x0c\\x0c × 100% (1)\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 3}, page_content='where Ai is the actual volume (in ml) of the i-th food item obtained from the scanned 3D food mesh,\\nand Fi is the volume calculated from the reconstructed 3D mesh.\\n3.2.2 Phase-II: Shape Accuracy\\nTeams that perform well in Phase-I are asked to submit complete 3D mesh files for each food item.\\nThis phase involves several steps to ensure precision and fairness:\\n• Model Verification: Submitted models are checked against the final Phase-I submissions for\\nconsistency, and visual inspections are conducted to prevent rule violations.\\n• Model Alignment: Participants receive ground truth 3D models and a script to compute the\\nfinal Chamfer distance. They must align their models with the ground truth and prepare a\\ntransformation matrix for each submitted object. The final Chamfer distance is calculated\\nusing these models and matrices.\\n• Chamfer Distance Calculation: Shape accuracy is assessed using the Chamfer distance\\nmetric. Given two point sets X and Y , the Chamfer distance is defined as:\\ndCD(X, Y) = 1\\n|X|\\nX\\nx∈X\\nmin\\ny∈Y\\n∥x − y∥2\\n2 + 1\\n|Y |\\nX\\ny∈Y\\nmin\\nx∈X\\n∥x − y∥2\\n2 (2)\\nThis metric offers a comprehensive measure of similarity between the reconstructed 3D models and\\nthe ground truth. The final ranking is determined by combining scores from both Phase-I (volume\\naccuracy) and Phase-II (shape accuracy). Note that after the Phase-I evaluation, quality issues were\\nfound with the data for object 12 (steak) and object 15 (chicken nugget), so these items were excluded\\nfrom the final overall evaluation.\\n4 First Place Team - VolETA\\n4.1 Methodology\\nThe team’s research employs multi-view reconstruction to generate detailed food meshes and calculate\\nprecise food volumes.\\n4.1.1 Overview\\nThe team’s method integrates computer vision and deep learning to accurately estimate food volume\\nfrom RGBD images and masks. Keyframe selection ensures data quality, supported by perceptual\\nhashing and blur detection. Camera pose estimation and object segmentation pave the way for neural\\nsurface reconstruction, creating detailed meshes for volume estimation. Refinement steps, including\\nisolated piece removal and scaling factor adjustments, enhance accuracy. This approach provides a\\nthorough solution for accurate food volume assessment, with potential uses in nutrition analysis.\\n4.1.2 The Team’s Proposal: VolETA\\nThe team starts by acquiring input data, specifically RGBD images and corresponding food object\\nmasks. The RGBD images, denoted as ID = {IDi}n\\ni=1, where n is the total number of frames,\\nprovide depth information alongside RGB images. The food object masks, {Mf\\ni }n\\ni=1, help identify\\nregions of interest within these images.\\nNext, the team selects keyframes. From the set {IDi}n\\ni=1, keyframes {IK\\nj }k\\nj=1 ⊆ {IDi}n\\ni=1 are\\nchosen. A method is implemented to detect and remove duplicate and blurry images, ensuring\\nhigh-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fourier\\ntransform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-\\ning to detect similar images and retain overlapping ones. Duplicates and blurry images are excluded\\nto maintain data integrity and accuracy.\\nUsing the selected keyframes {IK\\nj }k\\nj=1, the team estimates camera poses through a method called\\nPixSfM, which involves extracting features using SuperPoint, matching them with SuperGlue, and\\nrefining them. The outputs are the camera poses {Cj}k\\nj=1, crucial for understanding the scene’s\\nspatial layout.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 4}, page_content='In parallel, the team uses a tool called SAM for reference object segmentation. SAM segments\\nthe reference object with a user-provided prompt, producing a reference object mask MR for each\\nkeyframe. This mask helps track the reference object across all frames. The XMem++ method\\nextends the reference object mask MR to all frames, creating a comprehensive set of reference object\\nmasks {MR\\ni }n\\ni=1. This ensures consistent reference object identification throughout the dataset.\\nTo create RGBA images, the team combines RGB images, reference object masks {MR\\ni }n\\ni=1, and\\nfood object masks {MF\\ni }n\\ni=1. This step, denoted as {IR\\ni }n\\ni=1, integrates various data sources into a\\nunified format for further processing.\\nThe team converts the RGBA images {IR\\ni }n\\ni=1 and camera poses {Cj}k\\nj=1 into meaningful metadata\\nand modeled data Dm. This transformation facilitates accurate scene reconstruction.\\nThe modeled data Dm is input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes\\n{Rf , Rr} for the reference and food objects, providing detailed 3D representations. The team uses the\\n\"Remove Isolated Pieces\" technique to refine the meshes. Given that the scenes contain only one food\\nitem, the diameter threshold is set to 5% of the mesh size. This method deletes isolated connected\\ncomponents with diameters less than or equal to 5%, resulting in a cleaned mesh {RCf , RCr}. This\\nstep ensures that only significant parts of the mesh are retained.\\nThe team manually identifies an initial scaling factor S using the reference mesh via MeshLab. This\\nfactor is fine-tuned to Sf using depth information and food and reference masks, ensuring accurate\\nscaling relative to real-world dimensions. Finally, the fine-tuned scaling factor Sf is applied to the\\ncleaned food mesh RCf , producing the final scaled food mesh RFf . This step culminates in an\\naccurately scaled 3D representation of the food object, enabling precise volume estimation.\\n4.1.3 Detecting the scaling factor\\nGenerally, 3D reconstruction methods produce unitless meshes by default. To address this, the team\\nmanually determines the scaling factor by measuring the distance for each block of the reference\\nobject mesh. The average of all block lengths lavg is calculated, while the actual real-world length is\\nconstant at lreal = 0.012 meters. The scaling factor S = lreal/lavg is applied to the clean food mesh\\nRCf , resulting in the final scaled food mesh RFf in meters.\\nThe team uses depth information along with food and reference object masks to validate the scaling\\nfactors. The method for assessing food size involves using overhead RGB images for each scene.\\nInitially, the pixel-per-unit (PPU) ratio (in meters) is determined using the reference object. Subse-\\nquently, the food width (fw) and length (fl) are extracted using a food object mask. To determine the\\nfood height (fh), a two-step process is followed. First, binary image segmentation is performed using\\nthe overhead depth and reference images, yielding a segmented depth image for the reference object.\\nThe average depth is then calculated using the segmented reference object depth ( dr). Similarly,\\nemploying binary image segmentation with an overhead food object mask and depth image, the\\naverage depth for the segmented food depth image (df ) is computed. The estimated food height fh is\\nthe absolute difference between dr and df . To assess the accuracy of the scaling factor S, the food\\nbounding box volume (fw × fl × fh) × PPU is computed. The team evaluates if the scaling factor\\nS generates a food volume close to this potential volume, resulting in Sfine . Table 2 lists the scaling\\nfactors, PPU, 2D reference object dimensions, 3D food object dimensions, and potential volume.\\nFor one-shot 3D reconstruction, the team uses One-2-3-45 to reconstruct a 3D model from a single\\nRGBA view input after applying binary image segmentation to both food RGB and mask images.\\nIsolated pieces are removed from the generated mesh, and the scaling factor S, which is closer to the\\npotential volume of the clean mesh, is reused.\\n4.2 Experimental Results\\n4.2.1 Implementation settings\\nExperiments were conducted using two GPUs: GeForce GTX 1080 Ti/12G and RTX 3060/6G. The\\nHamming distance for near image similarity was set to 12. For Gaussian kernel radius, even numbers\\nin the range [0...30] were used for detecting blurry images. The diameter for removing isolated pieces\\nwas set to 5%. NeuS2 was run for 15,000 iterations with a mesh resolution of 512x512, a unit cube\\n\"aabb scale\" of 1, \"scale\" of 0.15, and \"offset\" of [0.5, 0.5, 0.5] for each food scene.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 5}, page_content='4.2.2 VolETA Results\\nThe team extensively validated their approach on the challenge dataset and compared their results\\nwith ground truth meshes using MAPE and Chamfer distance metrics. The team’s approach was\\napplied separately to each food scene. A one-shot food volume estimation approach was used if\\nthe number of keyframes k equaled 1; otherwise, a few-shot food volume estimation was applied.\\nNotably, the keyframe selection process chose 34.8% of the total frames for the rest of the pipeline,\\nshowing the minimum frames with the highest information.\\nTable 2: List of Extracted Information Using RGBD and Masks\\nLevel Id Label Sf PPU Rw × Rl (fw × fl × fh)\\n1 Strawberry 0.08955223881 0.01786 320 × 360 (238 × 257 × 2.353)\\n2 Cinnamon bun 0.1043478261 0.02347 236 × 274 (363 × 419 × 2.353)\\n3 Pork rib 0.1043478261 0.02381 246 × 270 (435 × 778 × 1.176)\\nEasy 4 Corn 0.08823529412 0.01897 291 × 339 (262 × 976 × 2.353)\\n5 French toast 0.1034482759 0.02202 266 × 292 (530 × 581 × 2.53)\\n6 Sandwich 0.1276595745 0.02426 230 × 265 (294 × 431 × 2.353)\\n7 Burger 0.1043478261 0.02435 208 × 264 (378 × 400 × 2.353)\\n8 Cake 0.1276595745 0.02143 256 × 300 (298 × 310 × 4.706)\\n9 Blueberry muffin 0.08759124088 0.01801 291 × 357 (441 × 443 × 2.353)\\n10 Banana 0.08759124088 0.01705 315 × 377 (446 × 857 × 1.176)\\nMedium 11 Salmon 0.1043478261 0.02390 242 × 269 (201 × 303 × 1.176)\\n13 Burrito 0.1034482759 0.02372 244 × 271 (251 × 917 × 2.353)\\n14 Frankfurt sandwich 0.1034482759 0.02115 266 × 304 (400 × 1022 × 2.353)\\n16 Everything bagel 0.08759124088 0.01747 306 × 368 (458 × 134 × 1.176)\\nHard 17 Croissant 0.1276595745 0.01751 319 × 367 (395 × 695 × 2.176)\\n18 Shrimp 0.08759124088 0.02021 249 × 318 (186 × 95 × 0.987)\\n19 Waffle 0.01034482759 0.01902 294 × 338 (465 × 537 × 0.8)\\n20 Pizza 0.01034482759 0.01913 292 × 336 (442 × 651 × 1.176)\\nAfter finding keyframes, PixSfM estimated the poses and point cloud. After generating scaled meshes,\\nthe team calculated volumes and Chamfer distance with and without transformation metrics. Meshes\\nwere registered with ground truth meshes using ICP to obtain transformation metrics.\\nTable 3 presents quantitative comparisons of the team’s volumes and Chamfer distance with and\\nwithout estimated transformation metrics from ICP. For overall method performance, Table 4 shows\\nthe MAPE and Chamfer distance with and without transformation metrics.\\nAdditionally, qualitative results on one- and few-shot 3D reconstruction from the challenge dataset\\nare shown. The model excels in texture details, artifact correction, missing data handling, and color\\nadjustment across different scene parts.\\nLimitations: Despite promising results, several limitations need to be addressed in future work:\\n• Manual processes: The current pipeline includes manual steps like providing segmentation\\nprompts and identifying scaling factors, which should be automated to enhance efficiency.\\n• Input requirements: The method requires extensive input information, including food\\nmasks and depth data. Streamlining these inputs would simplify the process and increase\\napplicability.\\n• Complex backgrounds and objects: The method has not been tested in environments with\\ncomplex backgrounds or highly intricate food objects.\\n• Capturing complexities: The method has not been evaluated under different capturing\\ncomplexities, such as varying distances and camera speeds.\\n• Pipeline complexity: For one-shot neural rendering, the team currently uses One-2-3-45.\\nThey aim to use only the 2D diffusion model, Zero123, to reduce complexity and improve\\nefficiency.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 6}, page_content='Table 3: Quantitative Comparison with Ground Truth Using Chamfer Distance\\nL Id Team’s V ol. GT V ol. Ch. w/ t.m Ch. w/o t.m\\n1 40.06 38.53 1.63 85.40\\n2 216.9 280.36 7.12 111.47\\n3 278.86 249.67 13.69 172.88\\nE 4 279.02 295.13 2.03 61.30\\n5 395.76 392.58 13.67 102.14\\n6 205.17 218.44 6.68 150.78\\n7 372.93 368.77 4.70 66.91\\n8 186.62 173.13 2.98 152.34\\n9 224.08 232.74 3.91 160.07\\n10 153.76 163.09 2.67 138.45\\nM 11 80.4 85.18 3.37 151.14\\n13 363.99 308.28 5.18 147.53\\n14 535.44 589.83 4.31 89.66\\n16 163.13 262.15 18.06 28.33\\nH 17 224.08 181.36 9.44 28.94\\n18 25.4 20.58 4.28 12.84\\n19 110.05 108.35 11.34 23.98\\n20 130.96 119.83 15.59 31.05\\nTable 4: Quantitative Comparison with Ground Truth Using MAPE and Chamfer Distance\\nMAPE Ch. w/ t.m Ch. w/o t.m\\n(%) sum mean sum mean\\n10.973 0.130 0.007 1.715 0.095\\n5 Second Place Team - ININ-VIAUN\\n5.1 Methodology\\nThis section details the team’s proposed network, illustrating the step-by-step process from original\\nimages to final mesh models.\\n5.1.1 Scale factor estimation\\nThe procedure for estimating the scale factor at the coordinate level is illustrated in Figure 9. The\\nteam adheres to a method involving corner projection matching. Specifically, utilizing the COLMAP\\ndense model, the team acquires the pose of each image along with dense point cloud data. For any\\ngiven image imgk and its extrinsic parameters [R|t]k, the team initially performs threshold-based\\ncorner detection, setting the threshold at 240. This step allows them to obtain the pixel coordinates\\nof all detected corners. Subsequently, using the intrinsic parameters k and the extrinsic parameters\\n[R|t]k, the point cloud is projected onto the image plane. Based on the pixel coordinates of the\\ncorners, the team can identify the closest point coordinates Pk\\ni for each corner, where i represents the\\nindex of the corner. Thus, they can calculate the distance between any two corners as follows:\\nDk\\nij = (Pk\\ni − Pk\\nj )2 ∀i ̸= j (3)\\nTo determine the final computed length of each checkerboard square in image k, the team takes the\\nminimum value of each row of the matrix Dk (excluding the diagonal) to form the vector dk. The\\nmedian of this vector is then used. The final scale calculation formula is given by Equation 4, where\\n0.012 represents the known length of each square (1.2 cm):\\nscale = 0.012Pn\\ni=1 med(dk) (4)\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 7}, page_content='5.1.2 3D Reconstruction\\nThe 3D reconstruction process, depicted in Figure 10, involves two different pipelines to accommodate\\nvariations in input viewpoints. The first fifteen objects are processed using one pipeline, while the\\nlast five single-view objects are processed using another.\\nFor the initial fifteen objects, the team uses COLMAP to estimate poses and segment the food using\\nthe provided segment masks. Advanced multi-view 3D reconstruction methods are then applied to\\nreconstruct the segmented food. The team employs three different reconstruction methods: COLMAP,\\nDiffusioNeRF, and NeRF2Mesh. They select the best reconstruction results from these methods and\\nextract the mesh. The extracted mesh is scaled using the estimated scale factor, and optimization\\ntechniques are applied to obtain a refined mesh.\\nFor the last five single-view objects, the team experiments with several single-view reconstruction\\nmethods, including Zero123, Zero123++, One2345, ZeroNVS, and DreamGaussian. They choose\\nZeroNVS to obtain a 3D food model consistent with the distribution of the input image. The\\nintrinsic camera parameters from the fifteenth object are used, and an optimization method based\\non reprojection error refines the extrinsic parameters of the single camera. Due to limitations in\\nsingle-view reconstruction, depth information from the dataset and the checkerboard in the monocular\\nimage are used to determine the size of the extracted mesh. Finally, optimization techniques are\\napplied to obtain a refined mesh.\\n5.1.3 Mesh refinement\\nDuring the 3D Reconstruction phase, it was observed that the model’s results often suffered from low\\nquality due to holes on the object’s surface and substantial noise, as shown in Figure 11.\\nTo address the holes, MeshFix, an optimization method based on computational geometry, is em-\\nployed. For surface noise, Laplacian Smoothing is used for mesh smoothing operations. The\\nLaplacian Smoothing method adjusts the position of each vertex to the average of its neighboring\\nvertices:\\nV (new)\\ni = V (old)\\ni + λ\\n\\uf8eb\\n\\uf8ed 1\\n|N(i)|\\nX\\nj∈N(i)\\nV (old)\\nj − V (old)\\ni\\n\\uf8f6\\n\\uf8f8 (5)\\nIn their implementation, the smoothing factor λ is set to 0.2, and 10 iterations are performed.\\n5.2 Experimental Results\\n5.2.1 Estimated scale factor\\nThe scale factors estimated using the described method are shown in Table 5. Each image and the\\ncorresponding reconstructed 3D model yield a scale factor, and the table presents the average scale\\nfactor for each object.\\n5.2.2 Reconstructed meshes\\nThe refined meshes obtained using the described methods are shown in Figure 12. The predicted\\nmodel volumes, ground truth model volumes, and the percentage errors between them are presented\\nin Table 6.\\n5.2.3 Alignment\\nThe team designs a multi-stage alignment method for evaluating reconstruction quality. Figure 13\\nillustrates the alignment process for Object 14. First, the central points of both the predicted and\\nground truth models are calculated, and the predicted model is moved to align with the central point\\nof the ground truth model. Next, ICP registration is performed for further alignment, significantly\\nreducing the Chamfer distance. Finally, gradient descent is used for additional fine-tuning to obtain\\nthe final transformation matrix.\\nThe total Chamfer distance between all 18 predicted models and the ground truths is 0.069441169.\\n8'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 8}, page_content='Table 5: Estimated Scale Factors\\nObject Index Food Item Scale Factor\\n1 Strawberry 0.060058\\n2 Cinnamon bun 0.081829\\n3 Pork rib 0.073861\\n4 Corn 0.083594\\n5 French toast 0.078632\\n6 Sandwich 0.088368\\n7 Burger 0.103124\\n8 Cake 0.068496\\n9 Blueberry muffin 0.059292\\n10 Banana 0.058236\\n11 Salmon 0.083821\\n13 Burrito 0.069663\\n14 Hotdog 0.073766\\nTable 6: Metric of V olume\\nObject Index Predicted V olume Ground Truth Error Percentage\\n1 44.51 38.53 15.52\\n2 321.26 280.36 14.59\\n3 336.11 249.67 34.62\\n4 347.54 295.13 17.76\\n5 389.28 392.58 0.84\\n6 197.82 218.44 9.44\\n7 412.52 368.77 11.86\\n8 181.21 173.13 4.67\\n9 233.79 232.74 0.45\\n10 160.06 163.09 1.86\\n11 86.0 85.18 0.96\\n13 334.7 308.28 8.57\\n14 517.75 589.83 12.22\\n16 176.24 262.15 32.77\\n17 180.68 181.36 0.37\\n18 13.58 20.58 34.01\\n19 117.72 108.35 8.64\\n20 117.43 119.83 20.03\\n6 Best 3D Mesh Reconstruction Team - FoodRiddle\\n6.1 Methodology\\nTo achieve high-fidelity food mesh reconstruction, the team developed two procedural pipelines as\\ndepicted in Figure 14. For simple and medium complexity cases, they employed a structure-from-\\nmotion strategy to ascertain the pose of each image, followed by mesh reconstruction. Subsequently,\\na sequence of post-processing steps was implemented to recalibrate the scale and improve mesh\\nquality. For cases involving only a single image, the team utilized image generation techniques to\\nfacilitate model generation.\\n6.1.1 Multi-View Reconstruction\\nFor Structure from Motion (SfM), the team enhanced the advanced COLMAP method by integrating\\nSuperPoint and SuperGlue techniques. This integration significantly addressed the issue of limited\\nkeypoints in scenes with minimal texture, as illustrated in Figure 15.\\nIn the mesh reconstruction phase, the team’s approach builds upon 2D Gaussian Splatting, which\\nemploys a differentiable 2D Gaussian renderer and includes regularization terms for depth distortion\\n9'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/CVPR/R007.pdf', 'page': 9}, page_content='and normal consistency. The Truncated Signed Distance Function (TSDF) results are utilized to\\nproduce a dense point cloud.\\nDuring post-processing, the team applied filtering and outlier removal methods, identified the outline\\nof the supporting surface, and projected the lower mesh vertices onto this surface. They utilized\\nthe reconstructed checkerboard to correct the model’s scale and employed Poisson reconstruction to\\ncreate a complete, watertight mesh of the subject.\\n6.1.2 Single-View Reconstruction\\nFor 3D reconstruction from a single image, the team utilized advanced methods such as LGM, Instant\\nMesh, and One-2-3-45 to generate an initial mesh. This initial mesh was then refined in conjunction\\nwith depth structure information.\\nTo adjust the scale, the team estimated the object’s length using the checkerboard as a reference,\\nassuming that the object and the checkerboard are on the same plane. They then projected the 3D\\nobject back onto the original 2D image to obtain a more precise scale for the object.\\n6.2 Experimental Results\\nThrough a process of nonlinear optimization, the team sought to identify a transformation that\\nminimizes the Chamfer distance between their mesh and the ground truth mesh. This optimization\\naimed to align the two meshes as closely as possible in three-dimensional space. Upon completion\\nof this process, the average Chamfer dis- tance across the final reconstructions of the 20 objects\\namounted to 0.0032175 meters. As shown in Table 7, Team FoodRiddle achieved the best scores for\\nboth multi- view and single-view reconstructions, outperforming other teams in the competition.\\nTable 7: Total Errors for Different Teams on Multi-view and Single-view Data\\nTeam Multi-view (1-14) Single-view (16-20)\\nFoodRiddle 0.036362 0.019232\\nININ-VIAUN 0.041552 0.027889\\nV olETA 0.071921 0.058726\\n7 Conclusion\\nThis report examines and compiles the techniques and findings from the MetaFood Workshop\\nchallenge on 3D Food Reconstruction. The challenge sought to enhance 3D reconstruction methods\\nby concentrating on food items, tackling the distinct difficulties presented by varied textures, reflective\\nsurfaces, and intricate geometries common in culinary subjects.\\nThe competition involved 20 diverse food items, captured under various conditions and with differing\\nnumbers of input images, specifically designed to challenge participants in creating robust reconstruc-\\ntion models. The evaluation was based on a two-phase process, assessing both portion size accuracy\\nthrough Mean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distance\\nmetric.\\nOf all participating teams, three reached the final submission stage, presenting a range of innovative\\nsolutions. Team V olETA secured first place with the best overall performance in both Phase-I and\\nPhase-II, followed by team ININ-VIAUN in second place. Additionally, the FoodRiddle team\\nexhibited superior performance in Phase-II, highlighting a competitive and high-caliber field of\\nentries for 3D mesh reconstruction. The challenge has successfully advanced the field of 3D food\\nreconstruction, demonstrating the potential for accurate volume estimation and shape reconstruction\\nin nutritional analysis and food presentation applications. The novel methods developed by the\\nparticipating teams establish a strong foundation for future research in this area, potentially leading\\nto more precise and user-friendly approaches for dietary assessment and monitoring.\\n10')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 0}, page_content='Examining the Convergence of Denoising Diffusion Probabilistic\\nModels: A Quantitative Analysis\\nAbstract\\nDeep generative models, particularly diffusion models, are a significant family within deep learning. This study\\nprovides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion model\\nand the target distribution. In contrast to earlier research, this analysis does not rely on presumptions regarding\\nthe learned score function. Furthermore, the findings are applicable to any data-generating distributions within\\nrestricted instance spaces, even those lacking a density relative to the Lebesgue measure, and the upper limit is not\\nexponentially dependent on the ambient space dimension. The primary finding expands upon recent research by\\nMbacke et al. (2023), and the proofs presented are fundamental.\\n1 Introduction\\nDiffusion models, alongside generative adversarial networks and variational autoencoders (V AEs), are among the most influential\\nfamilies of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,\\nas well as in various other applications.\\nTwo primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative\\nmodels (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while\\nsimultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Conversely, SGMs\\nemploy score-matching methods to approximate the score function of the data-generating distribution, subsequently generating new\\nsamples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function, adding varying\\nnoise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the score\\nfunction for all noise levels has been proposed.\\nAlthough DDPMs and SGMs may initially seem distinct, it has been demonstrated that DDPMs implicitly approximate the score\\nfunction, with the sampling process resembling Langevin dynamics. Moreover, a unified perspective of both methods using stochastic\\ndifferential equations (SDEs) has been derived. The SGM can be viewed as a discretization of Brownian motion, and the DDPM as a\\ndiscretization of an Ornstein-Uhlenbeck process. Consequently, both DDPMs and SGMs are commonly referred to as SGMs in the\\nliterature. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-based\\nframework, necessitating assumptions about the effectiveness of the learned score function.\\nIn this research, a different strategy is employed, applying methods created for V AEs to DDPMs, which can be viewed as hierarchical\\nV AEs with fixed encoders. This method enables the derivation of quantitative, Wasserstein-based upper bounds without making\\nassumptions about the data distribution or the learned score function, and with simple proofs that do not need the SDE toolkit.\\nFurthermore, the bounds presented here do not involve any complex discretization steps, as the forward and backward processes are\\nconsidered discrete-time from the beginning, rather than being viewed as discretizations of continuous-time processes.\\n1.1 Related Works\\nThere has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. However,\\nthese studies frequently depend on restrictive assumptions regarding the data-generating distribution, produce non-quantitative upper\\nbounds, or exhibit exponential dependencies on certain parameters. This work successfully circumvents all three of these limitations.\\nSome bounds are based on very restrictive assumptions about the data-generating distribution, such as log-Sobolev inequalities,\\nwhich are unrealistic for real-world data distributions. Furthermore, some studies establish upper bounds on the Kullback-Leibler\\n(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by the\\ndiffusion model; however, unless strong assumptions are made about the support of the data-generating distribution, KL and TV\\nreach their maximum values. Such assumptions arguably do not hold for real-world data-generating distributions, which are widely\\nbelieved to satisfy the manifold hypothesis. Other work establishes conditions under which the support of the input distribution\\nis equal to the support of the learned distribution, and generalizes the bound to all f-divergences. Assuming L2 accurate score'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 1}, page_content='estimation, some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, but\\ntheir Wasserstein-based bounds are not quantitative. Quantitative Wasserstein distance upper bounds under the manifold hypothesis\\nhave been derived, but these bounds exhibit exponential dependencies on some of the problem parameters.\\n1.2 Our contributions\\nIn this study, strong assumptions about the data-generating distribution are avoided, and a quantitative upper bound on the Wasserstein\\ndistance is established without exponential dependencies on problem parameters, including the ambient space dimension. Moreover,\\na common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According to\\nsome, providing precise guarantees for the estimation of the score function is challenging, as it necessitates an understanding of the\\nnon-convex training dynamics of neural network optimization, which is currently beyond reach. Therefore, upper bounds are derived\\nwithout making assumptions about the learned score function. Instead, the bound presented here is dependent on a reconstruction\\nloss calculated over a finite independent and identically distributed (i.i.d.) sample. Intuitively, a loss function is defined, which\\nquantifies the average Euclidean distance between a sample from the data-generating distribution and the reconstruction obtained by\\nsampling noise and passing it through the backward process (parameterized by ˘03b8). This method is inspired by previous work on\\nV AEs.\\nThis approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution, avoids\\nexponential dependencies on the dimension, and provides a quantitative upper bound based on the Wasserstein distance. Furthermore,\\nthis method benefits from utilizing very straightforward and basic proofs.\\n2 Preliminaries\\nThroughout this paper, lowercase letters are used to represent both probability measures and their densities with respect to the\\nLebesgue measure, and variables are added in parentheses to enhance readability (e.g., q(xt|xt−1) to denote a time-dependent\\nconditional distribution). An instance space X, which is a subset of RD with the Euclidean distance as the underlying metric, and\\na target data-generating distribution µ ∈ M+\\n1 (X) are considered. Note that it is not assumed that µ has a density with respect to\\nthe Lebesgue measure. Additionally, || · ||represents the Euclidean (L2) norm, and Ep(x) is used as shorthand for Ex∼p(x). Given\\nprobability measures p, q∈ M+\\n1 (X) and a real number k >1, the Wasserstein distance of order k is defined as (Villani, 2009):\\nWk(p, q) = inf\\nγ∈Γ(p,q)\\n\\x12Z\\nX×X\\n||x − y||kdγ(x, y)\\n\\x131/k\\n,\\nwhere Γ(p, q) denotes the set of couplings of p and q, meaning the set of joint distributions on X × X with respective marginals p\\nand q. The product measure p ⊗ q is referred to as the trivial coupling, and the Wasserstein distance of order 1 is simply referred to\\nas the Wasserstein distance.\\n2.1 Denoising Diffusion Models\\nInstead of employing the SDE framework, diffusion models are presented using the DDPM formulation with discrete-time processes.\\nA diffusion model consists of two discrete-time stochastic processes: a forward process and a backward process. Both processes are\\nindexed by time 0 ≤ t ≤ T, where the number of time steps T is a predetermined choice.\\n**The forward process.** The forward process transforms a data point x0 ∼ µ into a noise distribution q(xT |x0) through a sequence\\nof conditional distributions q(xt|xt−1) for 1 ≤ t ≤ T. It is assumed that the forward process is defined such that for sufficiently\\nlarge T, the distribution q(xT |x0) is close to a simple noise distribution p(xT ), which is referred to as the prior distribution. For\\ninstance, p(xT ) = N(xT ; 0, I), the standard multivariate normal distribution, has been chosen in previous work.\\n**The backward process.** The backward process is a Markov process with parametric transition kernels. The objective of the\\nbackward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samples\\nfrom the distribution µ. Following previous work, it is assumed that the backward process is defined by Gaussian distributions\\npθ(xt−1|xt) for 2 ≤ t ≤ T as\\npθ(xt−1|xt) = N(xt−1; gθ\\nt (xt), σ2\\nt I),\\nand\\npθ(x0|x1) = gθ\\n1(x1),\\nwhere the variance parameters σ2\\nt ∈ R≥0 are defined by a fixed schedule, the mean functions gθ\\nt : RD → RD are learned using a\\nneural network (with parameters θ) for 2 ≤ t ≤ T, and gθ\\n1 : RD → X is a separate function dependent on σ1. In practice, the same\\nnetwork has been used for the functions gθ\\nt for 2 ≤ t ≤ T, and a separate discrete decoder for gθ\\n1.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 2}, page_content='Generating new samples from a trained diffusion model is accomplished by sampling xt−1 ∼ pθ(xt−1|xt) for 1 ≤ t ≤ T, starting\\nfrom a noise vector xT ∼ p(xT ) sampled from the prior p(xT ).\\nThe following assumption is made regarding the backward process.\\n**Assumption 1.** It is assumed that for each 1 ≤ t ≤ T, there exists a constant Kθ\\nt > 0 such that for every x1, x2 ∈ X,\\n||gθ\\nt (x1) − gθ\\nt (x2)|| ≤Kθ\\nt ||x1 − x2||.\\nIn other words, gθ\\nt is Kθ\\nt -Lipschitz continuous. This assumption is discussed in Remark 3.2.\\n2.2 Additional Definitions\\nThe distribution πθ(·|x0) is defined as\\nπθ(·|x0) = q(xT |x0)pθ(xT−1|xT )pθ(xT−2|xT−1) . . . pθ(x1|x2)pθ(·|x1).\\nIntuitively, for each x0 ∈ X, πθ(·|x0) represents the distribution on X obtained by reconstructing samples from q(xT |x0) through\\nthe backward process. Another way to interpret this distribution is that for any function f : X → R, the following equation holds:\\nEπθ(ˆx0|x0)[f(ˆx0)] = Eq(xT |x0)Epθ(xT−1|xT ) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].\\nGiven a finite set S = {x1\\n0, . . . , xn\\n0 } i.i.d. ∼ µ, the regenerated distribution is defined as the following mixture:\\nµθ\\nn = 1\\nn\\nnX\\ni=1\\nπθ(·|xi\\n0).\\nThis definition is analogous to the empirical regenerated distribution defined for V AEs. The distribution on X learned by the\\ndiffusion model is denoted as πθ(·) and defined as\\nπθ(·) = p(xT )pθ(xT−1|xT )pθ(xT−2|xT−1) . . . pθ(x1|x2)pθ(·|x1).\\nIn other words, for any function f : X → R, the expectation of f with respect to πθ(·) is\\nEπθ(ˆx0)[f(ˆx0)] = Ep(xT )Epθ(xT−1|xT ) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].\\nHence, both πθ(·) and πθ(·|x0) are defined using the backward process, with the difference that πθ(·) starts with the prior\\np(xT ) = N(xT ; 0, I), while πθ(·|x0) starts with the noise distribution q(xT |x0).\\nFinally, the loss function lθ : X × X → R is defined as\\nlθ(xT , x0) = Epθ(xT−1|xT )Epθ(xT−2|xT−1) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[||x0 − ˆx0||].\\nHence, given a noise vector xT and a sample x0, the loss lθ(xT , x0) represents the average Euclidean distance between x0 and any\\nsample obtained by passing xT through the backward process.\\n2.3 Our Approach\\nThe goal is to upper-bound the distance W1(µ, πθ(·)). Since the triangle inequality implies\\nW1(µ, πθ(·)) ≤ W1(µ, µθ\\nn) + W1(µθ\\nn, πθ(·)),\\nthe distance W1(µ, πθ(·)) can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The\\nupper bound on W1(µ, µθ\\nn) is obtained using a straightforward adaptation of a proof. First, W1(µ, µθ\\nn) is upper-bounded using the\\nexpectation of the loss function lθ, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependent\\non the empirical risk and the prior-matching term.\\nThe upper bound on the second term W1(µθ\\nn, πθ(·)) uses the definition of µθ\\nn. Intuitively, the difference between πθ(·|xi\\n0) and πθ(·)\\nis determined by the corresponding initial distributions: q(xT |xi\\n0) and p(xT ) for πθ(·). Hence, if the two initial distributions are\\nclose, and if the steps of the backward process are smooth (see Assumption 1), then πθ(·|xi\\n0) and πθ(·) are close to each other.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 3}, page_content='3 Main Result\\n3.1 Theorem Statement\\nWe are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generating\\ndistribution µ and the learned distribution πθ(·).\\n**Theorem 3.1.** Assume the instance space X has finite diameter ∆ = supx,x′∈X ||x − x′|| < ∞, and let λ >0 and δ ∈ (0, 1) be\\nreal numbers. Using the definitions and assumptions of the previous section, the following inequality holds with probability at least\\n1 − δ over the random draw of S = {x1\\n0, . . . , xn\\n0 } i.i.d. ∼ µ:\\nW1(µ, πθ(·)) ≤1\\nn\\nnX\\ni=1\\nEq(xT |xi\\n0)[lθ(xT , xi\\n0)] + 1\\nλn\\nnX\\ni=1\\nKL(q(xT |xi\\n0)||p(xT )) + 1\\nλn log n\\nδ + λ∆2\\n8n\\n+\\n TY\\nt=1\\nKθ\\nt\\n!\\nEq(xT |xi\\n0)Ep(yT )[||xT − yT ||]\\n+\\nTX\\nt=2\\n t−1Y\\ni=1\\nKθ\\ni\\n!\\nσtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I) are standard Gaussian vectors.\\n**Remark 3.1.** Before presenting the proof, let us discuss Theorem 3.1.\\n* Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. sample S, the bound holds with\\nhigh probability with respect to the randomness of S. This is the price we pay for having a quantitative upper bound with no\\nexponential dependencies on problem parameters and no assumptions on the data-generating distribution µ. * The first term of the\\nright-hand side is the average reconstruction loss computed over the sample S = {x1\\n0, . . . , xn\\n0 }. Note that for each 1 ≤ i ≤ n, the\\nexpectation of lθ(xT |xi\\n0) is only computed with respect to the noise distribution q(xT |xi\\n0) defined by xi\\n0 itself. Hence, this term\\nmeasures how well a noise vector xT ∼ q(xT |xi\\n0) recovers the original sample xi\\n0 using the backward process, and averages over\\nthe set S = {x1\\n0, . . . , xn\\n0 }. * If the Lipschitz constants satisfy Kθ\\nt < 1 for all 1 ≤ t ≤ T, then the larger T is, the smaller the upper\\nbound gets. This is because the product of Kθ\\nt ’s then converges to 0. In Remark 3.2 below, we show that the assumption thatKθ\\nt < 1\\nfor all t is a quite reasonable one. * The hyperparameter λ controls the trade-off between the prior-matching (KL) term and the\\ndiameter term ∆2. If Kθ\\nt < 1 for all 1 ≤ t ≤ T and T → ∞, then the convergence of the bound largely depends on the choice of λ.\\nIn that case, λ ∝ n1/2 leads to faster convergence, while λ ∝ n leads to slower convergence to a smaller quantity. This is because\\nthe bound stems from PAC-Bayesian theory, where this trade-off is common. * The last term of the equation does not depend on the\\nsample size n. Hence, the upper bound given by Theorem 3.1 does not converge to 0 as n → ∞. However, if the Lipschitz factors\\n(Kθ\\nt )1≤t≤T are all less than 1, then this term can be very small, especially in low-dimensional spaces.\\n3.2 Proof of the main theorem\\nThe following result is an adaptation of a previous result.\\n**Lemma 3.2.** Let λ >0 and δ ∈ (0, 1) be real numbers. With probability at least 1 − δ over the randomness of the sample\\nS = {x1\\n0, . . . , xn\\n0 } i.i.d. ∼ µ, the following holds:\\nW1(µ, µθ\\nn) ≤ 1\\nn\\nnX\\ni=1\\nEq(xT |xi\\n0)[lθ(xT , xi\\n0)] + 1\\nλn\\nnX\\ni=1\\nKL(q(xT |xi\\n0)||p(xT )) + 1\\nλn log n\\nδ + λ∆2\\n8n .\\nThe proof of this result is a straightforward adaptation of a previous proof.\\nNow, let us focus our attention on the second term of the right-hand side of the equation, namely W1(µθ\\nn, πθ(·)). This part is trickier\\nthan for V AEs, for which the generative model’s distribution is simply a pushforward measure. Here, we have a non-deterministic\\nsampling process with T steps.\\nAssumption 1 leads to the following lemma on the backward process.\\n**Lemma 3.3.** For any given x1, y1 ∈ X, we have\\nEpθ(x0|x1)Epθ(y0|y1)[||x0 − y0||] ≤ Kθ\\n1 ||x1 − y1||.\\nMoreover, if 2 ≤ t ≤ T, then for any given xt, yt ∈ X, we have\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 4}, page_content='Epθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1 − yt−1||] ≤ Kθ\\nt ||xt − yt|| + σtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I), meaning Eϵ,ϵ′ is a shorthand for Eϵ,ϵ′∼N(0,I).\\n**Proof.** For the first part, let x1, y1 ∈ X. Since according to the equation pθ(x0|x1) = δgθ\\n1 (x1)(x0) and pθ(y0|y1) = δgθ\\n1 (y1)(y0),\\nthen\\nEpθ(x0|x1)Epθ(y0|y1)[||x0 − y0||] = ||gθ\\n1(x1) − gθ\\n1(y1)|| ≤Kθ\\n1 ||x1 − y1||.\\nFor the second part, let 2 ≤ t ≤ T and xt, yt ∈ X. Since pθ(xt−1|xt) = N(xt−1; gθ\\nt (xt), σ2\\nt I), the reparameterization trick implies\\nthat sampling xt−1 ∼ pθ(xt−1|xt) is equivalent to setting\\nxt−1 = gθ\\nt (xt) + σtϵt, with ϵt ∼ N(0, I).\\nUsing the above equation, the triangle inequality, and Assumption 1, we obtain\\nEpθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1 − yt−1||]\\n= Eϵt,ϵ′\\nt∼N(0,I)[||gθ\\nt (xt) + σtϵt − gθ\\nt (yt) − σtϵ′\\nt||]\\n≤ Eϵt,ϵ′\\nt∼N(0,I)[||gθ\\nt (xt) − gθ\\nt (yt)||] + σtEϵt,ϵ′\\nt∼N(0,I)[||ϵt − ϵ′\\nt||]\\n≤ Kθ\\nt ||xt − yt|| + σtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I).\\nNext, we can use the inequalities of Lemma 3.3 to prove the following result.\\n**Lemma 3.4.** Let T ≥ 1. The following inequality holds:\\nEpθ(xT−1|xT )Epθ(yT−1|yT )Epθ(xT−2|xT−1)Epθ(yT−2|yT−1) . . . Epθ(x0|x1)Epθ(y0|y1)[||x0 − y0||]\\n≤\\n TY\\nt=1\\nKθ\\nt\\n!\\n||xT − yT || +\\nTX\\nt=2\\n t−1Y\\ni=1\\nKθ\\ni\\n!\\nσtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I).\\n**Proof Idea.** Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step.\\nUsing the two previous lemmas, we obtain the following upper bound on W1(µθ\\nn, πθ(·)).\\n**Lemma 3.5.** The following inequality holds:\\nW1(µθ\\nn, πθ(·)) ≤ 1\\nn\\nnX\\ni=1\\n TY\\nt=1\\nKθ\\nt\\n!\\nEq(xT |xi\\n0)Ep(yT )[||xT − yT ||] +\\nTX\\nt=2\\n t−1Y\\ni=1\\nKθ\\ni\\n!\\nσtEϵ,ϵ′[||ϵ − ϵ′||],\\nwhere ϵ, ϵ′ ∼ N(0, I).\\n**Proof.** Using the definition of W1, the trivial coupling, the definitions of µθ\\nn and πθ(·), and Lemma 3.4, we get the desired result.\\nCombining Lemmas 3.2 and 3.5 with the triangle inequality yields Theorem 3.1.\\n3.3 Special case using the forward process of Ho et al. (2020)\\nTheorem 3.1 establishes a general upper bound that holds for any forward process, as long as the backward process satisfies\\nAssumption 1. In this section, we specialize the statement of the theorem to the particular case of the forward process defined in\\nprevious work.\\nLet X ⊆ RD. The forward process is a Gauss-Markov process with transition densities defined as\\nq(xt|xt−1) = N(xt; √αtxt−1, (1 − αt)I),\\nwhere α1, . . . , αT is a fixed noise schedule such that 0 < αt < 1 for all t. This definition implies that at each time step 1 ≤ t ≤ T,\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R015.pdf', 'page': 5}, page_content='q(xt|x0) = N(xt; √¯αtx0, (1 − ¯αt)I), with ¯αt =\\ntY\\ni=1\\nαi.\\nThe optimization objective to train the backward process ensures that for each time step t, the distribution pθ(xt−1|xt) remains close\\nto the ground-truth distribution q(xt−1|xt, x0) given by\\nq(xt−1|xt, x0) = N(xt−1; ˜µq\\nt (xt, x0), ˜σ2\\nt I),\\nwhere\\n˜µq\\nt (xt, x0) =\\n√αt(1 − ¯αt−1)\\n1 − ¯αt\\nxt +\\n√¯αt−1(1 − αt)\\n1 − ¯αt\\nx0.\\nNow, we discuss Assumption 1 under these definitions.\\n**Remark 3.2.** We can get a glimpse at the range of Kθ\\nt for a trained DDPM by looking at the distribution q(xt−1|xt, x0), since\\npθ(xt−1|xt) is optimized to be as close as possible to q(xt−1|xt, x0).\\nFor a given x0 ∼ µ, let us take a look at the Lipschitz norm of x 7→ ˜µq\\nt (x, x0). Using the above equation, we have\\n˜µq\\nt (xt, x0) − ˜µq\\nt (yt, x0) =\\n√αt(1 − ¯αt−1)\\n1 − ¯αt\\n(xt − yt).\\nHence, x 7→ ˜µq\\nt (x, x0) is K′\\nt-Lipschitz continuous with\\nK′\\nt =\\n√αt(1 − ¯αt−1)\\n1 − ¯αt\\n.\\nNow, if αt < 1 for all 1 ≤ t ≤ T, then we have 1 − ¯αt > 1 − ¯αt−1, which implies K′\\nt < 1 for all 1 ≤ t ≤ T.\\nRemark 3.2 shows that the Lipschitz norm of the mean function ˜µq\\nt (·, x0) does not depend on x0. Indeed, looking at the previous\\nequation, we can see that for any initial x0, the Lipschitz norm K′\\nt =\\n√αt(1−¯αt−1)\\n1−¯αt\\nonly depends on the noise schedule, not x0 itself.\\nSince gθ\\nt (·, x0) is optimized to match ˜µq\\nt (·, x0) for each x0 in the training set, and all the functions ˜µq\\nt (·, x0) have the same Lipschitz\\nnorm K′\\nt, we believe it is reasonable to assume gθ\\nt is Lipschitz continuous as well. This is the intuition behind Assumption 1.\\n**The prior-matching term.** With the definitions of this section, the prior matching term KL(q(xT |x0)||p(xT )) has the following\\nclosed form:\\nKL(q(xT |x0)||p(xT )) = 1\\n2\\n\\x02\\n−D log(1 − ¯αT ) − D¯αT + ¯αT ||x0||2\\x03\\n.\\n**Upper-bounds on the average distance between Gaussian vectors.** If ϵ, ϵ′ are D-dimensional vectors sampled from N(0, I), then\\nEϵ,ϵ′[||ϵ − ϵ′||] ≤\\n√\\n2D.\\nMoreover, since q(xT |x0) = N(xT ; √¯αT x0, (1 − ¯αT )I) and the prior p(yT ) = N(yT ; 0, I),\\nEq(xT |x0)Ep(yT )[||xT − yT ||] ≤\\np\\n¯αT ||x0||2 + (2 − ¯αT )D.\\n**Special case of the main theorem.** With the definitions of this section, the inequality of Theorem 3.1 implies that with probability\\nat least 1 − δ over the randomness of {x1\\n0, . . . , x\\n6')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 0}, page_content='Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems\\nwith Solutions Exhibiting Weak Minty Properties\\nAbstract\\nThis research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-\\nstrate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has\\nalready demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.\\nWe establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within\\nthis framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared\\noperator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified\\nversion of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem’s\\nspecific parameters.\\n1 Introduction\\nThe recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,\\nhave generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,\\nadversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed\\nthat generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization\\ncomponent and nonconcave in the maximization component remains limited, with some research even suggesting intractability in\\ncertain cases.\\nA specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)\\nexhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of\\nthe recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified\\n(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing\\nliterature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of\\nweak Minty solutions was quickly investigated.\\nAssumption 1 (Weak Minty solution). For a given operator F : Rd → Rd, there is a point u∗ ∈ Rd and a parameter ρ >0 such that:\\n⟨F(u), u− u∗⟩ ≥ −ρ\\n2∥F(u)∥2 ∀u ∈ Rd. (1)\\nMoreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving\\na complexity of O(ϵ−1) for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step\\nfollowed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant\\nof EG.\\nIn a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-\\nBackward (FoRB). We address the following question with an affirmative answer:\\nCan OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?\\nSpecifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0 and a parameter 0 < γ≤ 1\\nas follows:\\nuk = ¯uk − aF(¯uk),\\n¯uk+1 = ¯uk − γaF (uk), ∀k ≥ 0,\\ncan achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.\\nIt is worth noting that OGDA is most frequently expressed in a form where γ = 1. However, two recent studies have examined\\na more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of γ becomes'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 1}, page_content='apparent only when dealing with weak Minty solutions. In this context, we find that γ must be greater than 1 to ensure convergence,\\na phenomenon that is not observed in monotone problems.\\nWhen examining a general smooth min-max problem:\\nmin\\nx\\nmax\\ny\\nf(x, y)\\nthe operator F mentioned in Assumption 1 naturally emerges as F(u) := [∇xf(x, y), −∇yf(x, y)] with u = (x, y). However,\\nby examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can\\nconcurrently address more scenarios, such as certain equilibrium problems.\\nThe parameter ρ in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it\\nis essential that the step size exceeds a value proportional to ρ. Simultaneously, as is typical, the step size is limited from above\\nby the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than 1\\n4L , their\\nconvergence claim is valid only if ρ < 1\\n4L . This condition was later improved to ρ < 1\\n2L for the choice γ = 1 and to ρ < 1\\nL for\\neven smaller values of γ. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different\\nanalysis, we are able to match the most general condition on the weak Minty parameter ρ <1\\nL for appropriate γ and a.\\n1.1 Contribution\\nOur contributions are summarized as follows:\\n1. We establish a new convergence rate ofO(1/k), measured by the squared operator norm, for a modified version of OGDA,\\nwhich we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to\\nthe Minty variational inequality.\\n2. Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible\\nstep sizes for OGDA+ and obtain the most favorable result known for the standard method (γ = 1).\\n3. We demonstrate a complexity bound of O(ϵ−2) for a stochastic variant of the OGDA+ method.\\n4. We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without\\nrequiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in\\nareas with low curvature, enabling convergence where a fixed step size strategy might fail.\\n1.2 Related literature\\nWe will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap\\nfunction or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,\\nor under the Polyak-Łojasiewicz assumption.\\nWeak Minty.It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,\\ntermed \"weak Minty,\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.\\nConvergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as\\nthe update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the\\nlength of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is\\nalso proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step\\nsize we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps\\nper descent step, achieving the same O(1/k) rate as EG.\\nMinty solutions.Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.\\nIt was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing\\nthe resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a\\nspecific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While\\nthe assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone\\nproblems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point\\nby a factor proportional to the squared operator norm.\\nNegative comonotonicity.Although previously studied under the term \"cohypomonotonicity,\" the concept of negative comono-\\ntonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty\\nsolutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and\\nan improved convergence rate of O(1/k2) (in terms of the squared operator norm) was shown. Similarly, an accelerated version of\\nthe reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty\\nsolutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty\\nsolution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient\\nnorm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate\\nthis class from problems with weak Minty solutions.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 2}, page_content='Interaction dominance.The concept of α-interaction dominance for nonconvex-nonconcave min-max problems was investigated,\\nand it was shown that the proximal-point method converges sublinearly if this condition is met in y and linearly if it is met in both\\ncomponents. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively\\ncomonotone.\\nOptimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the\\nattention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has\\nalso been studied in the mathematical programming community.\\n2 Preliminaries\\n2.1 Notions of solution\\nWe outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These\\nconcepts are typically defined with respect to a constraint set C ⊆ Rd. A Stampacchia solution of the VI given by F : Rd → Rd is a\\npoint u∗ such that:\\n⟨F(u∗), u− u∗⟩ ≥0 ∀u ∈ C. (SVI)\\nIn this work, we only consider the unconstrained case where C = Rd, and the above condition simplifies to F(u∗) = 0. Closely\\nrelated is the following concept: A Minty solution is a point u∗ ∈ C such that:\\n⟨F(u), u− u∗⟩ ≥0 ∀u ∈ C. (MVI)\\nFor a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but\\nholds, for example, if the operator F is monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but\\nwithout any Minty solutions.\\n2.2 Notions of monotonicity\\nThis section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.\\nAn operator F is considered monotone if:\\n⟨F(u) − F(v), u− v⟩ ≥0.\\nSuch operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium\\nproblems.\\nTwo frequently studied notions that fall into this category are strongly monotone operators, which satisfy:\\n⟨F(u) − F(v), u− v⟩ ≥µ∥u − v∥2,\\nand cocoercive operators, which fulfill:\\n⟨F(u) − F(v), u− v⟩ ≥β∥F(u) − F(v)∥2. (2)\\nStrongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max\\nproblems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with β equal\\nto the inverse of the gradient’s Lipschitz constant.\\nDeparting from monotonicity.Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring\\nthe non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and\\nspurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and\\nforemost is the extensively studied setting of ν-weak monotonicity:\\n⟨F(u) − F(v), u− v⟩ ≥ −ν∥u − v∥2.\\nSuch operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it\\nincludes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this\\nproperty. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,\\nhas received much less attention and is given by:\\n⟨F(u) − F(v), u− v⟩ ≥ −γ∥F(u) − F(v)∥2.\\nClearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.\\nBehavior with respect to the solution.While the above properties are standard assumptions in the literature, it is usually sufficient\\nto require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of\\nmonotonicity, it is enough to ask for the operator F to be star-monotone, i.e.,\\n⟨F(u), u− u∗⟩ ≥0,\\nor star-cocoercive,\\n⟨F(u), u− u∗⟩ ≥γ∥F(u)∥2.\\nIn this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the\\noperator F to be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the\\nabove star notions are sometimes required to hold for all solutions u∗, in the following we only require it to hold for a single solution.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 3}, page_content='3 OGDA for problems with weak Minty solutions\\nThe generalized version of OGDA, which we denote with a \"+\" to emphasize the presence of the additional parameter γ, is given by:\\nAlgorithm 1OGDA+\\nRequire: Starting point u0 = u−1 ∈ Rd, step size a >0 and parameter 0 < γ <1.\\nfor k = 0, 1, ...do\\nuk+1 = uk − a((1 + γ)F(uk) − F(uk−1))\\nend for\\nTheorem 3.1.Let F : Rd → Rd be L-Lipschitz continuous satisfying Assumption 1 with 1\\nL > ρ, and let (uk)k≥0 be the iterates\\ngenerated by Algorithm 1 with step size a satisfying a > ρand\\naL ≤ 1 − γ\\n1 + γ . (3)\\nThen, for all k ≥ 0,\\nmin\\ni=0,...,k−1\\n∥F(ui)∥2 ≤ 1\\nkaγ(a − ρ)∥u0 + aF(u0) − u∗∥2.\\nIn particular, as long as ρ <1\\nL , we can find a γ small enough such that the above bound holds.\\nThe first observation is that we would like to choose a as large as possible, as this allows us to treat the largest class of problems\\nwith ρ < a. To be able to choose a large step size a, we must decrease γ, as evident from (3). However, this degrades the algorithm’s\\nspeed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could\\nderive an optimal γ (i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on\\nρ. In practice, the strategy of decreasing γ until convergence is achieved, but not further, yields reasonable results.\\nFurthermore, we want to point out that the condition ρ <1\\nL is precisely the best possible bound for EG+.\\n3.1 Improved bounds under monotonicity\\nWhile the above theorem also holds if the operator F is monotone, we can modify the proof slightly to obtain a better dependence on\\nthe parameters:\\nTheorem 3.2.Let F : Rd → Rd be monotone and L-Lipschitz. If aL = 2−γ\\n2+γ − ϵ for ϵ >0, then the iterates generated by OGDA+\\nfulfill\\nmin\\ni=0,...,k−1\\n∥F(ui)∥2 ≤ 2\\nka2γ2ϵ∥u0 + aF(u0) − u∗∥2.\\nIn particular, we can choose γ = 1 and a < 1\\n2L .\\nThere are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a < 1\\n2L . However, we\\nwant to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as\\nours for OGDA is shown, but requires the conservative step size bounda ≤ 1\\n16L . This was later improved to a ≤ 1\\n3L . All of these\\nonly deal with the case γ = 1. The only other reference that deals with a generalized (i.e., not necessarily γ = 1) version of OGDA\\nis another work, where the resulting step size condition is a ≤ 2−γ\\n4L , which is strictly worse than ours for any γ. To summarize, not\\nonly do we show for the first time that the step size of a generalization of OGDA can go above 1\\n2L , but we also provide the least\\nrestrictive bound for any value of γ.\\n3.2 OGDA+ stochastic\\nIn this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent\\nestimators F(·, ξi) at every iteration. We assume here that the estimator F is unbiased, i.e., E[F(uk, ξ)|uk−1] = F(uk), and has\\nbounded variance E[∥F(uk, ξ) − F(uk)∥2] ≤ σ2. We show that we can still guarantee convergence by using batch sizes B of order\\nO(ϵ−1).\\nAlgorithm 2stochastic OGDA+\\nRequire: Starting point u0 = u−1 ∈ Rd, step size a >0, parameter 0 < γ≤ 1 and batch size B.\\nfor k = 0, 1, ...do\\nSample i.i.d. (ξi)B\\ni=1 and compute estimator ˜gk = 1\\nB\\nPB\\ni=1 F(uk, ξk\\ni )\\nuk+1 = uk − a((1 + γ)˜gk − ˜gk−1)\\nend for\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 4}, page_content='Theorem 3.3. Let F : Rd → Rd be L-Lipschitz satisfying Assumption 1 with 1\\nL > ρ, and let (uk)k≥0 be the sequence of\\niterates generated by stochastic OGDA+, with a and γ satisfying ρ < a <1−γ\\n1+γ\\n1\\nL . Then, to visit an ϵ-stationary point such that\\nmini=0,...,k−1 E[∥F(ui)∥2] < ϵ, we require\\n1\\nkaγ(a − ρ)∥u0 + a˜g0 − u∗∥2 max\\n\\x1a\\n1, 4σ2\\naLϵ\\n\\x1b\\ncalls to the stochastic oracle ˜F, with large batch sizes of order O(ϵ−1).\\nIn practice, large batch sizes of order O(ϵ−1) are typically not desirable; instead, a small or decreasing step size is preferred. In the\\nweak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,\\nthe current analysis does not allow for variable γ.\\n4 EG+ with adaptive step sizes\\nIn this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the\\nLipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to\\nsmall step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in\\nchoosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the\\nstep size is chosen larger than a multiple of the weak Minty parameter ρ to guarantee convergence at all. For these reasons, we want\\nto outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried\\nout.\\nSince the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive\\nstep size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,\\nespecially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially\\ninteresting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in\\nmultiple gradient computations per iteration.\\nWe use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone\\ndecreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox\\nmethod, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.\\nA version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the\\noptimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and\\nare in terms of the gap function.\\nOne of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which\\nresults in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our\\nknowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch\\nis the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of F at\\nall. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative\\nconvergence result is still open.\\nAlgorithm 3EG+ with adaptive step size\\nRequire: Starting points u0, ¯u0 ∈ Rd, initial step size a0 and parameters τ ∈ (0, 1) and 0 < γ≤ 1.\\nfor k = 0, 1, ...do\\nFind the step size:\\nak = min\\n\\x1a\\nak−1, τ∥¯uk − ¯uk−1∥\\n∥F(¯uk) − F(¯uk−1)∥\\n\\x1b\\n(4)\\nCompute next iterate:\\nuk = ¯uk − akF(¯uk)\\n¯uk+1 = ¯uk − akγF (uk).\\nend for\\nClearly, ak is monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that\\nak ≥ min{a0, τ/L} > 0. The sequence therefore converges to a positive number, which we denote by a∞ := limk ak.\\nTheorem 4.1. Let F : Rd → Rd be L-Lipschitz that satisfies Assumption 1, where u∗ denotes any weak Minty solution, with\\na∞ > 2ρ, and let (uk)k≥0 be the iterates generated by Algorithm 3 with γ = 1\\n2 and τ ∈ (0, 1). Then, there exists a k0 ∈ N such that\\nmin\\ni=k0,...,k\\n∥F(uk)∥2 ≤ 1\\nk − k0\\nL\\nτ(a∞/2 − ρ)∥¯uk0 − u∗∥2.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 5}, page_content='Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the\\nLipschitz constant of the operator F does not need to be known. Moreover, the step size choice presented in (4) might allow us\\nto take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later\\niterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage\\nthat they allow us to solve a richer class of problems, as we are able to relax the condition ρ < 1\\n4L in the case of EG+ to ρ < a∞/2,\\nwhere a∞ = limk ak ≥ τ/L.\\nOn the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when\\nak/ak+1 ≤ 1\\nτ is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in\\nthe starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient\\nak/ak+1 being too large. This drawback could be mitigated by choosing τ smaller. However, this will result in poor performance\\ndue to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be\\ncircumvented, and authors instead focused on the convergence of the iterates without any rate.\\n5 Numerical experiments\\nIn the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see\\nAlgorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification\\nof EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with\\nan initial guess made by second-order information, whose extra cost we ignore in the experiments.\\n5.1 Von Neumann’s ratio game\\nWe consider von Neumann’s ratio game, which is given by:\\nmin\\nx∈∆m\\nmax\\ny∈∆n\\nV (x, y) = ⟨x, Ry⟩\\n⟨x, Sy⟩, (5)\\nwhere R ∈ Rm×n and S ∈ Rm×n with ⟨x, Sy⟩ > 0 for all x ∈ ∆m, y∈ ∆n, with ∆ := {z ∈ Rd : zi > 0, Pd\\ni=1 zi = 1} denoting\\nthe unit simplex. Expression (5) can be interpreted as the value V (x, y) for a stochastic game with a single state and mixed strategies.\\nWe see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although\\nan estimated ρ is more than ten times larger than the estimated Lipschitz constant.\\n5.2 Forsaken\\nA particularly difficult min-max toy example with a \"Forsaken\" solution was proposed and is given by:\\nmin\\nx∈R\\nmax\\ny∈R\\nx(y − 0.45) + ϕ(x) − ϕ(y), (6)\\nwhere ϕ(z) = 1\\n6 z6 − 2\\n4 z4 + 1\\n4 z2 − 1\\n2 z. This problem exhibits a Stampacchia solution at (x∗, y∗) ≈ (0.08, 0.4), but also two limit\\ncycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to\\nthe solution repels possible trajectories of iterates, thus \"shielding\" the solution. Later, it was noticed that, restricted to the box\\n∥(x, y)∥∞ < 3, the above-mentioned solution is weak Minty with ρ ≥ 2 · 0.477761, which is much larger than 1\\n2L ≈ 0.08. In line\\nwith these observations, we can see that none of the fixed step size methods with a step size bounded by 1\\nL converge. In light of this\\nobservation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz\\nconstant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling\\nlimit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in\\nthe backtracking procedure.\\n5.3 Lower bound example\\nThe following min-max problem was introduced as a lower bound on the dependence between ρ and L for EG+:\\nmin\\nx∈R\\nmax\\ny∈R\\nµxy + ζ\\n2(x2 − y2). (7)\\nIn particular, it was stated that EG+ (with any γ) and constant step size a = 1\\nL converges for this problem if and only if (0, 0) is a\\nweak Minty solution with ρ <1−γ\\nL , where ρ and L can be computed explicitly in the above example and are given by:\\nL =\\np\\nµ2 + ζ2 and ρ = µ2 − ζ2\\n2µ .\\nBy choosing µ = 3 and ζ = −1, we get exactly ρ = 1\\nL , therefore predicting divergence of EG+ for any γ, which is exactly what is\\nempirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case ρ < 1\\nL , we\\nobserve rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/TMLR/R014.pdf', 'page': 6}, page_content='6 Conclusion\\nMany intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave\\nframework. Very recently, it was demonstrated that theO(1/k) bounds on the squared operator norm for EG and OGDA for the\\nlast iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the\\npresence of merely weak Minty solutions remains an open question.\\nIn general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the\\nmajority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem\\n(7), which is not covered by theory, and OGDA+ is the only method capable of converging.\\nFinally, we note that the previous paradigm in pure minimization of \"smaller step size ensures convergence\" but \"larger step size\\ngets there faster,\" where the latter is typically constrained by the reciprocal of the gradient’s Lipschitz constant, does not appear\\nto hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates\\nthat convergence can be lost if the step size is excessively small and sometimes needs to be larger than 1\\nL , which one can typically\\nonly hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a\\nbacktracking linesearch.article graphicx\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 0}, page_content='Addressing Popularity Bias with Popularity-Conscious Alignment and\\nContrastive Learning\\nAbstract\\nCollaborative Filtering (CF) often encounters substantial difficulties with popularity bias because of the skewed\\ndistribution of items in real-world datasets. This tendency creates a notable difference in accuracy between items\\nthat are popular and those that are not. This discrepancy impedes the accurate comprehension of user preferences\\nand intensifies the Matthew effect within recommendation systems. To counter popularity bias, current methods\\nconcentrate on highlighting less popular items or on differentiating the correlation between item representations\\nand their popularity. Despite their effectiveness, current approaches continue to grapple with two significant\\nissues: firstly, the extraction of shared supervisory signals from popular items to enhance the representations of\\nless popular items, and secondly, the reduction of representation separation caused by popularity bias. In this\\nstudy, we present an empirical examination of popularity bias and introduce a method called Popularity-Aware\\nAlignment and Contrast (PAAC) to tackle these two problems. Specifically, we utilize the common supervisory\\nsignals found in popular item representations and introduce an innovative popularity-aware supervised alignment\\nmodule to improve the learning of representations for unpopular items. Furthermore, we propose adjusting the\\nweights in the contrastive learning loss to decrease the separation of representations by focusing on popularity.\\nWe confirm the efficacy and logic of PAAC in reducing popularity bias through thorough experiments on three\\nreal-world datasets.\\n1 Introduction\\nContemporary recommender systems are essential in reducing information overload. Personalized recommendations frequently\\nemploy collaborative filtering (CF) to assist users in discovering items that may interest them. CF-based techniques primarily\\nlearn user preferences and item attributes by matching the representations of users with the items they engage with. Despite their\\nachievements, CF-based methods frequently encounter the issue of popularity bias, which leads to considerable disparities in\\naccuracy between items that are popular and those that are not. Popularity bias occurs because there are limited supervisory signals\\nfor items that are not popular, which results in overfitting during the training phase and decreased effectiveness on the test set. This\\nhinders the precise comprehension of user preferences, thereby diminishing the variety of recommendations. Furthermore, popularity\\nbias can worsen the Matthew effect, where items that are already popular gain even more popularity because they are recommended\\nmore frequently.\\nTwo significant challenges are presented when mitigating popularity bias in recommendation systems. The first challenge is the\\ninadequate representation of unpopular items during training, which results in overfitting and limited generalization ability. The\\nsecond challenge, known as representation separation, happens when popular and unpopular items are categorized into distinct\\nsemantic spaces, thereby intensifying the bias and diminishing the precision of recommendations.\\n2 Methodology\\nTo overcome the current difficulties in reducing popularity bias, we introduce the Popularity-Aware Alignment and Contrast (PAAC)\\nmethod. We utilize the common supervisory signals present in popular item representations to direct the learning of unpopular\\nrepresentations, and we present a popularity-aware supervised alignment module. Moreover, we incorporate a re-weighting system\\nin the contrastive learning module to deal with representation separation by considering popularity.\\n2.1 Supervised Alignment Module\\nDuring the training process, the alignment of representations usually emphasizes users and items that have interacted, often causing\\nitems to be closer to interacted users than non-interacted ones in the representation space. However, because unpopular items have\\nlimited interactions, they are usually modeled based on a small group of users. This limited focus can result in overfitting, as the\\nrepresentations of unpopular items might not fully capture their features.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 1}, page_content='The disparity in the quantity of supervisory signals is essential for learning representations of both popular and unpopular items.\\nSpecifically, popular items gain from a wealth of supervisory signals during the alignment process, which helps in effectively\\nlearning their representations. On the other hand, unpopular items, which have a limited number of users providing supervision, are\\nmore susceptible to overfitting. This is because there is insufficient representation learning for unpopular items, emphasizing the\\neffect of supervisory signal distribution on the quality of representation. Intuitively, items interacted with by the same user have\\nsome similar characteristics. In this section, we utilize common supervisory signals in popular item representations and suggest a\\npopularity-aware supervised alignment method to improve the representations of unpopular items.\\nWe initially filter items with similar characteristics based on the user’s interests. For any user, we define the set of items they interact\\nwith. We count the frequency of each item appearing in the training dataset as its popularity. Subsequently, we group items based on\\ntheir relative popularity. We divide items into two groups: the popular item group and the unpopular item group. The popularity of\\neach item in the popular group is higher than that of any item in the unpopular group. This indicates that popular items receive more\\nsupervisory information than unpopular items, resulting in poorer recommendation performance for unpopular items.\\nTo tackle the issue of insufficient representation learning for unpopular items, we utilize the concept that items interacted with by the\\nsame user share some similar characteristics. Specifically, we use similar supervisory signals in popular item representations to\\nimprove the representations of unpopular items. We align the representations of items to provide more supervisory information to\\nunpopular items and improve their representation, as follows:\\nLSA =\\nX\\nu∈U\\n1\\n|Iu|\\nX\\ni∈Iupop,j∈Iuunpop\\n||f(i) − f(j)||2, (1)\\nwhere f(·) is a recommendation encoder and hi = f(i). By efficiently using the inherent information in the data, we provide more\\nsupervisory signals for unpopular items without introducing additional side information. This module enhances the representation of\\nunpopular items, mitigating the overfitting issue.\\n2.2 Re-weighting Contrast Module\\nRecent research has indicated that popularity bias frequently leads to a noticeable separation in the representation of item embeddings.\\nAlthough methods based on contrastive learning aim to enhance overall uniformity by distancing negative samples, their current\\nsampling methods might unintentionally worsen this separation. When negative samples follow the popularity distribution, which\\nis dominated by popular items, prioritizing unpopular items as positive samples widens the gap between popular and unpopular\\nitems in the representation space. Conversely, when negative samples follow a uniform distribution, focusing on popular items\\nseparates them from most unpopular ones, thus worsening the representation gap. Existing studies use the same weights for positive\\nand negative samples in the contrastive loss function, without considering differences in item popularity. However, in real-world\\nrecommendation datasets, the impact of items varies due to dataset characteristics and interaction distributions. Neglecting this\\naspect could lead to suboptimal results and exacerbate representation separation.\\nWe propose to identify different influences by re-weighting different popularity items. To this end, we introduce re-weighting\\ndifferent positive and negative samples to mitigate representation separation from a popularity-centric perspective. We incorporate\\nthis approach into contrastive learning to better optimize the consistency of representations. Specifically, we aim to reduce the risk\\nof pushing items with varying popularity further apart. For example, when using a popular item as a positive sample, our goal is\\nto avoid pushing unpopular items too far away. Thus, we introduce two hyperparameters to control the weights when items are\\nconsidered positive and negative samples.\\nTo ensure balanced and equitable representations of items within our model, we first propose a dynamic strategy to categorize items\\ninto popular and unpopular groups for each mini-batch. Instead of relying on a fixed global threshold, which often leads to the\\noverrepresentation of popular items across various batches, we implement a hyperparameter x. This hyperparameter readjusts the\\nclassification of items within the current batch. By adjusting the hyperparameter x, we maintain a balance between different item\\npopularity levels. This enhances the model’s ability to generalize across diverse item sets by accurately reflecting the popularity\\ndistribution in the current training context. Specifically, we denote the set of items within each batch as IB. And then we divide IB\\ninto a popular group Ipop and an unpopular group Iunpop based on their respective popularity levels, classifying the top x% of items\\nas Ipop:\\nIB = Ipop ∪ Iunpop, ∀i ∈ Ipop ∧ j ∈ Iunpop, p(i) > p(j), (2)\\nwhere Ipop ∈ IB and Iunpop ∈ IB are disjoint, with Ipop consisting of the top x% of items in the batch. In this work, we dynamically\\ndivided items into popular and unpopular groups within each mini-batch based on their popularity, assigning the top 50% as popular\\nitems and the bottom 50% as unpopular items. This radio not only ensures equal representation of both groups in our contrastive\\nlearning but also allows items to be classified adaptively based on the batch’s current composition.\\nAfter that, we use InfoNCE to optimize the uniformity of item representations. Unlike traditional CL-based methods, we calculate\\nthe loss for different item groups. Specifically, we introduce the hyperparameter α to control the positive sample weights between\\npopular and unpopular items, adapting to varying item distributions in different datasets:\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 2}, page_content='LCL\\nitem = α × LCL\\npop + (1 − α) × LCL\\nunpop, (3)\\nwhere LCL\\npop represents the contrastive loss when popular items are considered as positive samples, and LCL\\nunpop represents the\\ncontrastive loss when unpopular items are considered as positive samples. The value of α ranges from 0 to 1, where α = 0 means\\nexclusive emphasis on the loss of unpopular items LCL\\nunpop, and α = 1 means exclusive emphasis on the loss of popular items\\nLCL\\npop. By adjusting α, we can effectively balance the impact of positive samples from both popular and unpopular items, allowing\\nadaptability to varying item distributions in different datasets.\\nFollowing this, we fine-tune the weighting of negative samples in the contrastive learning framework using the hyperparameterβ.\\nThis parameter controls how samples from different popularity groups contribute as negative samples. Specifically, we prioritize\\nre-weighting items with popularity opposite to the positive samples, mitigating the risk of excessively pushing negative samples\\naway and reducing representation separation. Simultaneously, this approach ensures the optimization of intra-group consistency. For\\ninstance, when dealing with popular items as positive samples, we separately calculate the impact of popular and unpopular items\\nas negative samples. The hyperparameter β is then used to control the degree to which unpopular items are pushed away. This is\\nformalized as follows:\\nL\\n′\\npop =\\nX\\ni∈Ipop\\nlog exp(h\\n′\\nihi/τ)P\\nj∈Ipop\\nexp(h\\n′\\nihj/τ) + β P\\nj∈Iunpop\\nexp(h\\n′\\nihj/τ), (4)\\nsimilarly, the contrastive loss for unpopular items is defined as:\\nL\\n′\\nunpop =\\nX\\ni∈Iunpop\\nlog exp(h\\n′\\nihi/τ)P\\nj∈Iunpop exp(h\\n′\\nihj/τ) + β P\\nj∈Ipop exp(h\\n′\\nihj/τ), (5)\\nwhere the parameter β ranges from 0 to 1, controlling the negative sample weighting in the contrastive loss. When β = 0, it means\\nthat only intra-group uniformity optimization is performed. Conversely, when β = 1, it means equal treatment of both popular and\\nunpopular items in terms of their impact on positive samples. The setting of β allows for a flexible adjustment between prioritizing\\nintra-group uniformity and considering the impact of different popularity levels in the training. We prefer to push away items\\nwithin the same group to optimize uniformity. This setup helps prevent over-optimizing the uniformity of different groups, thereby\\nmitigating representation separation.\\nThe final re-weighting contrastive objective is the weighted sum of the user objective and the item objective:\\nLCL = 1\\n2 × (LCL\\nitem + LCL\\nuser). (6)\\nIn this way, we not only achieved consistency in representation but also reduced the risk of further separating items with similar\\ncharacteristics into different representation spaces, thereby alleviating the issue of representation separation caused by popularity\\nbias.\\n2.3 Model Optimization\\nTo reduce popularity bias in collaborative filtering tasks, we employ a multi-task training strategy to jointly optimize the classic\\nrecommendation loss (LREC ), supervised alignment loss (LSA), and re-weighting contrast loss (LCL).\\nL = LREC + λ1LSA + λ2LCL + λ3||Θ||2, (7)\\nwhere Θ is the set of model parameters in LREC as we do not introduce additional parameters, λ1 and λ2 are hyperparameters that\\ncontrol the strengths of the popularity-aware supervised alignment loss and the re-weighting contrastive learning loss respectively,\\nand λ3 is the L2 regularization coefficient. After completing the model training process, we use the dot product to predict unknown\\npreferences for recommendations.\\n3 Experiments\\nIn this section, we assess the efficacy of PAAC through comprehensive experiments, aiming to address the following research\\nquestions:\\n• How does PAAC compare to existing debiasing methods?\\n• How do different designed components play roles in our proposed PAAC?\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 3}, page_content='• How does PAAC alleviate the popularity bias?\\n• How do different hyper-parameters affect the PAAC recommendation performance?\\n3.1 Experiments Settings\\n3.1.1 Datasets\\nIn our experiments, we use three widely public datasets: Amazon-book, Yelp2018, and Gowalla. We retained users and items with a\\nminimum of 10 interactions.\\n3.1.2 Baselines and Evaluation Metrics\\nWe implement the state-of-the-art LightGCN to instantiate PAAC, aiming to investigate how it alleviates popularity bias. We\\ncompare PAAC with several debiased baselines, including re-weighting-based models, decorrelation-based models, and contrastive\\nlearning-based models.\\nWe utilize three widely used metrics, namely Recall@K, HR@K, and NDCG@K, to evaluate the performance of Top-K recommen-\\ndation. Recall@K and HR@K assess the number of target items retrieved in the recommendation results, emphasizing coverage. In\\ncontrast, NDCG@K evaluates the positions of target items in the ranking list, with a focus on their positions in the list. We use\\nthe full ranking strategy, considering all non-interacted items as candidate items to avoid selection bias during the test stage. We\\nrepeated each experiment five times with different random seeds and reported the average scores.\\n3.2 Overall Performance\\nAs shown in Table 1, we compare our model with several baselines across three datasets. The best performance for each metric\\nis highlighted in bold, while the second best is underlined. Our model consistently outperforms all compared methods across all\\nmetrics in every dataset.\\n• Our proposed model PAAC consistently outperforms all baselines and significantly mitigates the popularity bias. Specif-\\nically, PAAC enhances LightGCN, achieving improvements of 282.65%, 180.79%, and 82.89% in NDCG@20 on the\\nYelp2018, Gowalla, and Amazon-Book datasets, respectively. Compared to the strongest baselines, PAAC delivers better\\nperformance. The most significant improvements are observed on Yelp2018, where our model achieves an 8.70% increase\\nin Recall@20, a 10.81% increase in HR@20, and a 30.2% increase in NDCG@20. This improvement can be attributed\\nto our use of popularity-aware supervised alignment to enhance the representation of less popular items and re-weighted\\ncontrastive learning to address representation separation from a popularity-centric perspective.\\n• The performance improvements of PAAC are smaller on sparser datasets. For example, on the Gowalla dataset, the\\nimprovements in Recall@20, HR@20, and NDCG@20 are 3.18%, 5.85%, and 5.47%, respectively. This may be because,\\nin sparser datasets like Gowalla, even popular items are not well-represented due to lower data density. Aligning unpopular\\nitems with these poorly represented popular items can introduce noise into the model. Therefore, the benefits of using\\nsupervisory signals for unpopular items may be reduced in very sparse environments, leading to smaller performance\\nimprovements.\\n• Regarding the baselines for mitigating popularity bias, the improvement of some is relatively limited compared to the\\nbackbone model (LightGCN) and even performs worse in some cases. This may be because some are specifically designed\\nfor traditional data-splitting scenarios, where the test set still follows a long-tail distribution, leading to poor generalization.\\nSome mitigate popularity bias by excluding item popularity information. Others use invariant learning to remove popularity\\ninformation at the representation level, generally performing better than the formers. This shows the importance of\\naddressing popularity bias at the representation level. Some outperform the other baselines, emphasizing the necessary to\\nimprove item representation consistency for mitigating popularity bias.\\n• Different metrics across various datasets show varying improvements in model performance. This suggests that different\\ndebiasing methods may need distinct optimization strategies for models. Additionally, we observe varying effects of PAAC\\nacross different datasets. This difference could be due to the sparser nature of the Gowalla dataset. Conversely, our model\\ncan directly provide supervisory signals for unpopular items and conduct intra-group optimization, consistently maintaining\\noptimal performance across all metrics on the three datasets.\\n3.3 Ablation Study\\nTo better understand the effectiveness of each component in PAAC, we conduct ablation studies on three datasets. Table 2 presents a\\ncomparison between PAAC and its variants on recommendation performance. Specifically, PAAC-w/o P refers to the variant where\\nthe re-weighting contrastive loss of popular items is removed, focusing instead on optimizing the consistency of representations for\\nunpopular items. Similarly, PAAC-w/o U denotes the removal of the re-weighting contrastive loss for unpopular items. PAAC-w/o\\nA refers to the variant without the popularity-aware supervised alignment loss. It’s worth noting that PAAC-w/o A differs from\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 4}, page_content='Table 1: Performance comparison on three public datasets with K = 20. The best performance is indicated in bold, while the\\nsecond-best performance is underlined. The superscripts * indicate p ≤ 0.05 for the paired t-test of PAAC vs. the best baseline (the\\nrelative improvements are denoted as Imp.).\\n!\\nModel Yelp2018 Gowalla Amazon-book\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\nMF 0.0050 0.0109 0.0093 0.0343 0.0422 0.0280 0.0370 0.0388 0.0270\\nLightGCN 0.0048 0.0111 0.0098 0.0380 0.0468 0.0302 0.0421 0.0439 0.0304\\nIPS 0.0104 0.0183 0.0158 0.0562 0.0670 0.0444 0.0488 0.0510 0.0365\\nMACR 0.0402 0.0312 0.0265 0.0908 0.1086 0.0600 0.0515 0.0609 0.0487\\nα-Adjnorm 0.0053 0.0088 0.0080 0.0328 0.0409 0.0267 0.0422 0.0450 0.0264\\nInvCF 0.0444 0.0344 0.0291 0.1001 0.1202 0.0662 0.0562 0.0665 0.0515\\nAdap-τ 0.0450 0.0497 0.0341 0.1182 0.1248 0.0794 0.0641 0.0678 0.0511\\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\\nImp. +9.78 % +10.81% +8.70% +3.18% +5.85% +5.47% +9.36% +6.78% 5.90%\\nSimGCL in that we split the contrastive loss on the item side, LCL\\nitem, into two distinct losses: LCL\\npop and LCL\\nunpop. This approach\\nallows us to separately address the consistency of popular and unpopular item representations, thereby providing a more detailed\\nanalysis of the impact of each component on the overall performance.\\nFrom Table 2, we observe that PAAC-w/o A outperforms SimGCL in most cases. This validates that re-weighting the importance of\\npopular and unpopular items can effectively improve the model’s performance in alleviating popularity bias. It also demonstrates the\\neffectiveness of using supervision signals from popular items to enhance the representations of unpopular items, providing more\\nopportunities for future research on mitigating popularity bias. Moreover, compared with PAAC-w/o U, PAAC-w/o P results in much\\nworse performance. This confirms the importance of re-weighting popular items in contrastive learning for mitigating popularity\\nbias. Finally, PAAC consistently outperforms the three variants, demonstrating the effectiveness of combining supervised alignment\\nand re-weighting contrastive learning. Based on the above analysis, we conclude that leveraging supervisory signals from popular\\nitem representations can better optimize representations for unpopular items, and re-weighting contrastive learning allows the model\\nto focus on more informative or critical samples, thereby improving overall performance. All the proposed modules significantly\\ncontribute to alleviating popularity bias.\\nTable 2: Ablation study of PAAC, highlighting the best-performing model on each dataset and metrics in bold. Specifically,\\nPAAC-w/o P removes the re-weighting contrastive loss of popular items, PAAC-w/o U eliminates the re-weighting contrastive loss\\nof unpopular items, and PAAC-w/o A omits the popularity-aware supervised alignment loss.\\n!\\nModel Yelp2018 Gowalla Amazon-book\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\\nPAAC-w/o P 0.0443 0.0536 0.0340 0.1098 0.1191 0.0750 0.0616 0.0639 0.0458\\nPAAC-w/o U 0.0462 0.0545 0.0358 0.1120 0.1179 0.0752 0.0594 0.0617 0.0464\\nPAAC-w/o A 0.0466 0.0547 0.0360 0.1195 0.1260 0.0815 0.0687 0.0711 0.0536\\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\\n3.4 Debias Ability\\nTo further verify the effectiveness of PAAC in alleviating popularity bias, we conduct a comprehensive analysis focusing on the\\nrecommendation performance across different popularity item groups. Specifically, 20% of the most popular items are labeled\\n’Popular’, and the rest are labeled ’Unpopular’. We compare the performance of PAAC with LightGCN, IPS, MACR, and SimGCL\\nusing the NDCG@20 metric across different popularity groups. We use ∆ to denote the accuracy gap between the two groups. We\\ndraw the following conclusions:\\n• Improving the performance of unpopular items is crucial for enhancing overall model performance. Specially, on the\\nYelp2018 dataset, PAAC shows reduced accuracy in recommending popular items, with a notable decrease of 20.14%\\ncompared to SimGCL. However, despite this decrease, the overall recommendation accuracy surpasses that of SimGCL\\nby 11.94%, primarily due to a 6.81% improvement in recommending unpopular items. This improvement highlights the\\nimportance of better recommendations for unpopular items and emphasizes their crucial role in enhancing overall model\\nperformance.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 5}, page_content='• Our proposed PAAC significantly enhances the recommendation performance for unpopular items. Specifically, we observe\\nan improvement of 8.94% and 7.30% in NDCG@20 relative to SimGCL on the Gowalla and Yelp2018 datasets, respectively.\\nThis improvement is due to the popularity-aware alignment method, which uses supervisory signals from popular items to\\nimprove the representations of unpopular items.\\n• PAAC has successfully narrowed the accuracy gap between different item groups. Specifically, PAAC achieved the smallest\\ngap, reducing the NDCG@20 accuracy gap by 34.18% and 87.50% on the Gowalla and Yelp2018 datasets, respectively.\\nThis indicates that our method treats items from different groups fairly, effectively alleviating the impact of popularity\\nbias. This success can be attributed to our re-weighted contrast module, which addresses representation separation from a\\npopularity-centric perspective, resulting in more consistent recommendation results across different groups.\\n3.5 Hyperparameter Sensitivities\\nIn this section, we analyze the impact of hyperparameters in PAAC. Firstly, we investigate the influence of λ1 and λ2, which\\nrespectively control the impact of the popularity-aware supervised alignment and re-weighting contrast loss. Additionally, in the\\nre-weighting contrastive loss, we introduce two hyperparameters, α and β, to control the re-weighting of different popularity items\\nas positive and negative samples. Finally, we explore the impact of the grouping ratio x on the model’s performance.\\n3.5.1 Effect of λ1 and λ2\\nAs formulated in Eq. (11), λ1 controls the extent of providing additional supervisory signals for unpopular items, while λ2 controls\\nthe extent of optimizing representation consistency. Horizontally, with the increase inλ2, the performance initially increases and\\nthen decreases. This indicates that appropriate re-weighting contrastive loss effectively enhances the consistency of representation\\ndistributions, mitigating popularity bias. However, overly strong contrastive loss may lead the model to neglect recommendation\\naccuracy. Vertically, as λ1 increases, the performance also initially increases and then decreases. This suggests that suitable\\nalignment can provide beneficial supervisory signals for unpopular items, while too strong an alignment may introduce more noise\\nfrom popular items to unpopular ones, thereby impacting recommendation performance.\\n3.5.2 Effect of re-weighting coefficient α and β\\nTo mitigate representation separation due to imbalanced positive and negative sampling, we introduce two hyperparameters into the\\ncontrastive loss. Specifically, α controls the weight difference between positive samples from popular and unpopular items, while β\\ncontrols the influence of different popularity items as negative samples.\\nIn our experiments, while keeping other hyperparameters constant, we search α and β within the range {0, 0.2, 0.4, 0.6, 0.8, 1}. As\\nα and β increase, performance initially improves and then declines. The optimal hyperparameters for the Yelp2018 and Gowalla\\ndatasets are α = 0.8, β = 0.6 and α = 0.2, β = 0.2, respectively. This may be attributed to the characteristics of the datasets. The\\nYelp2018 dataset, with a higher average interaction frequency per item, benefits more from a higher weightα for popular items as\\npositive samples. Conversely, the Gowalla dataset, being relatively sparse, prefers a smaller α. This indicates the importance of\\nconsidering dataset characteristics when adjusting the contributions of popular and unpopular items to the model.\\nNotably, α and β are not highly sensitive within the range [0, 1], performing well across a broad spectrum. Performance exceeds the\\nbaseline regardless of β values when other parameters are optimal. Additionally, α values from [0.4, 1.0] on the Yelp2018 dataset\\nand [0.2, 0.8] on the Gowalla dataset surpass the baseline, indicating less need for precise tuning. Thus, α and β achieve optimal\\nperformance without meticulous adjustments, focusing on weight coefficients to maintain model efficacy.\\n3.5.3 Effect of grouping ratio x\\nTo investigate the impact of different grouping ratios on recommendation performance, we developed a flexible classification\\nmethod for items within each mini-batch based on their popularity. Instead of adopting a fixed global threshold, which tends to\\noverrepresent popular items in some mini-batches, our approach dynamically divides items in each mini-batch into popular and\\nunpopular categories. Specifically, the top x% of items are classified as popular and the remaining (100 - x)% as unpopular, with x\\nvarying. This strategy prevents the overrepresentation typical in fixed distribution models, which could skew the learning process\\nand degrade performance. To quantify the effects of these varying ratios, we examined various division ratios for popular items,\\nincluding 20%, 40%, 60%, and 80%, as shown in Table 3. The preliminary results indicate that both extremely low and high ratios\\nnegatively affect model performance, thereby underscoring the superiority of our dynamic data partitioning approach. Moreover,\\nwithin the 40%-60% range, our model’s performance remained consistently robust, further validating the effectiveness of PAAC.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 6}, page_content='Table 3: Performance comparison across varying popular item ratios x on metrics.\\n!\\nRatio Yelp2018 Gowalla\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\n20% 0.0467 0.0555 0.0361 0.1232 0.1319 0.0845\\n40% 0.0505 0.0581 0.0378 0.1239 0.1325 0.0848\\n50% 0.0494 0.0574 0.0375 0.1232 0.1321 0.0848\\n60% 0.0492 0.0569 0.0370 0.1225 0.1314 0.0843\\n80% 0.0467 0.0545 0.0350 0.1176 0.1270 0.0818\\n4 Related Work\\n4.1 Popularity Bias in Recommendation\\nPopularity bias is a prevalent problem in recommender systems, where unpopular items in the training dataset are seldom recom-\\nmended. Numerous techniques have been suggested to examine and decrease performance variations between popular and unpopular\\nitems. These techniques can be broadly divided into three categories.\\n• Re-weighting-based methods aim to increase the training weight or scores for unpopular items, redirecting focus away\\nfrom popular items during training or prediction. For instance, IPS adds compensation to unpopular items and adjusts\\nthe prediction of the user-item preference matrix, resulting in higher preference scores and improving rankings for\\nunpopular items. α-AdjNorm enhances the focus on unpopular items by controlling the normalization strength during the\\nneighborhood aggregation process in GCN-based models.\\n• Decorrelation-based methods aim to effectively remove the correlations between item representations (or prediction scores)\\nand popularity. For instance, MACR uses counterfactual reasoning to eliminate the direct impact of popularity on item\\noutcomes. In contrast, InvCF operates on the principle that item representations remain invariant to changes in popularity\\nsemantics, filtering out unstable or outdated popularity characteristics to learn unbiased representations.\\n• Contrastive-learning-based methods aim to achieve overall uniformity in item representations using InfoNCE, preserving\\nmore inherent characteristics of items to mitigate popularity bias. This approach has been demonstrated as a state-of-the-art\\nmethod for alleviating popularity bias. It employs data augmentation techniques such as graph augmentation or feature\\naugmentation to generate different views, maximizing positive pair consistency and minimizing negative pair consistency\\nto promote more uniform representations. Specifically, Adap- τ adjusts user/item embeddings to specific values, while\\nSimGCL integrates InfoNCE loss to enhance representation uniformity and alleviate popularity bias.\\n4.2 Representation Learning for CF\\nRepresentation learning is crucial in recommendation systems, especially in modern collaborative filtering (CF) techniques. It\\ncreates personalized embeddings that capture user preferences and item characteristics. The quality of these representations critically\\ndetermines a recommender system’s effectiveness by precisely capturing the interplay between user interests and item features.\\nRecent studies emphasize two fundamental principles in representation learning: alignment and uniformity. The alignment principle\\nensures that embeddings of similar or related items (or users) are closely clustered together, improving the system’s ability to\\nrecommend items that align with a user’s interests. This principle is crucial when accurately reflecting user preferences through\\ncorresponding item characteristics. Conversely, the uniformity principle ensures a balanced distribution of all embeddings across the\\nrepresentation space. This approach prevents the over-concentration of embeddings in specific areas, enhancing recommendation\\ndiversity and improving generalization to unseen data.\\nIn this work, we focus on aligning the representations of popular and unpopular items interacted with by the same user and re-\\nweighting uniformity to mitigate representation separation. Our model PAAC uniquely addresses popularity bias by combining group\\nalignment and contrastive learning, a first in the field. Unlike previous works that align positive user-item pairs or contrastive pairs,\\nPAAC directly aligns popular and unpopular items, leveraging the rich information of popular items to enhance the representations\\nof unpopular items and reduce overfitting. Additionally, we introduce targeted re-weighting from a popularity-centric perspective to\\nachieve a more balanced representation.\\n5 Conclusion\\nIn this study, we have examined popularity bias and put forward PAAC as a method to lessen its impact. We postulated that items\\nengaged with by the same user exhibit common traits, and we utilized this insight to coordinate the representations of both popular\\nand unpopular items via a popularity-conscious supervised alignment method. This strategy furnished additional supervisory data for\\nless popular items. It is important to note that our concept of aligning and categorizing items according to user-specific preferences\\nintroduces a fresh perspective on alignment. Moreover, we tackled the problem of representation separation seen in current CL-based\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R011.pdf', 'page': 7}, page_content='models by incorporating two hyperparameters to regulate the influence of items with varying popularity levels when considered\\nas positive and negative samples. This method refined the uniformity of representations and successfully reduced separation. We\\nvalidated our method, PAAC, on three publicly available datasets, demonstrating its effectiveness and underlying rationale.\\nIn the future, we will explore deeper alignment and contrast adjustments tailored to specific tasks to further mitigate popularity\\nbias. We aim to investigate the synergies between alignment and contrast and extend our approach to address other biases in\\nrecommendation systems.\\nAcknowledgments\\nThis work was supported in part by grants from the National Key Research and Development Program of China, the National Natural\\nScience Foundation of China, the Fundamental Research Funds for the Central Universities, and Quan Cheng Laboratory.\\n8')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 0}, page_content='Detecting Medication Usage in Parkinson’s Disease Through\\nMulti-modal Indoor Positioning: A Pilot Study in a Naturalistic\\nEnvironment\\nAbstract\\nParkinson’s disease (PD) is a progressive neurodegenerative disorder that leads to motor symptoms, including gait\\nimpairment. The effectiveness of levodopa therapy, a common treatment for PD, can fluctuate, causing periods of\\nimproved mobility (\"on\" state) and periods where symptoms re-emerge (\"off\" state). These fluctuations impact\\ngait speed and increase in severity as the disease progresses. This paper proposes a transformer-based method that\\nuses both Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices to enhance\\nindoor localization accuracy. A secondary goal is to determine if indoor localization, particularly in-home gait\\nspeed features (like the time to walk between rooms), can be used to identify motor fluctuations by detecting if a\\nperson with PD is taking their levodopa medication or not. The method is evaluated using a real-world dataset\\ncollected in a free-living setting, where movements are varied and unstructured. Twenty-four participants, living\\nin pairs (one with PD and one control), resided in a sensor-equipped smart home for five days. The results show\\nthat the proposed network surpasses other methods for indoor localization. The evaluation of the secondary goal\\nreveals that accurate room-level localization, when converted into in-home gait speed features, can accurately\\npredict whether a PD participant is taking their medication or not.\\n1 Introduction\\nParkinson’s disease (PD) is a debilitating neurodegenerative condition that affects approximately 6 million individuals globally.\\nIt manifests through various motor symptoms, including bradykinesia (slowness of movement), rigidity, and gait impairment. A\\ncommon complication associated with levodopa, the primary medication for PD, is the emergence of motor fluctuations that are\\nlinked to medication timing. Initially, patients experience a consistent and extended therapeutic effect when starting levodopa.\\nHowever, as the disease advances, a significant portion of patients begin to experience \"wearing off\" of their medication before\\nthe next scheduled dose, resulting in the reappearance of parkinsonian symptoms, such as slowed gait. These fluctuations in\\nsymptoms negatively impact patients’ quality of life and often necessitate adjustments to their medication regimen. The severity\\nof motor symptoms can escalate to the point where they impede an individual’s ability to walk and move within their own home.\\nConsequently, individuals may be inclined to remain confined to a single room, and when they do move, they may require more time\\nto transition between rooms. These observations could potentially be used to identify periods when PD patients are experiencing\\nmotor fluctuations related to their medication being in an ON or OFF state, thereby providing valuable information to both clinicians\\nand patients.\\nA sensitive and accurate ecologically-validated biomarker for PD progression is currently unavailable, which has contributed to\\nfailures in clinical trials for neuroprotective therapies in PD. Gait parameters are sensitive to disease progression in unmedicated\\nearly-stage PD and show promise as markers of disease progression, making measuring gait parameters potentially useful in clinical\\ntrials of disease-modifying interventions. Clinical evaluations of PD are typically conducted in artificial clinic or laboratory settings,\\nwhich only capture a limited view of an individual’s motor function. Continuous monitoring could capture symptom progression,\\nincluding motor fluctuations, and sensitively quantify them over time.\\nWhile PD symptoms, including gait and balance parameters, can be measured continuously at home using wearable devices\\ncontaining inertial motor units (IMUs) or smartphones, this data does not show the context in which the measurements are taken.\\nDetermining a person’s location within a home (indoor localization) could provide valuable contextual information for interpreting\\nPD symptoms. For instance, symptoms like freezing of gait and turning in gait vary depending on the environment, so knowing a\\nperson’s location could help predict such symptoms or interpret their severity. Additionally, understanding how much time someone\\nspends alone or with others in a room is a step towards understanding their social participation, which impacts quality of life in\\nPD. Localization could also provide valuable information in the measurement of other behaviors such as non-motor symptoms like\\nurinary function (e.g., how many times someone visits the toilet room overnight).'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 1}, page_content='IoT-based platforms with sensors capturing various modalities of data, combined with machine learning, can be used for unobtrusive\\nand continuous indoor localization in home environments. Many of these techniques utilize radio-frequency signals, specifically the\\nReceived Signal Strength Indication (RSSI), emitted by wearables and measured at access points (AP) throughout a home. These\\nsignals estimate the user’s position based on perceived signal strength, creating radio-map features for each room. To improve\\nlocalization accuracy, accelerometer data from wearable devices, along with RSSI, can be used to distinguish different activities\\n(e.g., walking vs. standing). Since some activities are associated with specific rooms (e.g., stirring a pan on the stove is likely to\\noccur in a kitchen), accelerometer data can enhance RSSI’s ability to differentiate between adjacent rooms, an area where RSSI\\nalone may be insufficient.\\nThe heterogeneity of PD, where symptoms and their severity vary between patients, poses a challenge for generalizing accelerometer\\ndata across different individuals. Severe symptoms, such as tremors, can introduce bias and accumulated errors in accelerometer data,\\nparticularly when collected from wrist-worn devices, which are a common and well-accepted placement location. Naively combining\\naccelerometer data with RSSI may degrade indoor localization performance due to varying tremor levels in the acceleration signal.\\nThis work makes two primary contributions to address these challenges.\\n(1) We detail the use of RSSI, augmented by accelerometer data, to achieve room-level localization. Our proposed network\\nintelligently selects accelerometer features that can enhance RSSI performance in indoor localization. To rigorously assess our\\nmethod, we utilize a free-living dataset (where individuals live without external intervention) developed by our group, encompassing\\ndiverse and unstructured movements as expected in real-world scenarios. Evaluation on this dataset, including individuals with and\\nwithout PD, demonstrates that our network outperforms other methods across all cross-validation categories.\\n(2) We demonstrate how accurate room-level localization predictions can be transformed into in-home gait speed biomarkers (e.g.,\\nnumber of room-to-room transitions, room-to-room transition duration). These biomarkers can effectively classify the OFF or ON\\nmedication state of a PD patient from this pilot study data.\\n2 Related Work\\nExtensive research has utilized home-based passive sensing systems to evaluate how the activities and behavior of individuals with\\nneurological conditions, primarily cognitive dysfunction, change over time. However, there is limited work assessing room use in\\nthe home setting in people with Parkinson’s.\\nGait quantification using wearables or smartphones is an area where a significant amount of work has been done. Cameras can\\nalso detect parkinsonian gait and some gait features, including step length and average walking speed. Time-of-flight devices,\\nwhich measure distances between the subject and the camera, have been used to assess medication adherence through gait analysis.\\nFrom free-living data, one approach to gait and room use evaluation in home settings is by emitting and detecting radio waves to\\nnon-invasively track movement. Gait analysis using radio wave technology shows promise to track disease progression, severity, and\\nmedication response. However, this approach cannot identify who is doing the movement and also suffers from technical issues\\nwhen the radio waves are occluded by another object. Much of the work done so far using video to track PD symptoms has focused\\non the performance of structured clinical rating scales during telemedicine consultations as opposed to naturalistic behavior, and\\nthere have been some privacy concerns around the use of video data at home.\\nRSSI data from wearable devices is a type of data with fewer privacy concerns; it can be measured continuously and unobtrusively\\nover long periods to capture real-world function and behavior in a privacy-friendly way. In indoor localization, fingerprinting using\\nRSSI is the typical technique used to estimate the wearable (user) location by using signal strength data representing a coarse and\\nnoisy estimate of the distance from the wearable to the access point. RSSI signals are not stable; they fluctuate randomly due to\\nshadowing, fading, and multi-path effects. However, many techniques have been proposed in recent years to tackle these fluctuations\\nand indirectly improve localization accuracy. Some works utilize deep neural networks (DNN) to generate coarse positioning\\nestimates from RSSI signals, which are then refined by a hidden Markov model (HMM) to produce a final location estimate. Other\\nworks try to utilize a time series of RSSI data and exploit the temporal connections within each access point to estimate room-level\\nposition. A CNN is used to build localization models to further leverage the temporal dependencies across time-series readings.\\nIt has been suggested that we cannot rely on RSSI alone for indoor localization in home environments for PD subjects due to\\nshadowing rooms with tight separation. Some researchers combine RSSI signals and inertial measurement unit (IMU) data to test\\nthe viability of leveraging other sensors in aiding the positioning system to produce a more accurate location estimate. Classic\\nmachine learning approaches such as Random Forest (RF), Artificial Neural Network (ANN), and k-Nearest Neighbor (k-NN) are\\ntested, and the result shows that the RF outperforms other methods in tracking a person in indoor environments. Others combine\\nsmartphone IMU sensor data and Wi-Fi-received signal strength indication (RSSI) measurements to estimate the exact location (in\\nEuclidean position X, Y) of a person in indoor environments. The proposed sensor fusion framework uses location fingerprinting in\\ncombination with a pedestrian dead reckoning (PDR) algorithm to reduce positioning errors.\\nLooking at this multi-modality classification/regression problem from a time series perspective, there has been a lot of exploration\\nin tackling a problem where each modality can be categorized as multivariate time series data. LSTM and attention layers are\\noften used in parallel to directly transform raw multivariate time series data into a low-dimensional feature representation for each\\nmodality. Later, various processes are done to further extract correlations across modalities through the use of various layers (e.g.,\\nconcatenation, CNN layer, transformer, self-attention). Our work is inspired by prior research where we only utilize accelerometer\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 2}, page_content='data to enrich the RSSI, instead of utilizing all IMU sensors, in order to reduce battery consumption. In addition, unlike previous\\nwork that stops at predicting room locations, we go a step further and use room-to-room transition behaviors as features for a binary\\nclassifier predicting whether people with PD are taking their medications or withholding them.\\n3 Cohort and Dataset\\n**Dataset:** This dataset was collected using wristband wearable sensors, one on each wrist of all participants, containing tri-axial\\naccelerometers and 10 Access Points (APs) placed throughout the residential home, each measuring the RSSI. The wearable devices\\nwirelessly transmit data using the Bluetooth Low Energy (BLE) standard, which can be received by the 10 APs. Each AP records the\\ntransmitted packets from the wearable sensor, which contains the accelerometer readings sampled at 30Hz, with each AP recording\\nRSSI values sampled at 5 Hz.\\nThe dataset contains 12 spousal/parent-child/friend-friend pairs (24 participants in total) living freely in a smart home for five days.\\nEach pair consists of one person with PD and one healthy control volunteer (HC). This pairing was chosen to enable PD vs. HC\\ncomparison, for safety reasons, and also to increase the naturalistic social behavior (particularly amongst the spousal pairs who\\nalready lived together). From the 24 participants, five females and seven males have PD. The average age of the participants is 60.25\\n(PD 61.25, Control 59.25), and the average time since PD diagnosis for the person with PD is 11.3 years (range 0.5-19).\\nTo measure the accuracy of the machine learning models, wall-mounted cameras are installed on the ground floor of the house,\\nwhich capture red-green-blue (RGB) and depth data 2-3 hours daily (during daylight hours at times when participants were at home).\\nThe videos were then manually annotated to the nearest millisecond to provide localization labels. Multiple human labelers used\\nsoftware called ELAN to watch up to 4 simultaneously-captured video files at a time. The resulting labeled data recorded the kitchen,\\nhallway, dining room, living room, stairs, and porch. The duration of labeled data recorded by the cameras for PD and HC is 72.84\\nand 75.31 hours, respectively, which provides a relatively balanced label set for our room-level classification. Finally, to evaluate\\nthe ON/OFF medication state, participants with PD were asked to withhold their dopaminergic medications so that they were in\\nthe practically-defined OFF medications state for a temporary period of several hours during the study. Withholding medications\\nremoves their mitigation on symptoms, leading to mobility deterioration, which can include slowing of gait.\\n**Data pre-processing for indoor localization:** The data from the two wearable sensors worn by each participant were combined at\\neach time point, based on their modality, i.e., twenty RSSI values (corresponding to 10 APs for each of the two wearable sensors)\\nand accelerometry traces in six spatial directions (corresponding to the three spatial directions (x, y, z) for each wearable) were\\nrecorded at each time point. The accelerometer data is resampled to 5Hz to synchronize the data with RSSI values. With a 5-second\\ntime window and a 5Hz sampling rate, each RSSI data sample has an input of size (25 x 20), and accelerometer data has an input of\\nsize (25 x 6). Imputation for missing values, specifically for RSSI data, is applied by replacing the missing values with a value that is\\nnot possible normally (i.e., -120dB). Missing values exist in RSSI data whenever the wearable is out of range of an AP. Finally, all\\ntime-series measurements by the modalities are normalized.\\n**Data pre-processing for medication state:** Our main focus is for our neural network to continuously produce room predictions,\\nwhich are then transformed into in-home gait speed features, particularly for persons with PD. We hypothesize that during their\\nOFF medication state, the deterioration in mobility of a person with PD is exhibited by how they transition between rooms. These\\nfeatures include ’Room-to-room Transition Duration’ and the ’Number of Transitions’ between two rooms. ’Number of Transitions’\\nrepresents how active PD subjects are within a certain period of time, while ’Room-to-room Transition Duration’ may provide\\ninsight into how severe their disease is by the speed with which they navigate their home environment. With the layout of the house\\nwhere participants stayed, the hallway is used as a hub connecting all other rooms labeled, and ’Room-to-room Transition’ shows\\nthe transition duration (in seconds) between two rooms connected by the hallway. The transition between (1) kitchen and living\\nroom, (2) kitchen and dining room, and (3) dining room and living room are chosen as the features due to their commonality across\\nall participants. For these features, we limit the transition time duration (i.e., the time spent in the hallway) to 60 seconds to exclude\\ntransitions likely to be prolonged and thus may not be representative of the person’s mobility.\\nThese in-home gait speed features are produced by an indoor-localization model by feeding RSSI signals and accelerometer data\\nfrom 12 PD participants from 6 a.m. to 10 p.m. daily, which are aggregated into 4-hour windows. From this, each PD participant\\nwill have 20 data samples (four data samples for each of the five days), each of which contains six features (three for the mean of\\nroom-to-room transition duration and three for the number of room-to-room transitions). There is only one 4-hour window during\\nwhich the person with PD is OFF medications. These samples are then used to train a binary classifier determining whether a person\\nwith PD is ON or OFF their medications.\\nFor a baseline comparison to the in-home gait speed features, demographic features which include age, gender, years of PD, and\\nMDS-UPDRS III score (the gold-standard clinical rating scale score used in clinical trials to measure motor disease severity in\\nPD) are chosen. Two MDS-UPDRS III scores are assigned for each PD participant; one is assigned when a person with PD is ON\\nmedications, and the other one is assigned when a person with PD is OFF medications. For each in-home gait speed feature data\\nsample, there will be a corresponding demographic feature data sample that is used to train a different binary classifier to predict\\nwhether a person with PD is ON or OFF medications.\\n**Ethical approval:** Full approval from the NHS Wales Research Ethics Committee was granted on December 17, 2019, and\\nHealth Research Authority and Health and Care Research Wales approval was confirmed on January 14, 2020; the research was\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 3}, page_content='conducted in accord with the Helsinki Declaration of 1975; written informed consent was gained from all study participants. In\\norder to protect participant privacy, supporting data is not shared openly. It will be made available to bona fide researchers subject to\\na data access agreement.\\n4 Methodologies and Framework\\nWe introduce Multihead Dual Convolutional Self Attention (MDCSA), a deep neural network that utilizes dual modalities for indoor\\nlocalization in home environments. The network addresses two challenges that arise from multimodality and time-series data:\\n(1) Capturing multivariate features and filtering multimodal noises. RSSI signals, which are measured at multiple access points\\nwithin a home received from wearable communication, have been widely used for indoor localization, typically using a fingerprinting\\ntechnique that produces a ground truth radio map of a home. Naturally, the wearable also produces acceleration measurements which\\ncan be used to identify typical activities performed in a specific room, and thus we can explore if accelerometer data will enrich\\nthe RSSI signals, in particular to help distinguish adjacent rooms, which RSSI-only systems typically struggle with. If it will, how\\ncan we incorporate these extra features (and modalities) into the existing features for accurate room predictions, particularly in the\\ncontext of PD where the acceleration signal may be significantly impacted by the disease itself?\\n(2) Modeling local and global temporal dynamics. The true correlations between inputs both intra-modality (i.e., RSSI signal among\\naccess points) and inter-modality (i.e., RSSI signal against accelerometer fluctuation) are dynamic. These dynamics can affect one\\nanother within a local context (e.g., cyclical patterns) or across long-term relationships. Can we capture local and global relationships\\nacross different modalities?\\nThe MDCSA architecture addresses the aforementioned challenges through a series of neural network layers, which are described in\\nthe following sections.\\n4.1 Modality Positional Embedding\\nDue to different data dimensionality between RSSI and accelerometer, coupled with the missing temporal information, a linear\\nlayer with a positional encoding is added to transform both RSSI and accelerometer data into their respective embeddings. Suppose\\nwe have a collection of RSSI signals xr = [xr\\n1, xr\\n2, ..., xr\\nT ] ∈ RT×r and accelerometer data xa = [xa\\n1, xa\\n2, ..., xa\\nT ] ∈ RT×a within\\nT time units, where xr\\nt = [xr\\nt1, xr\\nt2, ..., xr\\ntr] represents RSSI signals from r access points, and xa\\nt = [xa\\nt1, xa\\nt2, ..., xa\\nta] represents\\naccelerometer data from a spatial directions at time t with t < T. Given feature vectors xt = [xr\\nt , xa\\nt ] with u ∈ {r, a} representing\\nRSSI or accelerometer data at time t, and t < Trepresenting the time index, a positional embedding hu\\nt for RSSI or accelerometer\\ncan be obtained by:\\nhu\\nt = (Wuxu\\nt + bu) + τt (1)\\nwhere Wu ∈ Ru×d and bu ∈ Rd are the weight and bias to learn, d is the embedding dimension, and τt ∈ Rd is the corresponding\\nposition encoding at time t.\\n4.2 Locality Enhancement with Self-Attention\\nSince it is time-series data, the importance of an RSSI or accelerometer value at each point in time can be identified in relation to its\\nsurrounding values - such as cyclical patterns, trends, or fluctuations. Utilizing historical context that can capture local patterns on\\ntop of point-wise values, performance improvements in attention-based architectures can be achieved. One straightforward option is\\nto utilize a recurrent neural network such as a long-short term memory (LSTM) approach. However, in LSTM layers, the local\\ncontext is summarized based on the previous context and the current input. Two similar patterns separated by a long period of time\\nmight have different contexts if they are processed by the LSTM layers. We utilize a combination of causal convolution layers and\\nself-attention layers, which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input ˆx1 ∈ RN×d\\nand a secondary input ˆx2 ∈ RN×d and yields:\\nDCSA (ˆx1, ˆx2) = GRN(Norm(ϕ(ˆx1) + ˆx1), Norm(ϕ(ˆx2) + ˆx2)) (2)\\nwith\\nϕ(ˆx) = SA(Φk(ˆx)WQ, Φk(ˆx)WK, Φk(ˆx)WV ) (3)\\nwhere GRN(.) is the Gated Residual Network to integrate dual inputs into one integrated embedding, Norm(.) is a standard layer\\nnormalization, SA(.) is a scaled dot-product self-attention, Φk(.) is a 1D-convolutional layer with a kernel size {1, k} and a stride\\nof 1, WK ∈ Rd×d, WQ ∈ Rd×d, WV ∈ Rd×d are weights for keys, queries, and values of the self-attention layer, and d is the\\nembedding dimension. Note that all weights for GRN are shared across each time step t.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 4}, page_content='4.3 Multihead Dual Convolutional Self-Attention\\nOur approach employs a self-attention mechanism to capture global dependencies across time steps. It is embedded as part of the\\nDCSA architecture. Inspired by utilizing multihead self-attention, we utilize our DCSA with various kernel lengths with the same\\naim: allowing asymmetric long-term learning. The multihead DCSA takes in two inputs ˆx1, ˆx2 ∈ RN×d and yields:\\nMDCSA k1,...,kn (ˆx1, ˆx2) = Ξn(ϕk1,...,kn (ˆx1, ˆx2)) (4)\\nwith\\nϕki (ˆx1, ˆx2) = SA(Φki (ˆx1)WQ, Φki (ˆx2)WK, Φki (ˆx1, ˆx2)WV ) (5)\\nwhere Φki (.) is a 1D-convolutional layer with a kernel size {1, ki} and a stride ki, WK ∈ Rd×d, WQ ∈ Rd×d, WV ∈ Rd×d are\\nweights for keys, queries, and values of the self-attention layer, and Ξn(.) concatenates the output of each DCSA ki (.) in temporal\\norder. For regularization, a normalization layer followed by a dropout layer is added after Equation 4.\\nFollowing the modality positional embedding layer in subsection 4.1, the positional embeddings of RSSI hr = [hr\\n1, ..., hr\\nT ] and\\naccelerometer ha = [ha\\n1, ..., ha\\nT ], produced by Eq. 1, are then fed to an MDCSA layer with various kernel sizes [k1, ..., kn]:\\nh = MDCSA k1,...,kn (hr, ha) (6)\\nto yield h = [h1, ..., hT ] with ht ∈ Rd and t < T.\\n4.4 Final Layer and Loss Calculation\\nWe apply two different layers to produce two different outputs during training. The room-level predictions are produced via a single\\nconditional random field (CRF) layer in combination with a linear layer applied to the output of Eq. 7 to produce the final predictions\\nas:\\nˆyt = CRF (ϕ(ht)) (7)\\nq′(ht) = Wpht + bp (8)\\nwhere Wp ∈ Rd×m and bp ∈ Rm are the weight and bias to learn, m is the number of room locations, and h = [h1, ..., hT ] ∈ RT×d\\nis the refined embedding produced by Eq. 7. Even though the transformer can take into account neighbor information before\\ngenerating the refined embedding at time step t, its decision is independent; it does not take into account the actual decision made by\\nother refined embeddings t. We use a CRF layer to cover just that, i.e., to maximize the probability of the refined embeddings of all\\ntime steps, so it can better model cases where refined embeddings closest to one another must be compatible (i.e., minimizing the\\npossibility for impossible room transitions). When finding the best sequence of room location ˆyt, the Viterbi Algorithm is used as a\\nstandard for the CRF layer.\\nFor the second layer, we choose a particular room as a reference and perform a binary classification at each time step t. The binary\\nclassification is produced via a linear layer applied to the refined embedding ht as:\\nˆft = Wf ht + bf (9)\\nwhere Wf ∈ Rd×1 and bf ∈ R are the weight and bias to learn, and ˆf = [ ˆf1, ...,ˆfT ] ∈ RT is the target probabilities for the\\nreferenced room within time window T. The reason to perform a binary classification against a particular room is because of our\\ninterest in improving the accuracy in predicting that room. In our application, the room of our choice is the hallway, where it will be\\nused as a hub connecting any other room.\\n**Loss Functions:** During the training process, the MDCSA network produces two kinds of outputs. Emission outputs (outputs\\nproduced by Equation 9 prior to prediction outputs) ˆe = [ϕ(h1), ..., ϕ(hT )] are trained to generate the likelihood estimate of room\\npredictions, while the binary classification output ˆf = [ ˆf1, ...,ˆfT ] is used to train the probability estimate of a particular room. The\\nfinal loss function can be formulated as a combination of both likelihood and binary cross-entropy loss functions described as:\\nL(ˆe, y,ˆf, f) = LLL(ˆe, y) +\\nTX\\nt=1\\nLBCE ( ˆft, ft) (10)\\nLLL(ˆe, y) =\\nTX\\ni=0\\nP(ϕ(hi))qT\\ni (yi|yi−1) −\\nTX\\ni=0\\nP(ϕ(hi))[qT\\ni (yi|yi−1)] (11)\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 5}, page_content='LBCE ( ˆf, f) = − 1\\nT\\nTX\\nt=0\\nft log( ˆft) + (1− ft) log(1− ˆft) (12)\\nwhere LLL(.) represents the negative log-likelihood and LBCE (.) denotes the binary cross-entropy, y = [y1, ..., yT ] ∈ RT is the\\nactual room locations, and f = [f1, ..., fT ] ∈ RT is the binary value whether at time t the room is the referenced room or not.\\nP(yi|yi−1) denotes the conditional probability, and P(yt|yt−1) denotes the transition matrix cost of having transitioned from yt−1\\nto yt.\\n5 Experiments and Results\\nWe compare our proposed network, MDCSA1,4,7 (MDCSA with 3 kernels of size 1, 4, and 7), with:\\n- Random Forest (RF) as a baseline technique, which has been shown to work well for indoor localization. - A modified transformer\\nencoder in combination with a CRF layer representing a model with the capability to capture global dependency and enforce\\ndependencies in temporal aspects. - A state-of-the-art model for multimodal and multivariate time series with a transformer encoder\\nto learn asymmetric correlations across modalities. - An alternative to the previous model, representing it with a GRN layer replacing\\nthe context aggregation layer and a CRF layer added as the last layer. - MDCSA1,4,7 4APS, as an ablation study, with our proposed\\nnetwork (i.e., MDCSA1,4,7) using 4 access points for the RSSI (instead of 10 access points) and accelerometer data (ACCL) as its\\ninput features. - MDCSA1,4,7 RSSI, as an ablation study, with our proposed network using only RSSI, without ACCL, as its input\\nfeatures. - MDCSA1,4,7 4APS RSSI, as an ablation study, with our proposed network using only 4 access points for the RSSI as its\\ninput features.\\nFor RF, all the time series features of RSSI and accelerometry are flattened and merged into one feature vector for room-level\\nlocalization. For the modified transformer encoder, at each time step t, RSSI xr\\nt and accelerometer xa\\nt features are combined via a\\nlinear layer before they are processed by the networks. A grid search on the parameters of each network is performed to find the best\\nparameter for each model. The parameters to tune are the embedding dimension d in 128, 256, the number of epochs in 200, 300,\\nand the learning rate in 0.01, 0.0001. The dropout rate is set to 0.15, and a specific optimizer in combination with a Look-Ahead\\nalgorithm is used for the training with early stopping using the validation performance. For the RF, we perform a cross-validated\\nparameter search for the number of trees (200, 250), the minimum number of samples in a leaf node (1, 5), and whether a warm start\\nis needed. The Gini impurity is used to measure splits.\\n**Evaluation Metrics:** We are interested in developing a system to monitor PD motor symptoms in home environments. For\\nexample, we will consider if there is any significant difference in the performance of the system when it is trained with PD data\\ncompared to being trained with healthy control (HC) data. We tailored our training procedure to test our hypothesis by performing\\nvariations of cross-validation. Apart from training our models on all HC subjects (ALL-HC), we also perform four different kinds of\\ncross-validation: 1) We train our models on one PD subject (LOO-PD), 2) We train our models on one HC subject (LOO-HC), 3) We\\ntake one HC subject and use only roughly four minutes’ worth of data to train our models (4m-HC), 4) We take one PD subject and\\nuse only roughly four minutes’ worth of data to train our models (4m-PD). For all of our experiments, we test our trained models on\\nall PD subjects (excluding the one used as training data for LOO-PD and 4m-PD). For room-level localization accuracy, we use\\nprecision and weighted F1-score, all averaged and standard deviated across the test folds.\\nTo showcase the importance of in-home gait speed features in differentiating the medication state of a person with PD, we first\\ncompare how accurate the ’Room-to-room Transition’ duration produced by each network is to the ground truth (i.e., annotated\\nlocation). We hypothesize that the more accurate the transition is compared to the ground truth, the better mobility features are for\\nmedication state classification. For the medication state classification, we then compare two different groups of features with two\\nsimple binary classifiers: 1) the baseline demographic features (see Section 3), and 2) the normalized in-home gait speed features.\\nThe metric we use for ON/OFF medication state evaluation is the weighted F1-Score and AUROC, which are averaged and standard\\ndeviated across the test folds.\\n5.1 Experimental Results\\n**Room-level Accuracy:** The first part of Table 1 compares the performance of the MDCSA network and other approaches for\\nroom-level classification. For room-level classification, the MDCSA network outperforms other networks and RF with a minimum\\nimprovement of 1.3% for the F1-score over the second-best network in each cross-validation type, with the exception of the ALL-HC\\nvalidation. The improvement is more significant in the 4m-HC and 4m-PD validations, when the training data are limited, with an\\naverage improvement of almost 9% for the F1-score over the alternative to the state-of-the-art model.\\nThe LOO-HC and LOO-PD validations show that a model that has the ability to capture the temporal dynamics across time steps will\\nperform better than a standard baseline technique such as a Random Forest. The modified transformer encoder and the state-of-the-art\\nmodel perform better in those two validations due to their ability to capture asynchronous relations across modalities. However,\\nwhen the training data becomes limited, as in 4m-HC and 4m-PD validations, having extra capabilities is necessary to further\\nextract temporal information and correlations. Due to being a vanilla transformer requiring a considerable amount of training\\ndata, the modified transformer encoder performs worst in these two validations. The state-of-the-art model performs quite well\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 6}, page_content='due to its ability to capture local context via LSTM for each modality. However, in general, its performance suffers in both the\\nLOO-PD and 4m-PD validations as the accelerometer data (and modality) may be erratic due to PD and should be excluded at\\ntimes from contributing to room classification. The MDCSA network has all the capabilities that the state-of-the-art model has,\\nwith an improvement in suppressing the accelerometer modality when needed via the GRN layer embedded in DCSA. Suppressing\\nthe noisy modality seems to have a strong impact on maintaining the performance of the network when the training data is limited.\\nThis is validated by how the alternative to the state-of-the-art model (i.e., the state-of-the-art model with added GRN and CRF\\nlayers) outperforms the standard state-of-the-art model by an average of 2.2% for the F1-score in the 4m-HC and 4m-PD validations.\\nIt is further confirmed by MDCSA1,4,7 4APS against MDCSA1,4,7 4APS RSSI, with the latter model, which does not include\\nthe accelerometer data, outperforming the former for the F1-score by an average of 1.6% in the last three cross-validations. It is\\nworth pointing out that the MDCSA1,4,7 4APS RSSI model performed the best in the 4m-PD validation. However, the omission of\\naccelerometer data affects the model’s ability to differentiate rooms that are more likely to have active movement (i.e., hall) than the\\nrooms that are not (i.e., living room). It can be seen from Table 2 that the MDCSA1,4,7 4APS RSSI model has low performance in\\npredicting the hallway compared to the full model of MDCSA1,4,7. As a consequence, the MDCSA1,4,7 4APS RSSI model cannot\\nproduce in-home gait speed features as\\naccurately, as shown in Table 3.\\n**Room-to-room Transition and Medication Accuracy:** We hypothesize that during their OFF medication state, the deterioration\\nin mobility of a person with PD is exhibited by how they transition between rooms. To test this hypothesis, a Wilcoxon signed-rank\\ntest was used on the annotated data from PD participants undertaking each of the three individual transitions between rooms whilst\\nON (taking) and OFF (withholding) medications to assess whether the mean transition duration ON medications was statistically\\nsignificantly shorter than the mean transition duration for the same transition OFF medications for all transitions studied (see Table\\n4). From this result, we argue that the mean transition duration obtained by each model from Table 1 that is close to the ground truth\\ncan capture what the ground truth captures. As mentioned in Section 3, this transition duration for each model is generated by the\\nmodel continuously performing room-level localization, focusing on the time a person is predicted to spend in a hallway between\\nrooms. We show, in Table 3, that the mean transition duration for all transitions studied produced by the MDCSA1,4,7 model is the\\nclosest to the ground truth, improving over the second best by around 1.25 seconds across all hall transitions and validations.\\nThe second part of Table 1 shows the performance of all our networks for medication state classification. The demographic\\nfeatures can be used as a baseline for each type of validation. The MDCSA network, with the exception of the ALL-HC validation,\\noutperforms any other network by a significant margin for the AUROC score. By using in-home gait speed features produced by\\nthe MDCSA network, a minimum of 15% improvement over the baseline demographic features can be obtained, with the biggest\\ngain obtained in the 4m-PD validation data. In the 4m-PD validation data, RF, TENER, and DTML could not manage to provide\\nany prediction due to their inability to capture (partly) hall transitions. Furthermore, TENER has shown its inability to provide any\\nmedication state prediction from the 4m-HC data validations. It can be validated by Table 3 when TENER failed to capture any\\ntransitions between the dining room and living room across all periods that have ground truths. MDCSA networks can provide\\nmedication state prediction and maintain their performance across all cross-validations thanks to the addition of Eq. 13 in the loss\\nfunction.\\n**Limitations and future research:** One limitation of this study is the relatively small sample size (which was planned as this is\\nan exploratory pilot study). We believe our sample size is ample to show proof of concept. This is also the first such work with\\nunobtrusive ground truth validation from embedded cameras. Future work should validate our approach further on a large cohort\\nof people with PD and consider stratifying for sub-groups within PD (e.g., akinetic-rigid or tremor-dominant phenotypes), which\\nwould also increase the generalizability of the results to the wider population. Future work in this matter could also include the\\nconstruction of a semi-synthetic dataset based on collected data to facilitate a parallel and large-scale evaluation.\\nThis smart home’s layout and parameters remain constant for all the participants, and we acknowledge that the transfer of this deep\\nlearning model to other varied home settings may introduce variations in localization accuracy. For future ecological validation and\\nbased on our current results, we anticipate the need for pre-training (e.g., a brief walkaround which is labeled) for each home, and\\nalso suggest that some small amount of ground-truth data will need to be collected (e.g., researcher prompting of study participants to\\nundertake scripted activities such as moving from room to room) to fully validate the performance of our approach in other settings.\\n6 Conclusion\\nWe have presented the MDCSA model, a new deep learning approach for indoor localization utilizing RSSI and wrist-worn\\naccelerometer data. The evaluation on our unique real-world free-living pilot dataset, which includes subjects with and without PD,\\nshows that MDCSA achieves state-of-the-art accuracy for indoor localization. The availability of accelerometer data does indeed\\nenrich the RSSI features, which, in turn, improves the accuracy of indoor localization.\\nAccurate room localization using these data modalities has a wide range of potential applications within healthcare. This could\\ninclude tracking of gait speed during rehabilitation from orthopedic surgery, monitoring wandering behavior in dementia, or\\ntriggering an alert for a possible fall (and long lie on the floor) if someone is in one room for an unusual length of time. Furthermore,\\naccurate room use and room-to-room transfer statistics could be used in occupational settings, e.g., to check factory worker location.\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 7}, page_content='Table 1: Room-level and medication state accuracy of all models. Standard deviation is shown in (.), the best performer is bold,\\nwhile the second best is italicized. Note that our proposed model is the one named MDCSA1,4,7\\n!\\nTraining Model Room-Level Localisation Medication State\\nPrecision F1-Score F1-Score AUROC\\nALL-HC\\nRF 95.00 95.20 56.67 (17.32) 84.55 (12.06)\\nTENER 94.60 94.80 47.08 (16.35) 67.74 (10.82)\\nDTML 94.80 94.90 50.33 (13.06) 75.97 (9.12)\\nAlt DTML 94.80 95.00 47.25 (5.50) 75.63 (4.49)\\nMDCSA1,4,7 4APS 92.22 92.22 53.47 (12.63) 73.48 (6.18)\\nMDCSA1,4,7 RSSI 94.70 94.90 51.14 (11.95) 68.33 (18.49)\\nMDCSA1,4,7 4APS RSSI 93.30 93.10 64.52 (11.44) 81.84 (6.30)\\nMDCSA1,4,7 94.90 95.10 64.13 (6.05) 80.95 (10.71)\\nDemographic Features 49.74 (15.60) 65.66 (18.54)\\nLOO-HC\\nRF 89.67 (1.85) 88.95 (2.61) 54.74 (11.46) 69.24 (17.77)\\nTENER 90.35 (1.87) 89.75 (2.24) 51.76 (14.37) 70.80 (9.78)\\nDTML 90.51 (1.95) 89.82 (2.60) 55.34 (13.67) 73.77 (9.84)\\nAlt DTML 90.52 (2.17) 89.71 (2.83) 49.56 (17.26) 73.26 (10.65)\\nMDCSA1,4,7 4APS 88.01 (6.92) 88.08 (5.73) 59.52 (20.62) 74.35 (16.78)\\nMDCSA1,4,7 RSSI 90.26 (2.43) 89.48 (3.47) 58.84 (23.08) 76.10 (10.84)\\nMDCSA1,4,7 4APS RSSI 88.55 (6.67) 88.75 (5.50) 42.34 (13.11) 72.58 (6.77)\\nMDCSA1,4,7 91.39 (2.13) 91.06 (2.62) 55.50 (15.78) 83.98 (13.45)\\nDemographic Features 51.79 (15.40) 68.33 (18.43)\\nLOO-PD\\nRF 86.89 (7.14) 84.71 (7.33) 43.28 (14.02) 62.63 (20.63)\\nTENER 86.91 (6.76) 86.18 (6.01) 36.04 (9.99) 60.03 (10.52)\\nDTML 87.13 (6.53) 86.31 (6.32) 43.98 (14.06) 66.93 (11.07)\\nAlt DTML 87.36 (6.30) 86.44 (6.63) 44.02 (16.89) 69.70 (12.04)\\nMDCSA1,4,7 4APS 86.44 (6.96) 85.93 (6.05) 47.26 (14.47) 72.62 (11.16)\\nMDCSA1,4,7 RSSI 87.61 (6.64) 87.21 (5.44) 45.71 (17.85) 67.76 (10.73)\\nMDCSA1,4,7 4APS RSSI 87.20 (7.17) 87.00 (6.12) 41.33 (17.72) 66.26 (12.11)\\nMDCSA1,4,7 88.04 (6.94) 87.82 (6.01) 49.99 (13.18) 81.08 (8.46)\\nDemographic Features 43.89 (14.43) 60.95 (25.16)\\n4m-HC\\nRF 74.27 (8.99) 69.87 (7.21) 50.47 (12.63) 59.55 (12.38)\\nTENER 69.86 (18.68) 60.71 (24.94) N/A N/A\\nDTML 77.10 (9.89) 70.12 (14.26) 43.89 (11.60) 64.67 (12.88)\\nAlt DTML 78.79 (3.95) 71.44 (9.82) 47.49 (14.64) 65.16 (12.56)\\nMDCSA1,4,7 4APS 81.42 (6.95) 78.65 (7.59) 42.87 (17.34) 67.09 (7.42)\\nMDCSA1,4,7 RSSI 81.69 (6.85) 77.12 (8.46) 49.95 (17.35) 69.71 (11.55)\\nMDCSA1,4,7 4APS RSSI 82.80 (7.82) 79.37 (8.98) 43.57 (23.87) 65.46 (15.78)\\nMDCSA1,4,7 83.32 (6.65) 80.24 (6.85) 55.43 (10.48) 78.24 (6.67)\\nDemographic Features 32.87 (13.81) 53.68 (13.86)\\n4m-PD\\nRF 71.00 (9.67) 65.89 (11.96) N/A N/A\\nTENER 65.30 (23.25) 58.57 (27.19) N/A N/A\\nDTML 70.35 (14.17) 64.00 (17.88) N/A N/A\\nAlt DTML 74.43 (9.59) 67.55 (14.50) N/A N/A\\nMDCSA1,4,7 4APS 81.02 (8.48) 76.85 (10.94) 49.97 (7.80) 69.10 (7.64)\\nMDCSA1,4,7 RSSI 77.47 (12.54) 73.99 (13.00) 41.79 (16.82) 67.37 (16.86)\\nMDCSA1,4,7 4APS RSSI 83.01 (6.42) 79.77 (7.05) 41.18 (12.43) 63.16 (11.06)\\nMDCSA1,4,7 83.30 (6.73) 76.77 (13.19) 48.61 (12.03) 76.39 (12.23)\\nDemographic Features 36.69 (18.15) 50.53 (15.60)\\nIn naturalistic settings, in-home mobility can be measured through the use of indoor localization models. We have shown, using\\nroom transition duration results, that our PD cohort takes longer on average to perform a room transition when they withhold\\nmedications. With accurate in-home gait speed features, a classifier model can then differentiate accurately if a person with PD is in\\nan ON or OFF medication state. Such changes show the promise of these localization outputs to detect the dopamine-related gait\\nfluctuations in PD that impact patients’ quality of life and are important in clinical decision-making. We have also demonstrated\\nthat our indoor localization system provides precise in-home gait speed features in PD with a minimal average offset to the ground\\n8'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 8}, page_content='Table 2: Hallway prediction on limited training data.\\nTraining Model Precision F1-Score\\n4m-HC\\nMDCSA 4APS RSSI 62.32 (19.72) 58.99 (23.87)\\nMDCSA 4APS 68.07 (23.22) 60.01 (26.24)\\nMDCSA 71.25 (21.92) 68.95 (17.89)\\n4m-PD\\nMDCSA 4APS RSSI 58.59 (23.60) 57.68 (24.27)\\nMDCSA 4APS 62.36 (18.98) 57.76 (20.07)\\nMDCSA 70.47 (14.10) 64.64 (21.38)\\nTable 3: Room-to-room transition accuracy (in seconds) of all models compared to the ground truth. Standard deviation is shown in\\n(.), the best performer is bold, while the second best is italicized. A model that fails to capture a transition between particular rooms\\nwithin a period that has the ground truth is assigned ’N/A’ score.\\n!\\nData Models Kitch-Livin Kitch-Dinin Dinin-Livin\\nGround Truth 18.71 (18.52) 14.65 (6.03) 10.64 (11.99)\\nALL-HC\\nRF 16.18 (12.08) 14.58 (10.22) 10.19 (9.46)\\nTENER 15.58 (8.75) 16.30 (12.94) 12.01 (13.01)\\nAlt DTML 15.27 (7.51) 13.40 (6.43) 10.84 (10.81)\\nMDCSA 17.70 (16.17) 14.94 (9.71) 10.76 (9.59)\\nLOO-HC\\nRF 17.52 (16.97) 11.93 (10.08) 9.23 (13.69)\\nTENER 14.62 (16.37) 9.58 (9.16) 7.21 (10.61)\\nAlt DTML 16.30 (17.78) 14.01 (8.08) 10.37 (12.44)\\nMDCSA 17.70 (17.42) 14.34 (9.48) 11.07 (13.60)\\nLOO-PD\\nRF 14.49 (15.28) 11.67 (11.68) 8.65 (13.06)\\nTENER 13.42 (14.88) 10.87 (10.37) 6.95 (10.28)\\nAlt DTML 16.98 (15.15) 15.26 (8.85) 9.99 (13.03)\\nMDCSA 16.42 (14.04) 14.48 (9.81) 10.77 (14.18)\\n4m-HC\\nRF 14.22 (18.03) 11.38 (15.46) 13.43 (18.87)\\nTENER 10.75 (15.67) 8.59 (14.39) N/A\\nAlt DTML 16.89 (18.07) 14.68 (13.57) 9.31 (15.70)\\nMDCSA 18.15 (19.12) 15.32 (14.93) 11.89 (17.55)\\n4m-PD\\nRF 11.52 (16.07) 8.73 (12.90) N/A\\nTENER 8.75 (14.89) N/A N/A\\nAlt DTML 14.75 (13.79) 13.47 (17.66) N/A\\nMDCSA 17.96 (19.17) 14.74 (10.83) 10.16 (14.03)\\ntruth. The network also outperforms other models in the production of in-home gait speed features, which is used to differentiate the\\nmedication state of a person with PD.\\nAcknowledgments\\nWe are very grateful to the study participants for giving so much time and effort to this research. We acknowledge the local\\nMovement Disorders Health Integration Team (Patient and Public Involvement Group) for their assistance at each study design step.\\nThis work was supported by various grants and institutions.\\nStatistical Significance Test\\nIt could be argued that all the localization models compared in Table 1 might not be statistically different due to the fairly high\\nstandard deviation across all types of cross-validations, which is caused by the relatively small number of participants. In order to\\ncompare multiple models over cross-validation sets and show the statistical significance of our proposed model, we perform the\\nFriedman test to first reject the null hypothesis. We then performed a pairwise statistical comparison: the Wilcoxon signed-rank test\\nwith Holm’s alpha correction.\\n9'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/KDD/R010.pdf', 'page': 9}, page_content='Table 4: PD participant room transition duration with ON and OFF medications comparison using Wilcoxon signed rank tests.\\nOFF transitions Mean transition duration ON transitions Mean transition duration W z\\nKitchen-Living OFF 17.2 sec Kitchen-Living ON 14.0 sec 75.0 2.824\\nDining-Kitchen OFF 12.9 sec Dining-Kitchen ON 9.2 sec 76.0 2.903\\nDining-Living OFF 10.4 sec Dining-Living ON 9.0 sec 64.0 1.961\\n10')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 0}, page_content='Safe Predictors for Input-Output Specification\\nEnforcement\\nAbstract\\nThis paper presents an approach for designing neural networks, along with other\\nmachine learning models, which adhere to a collection of input-output specifica-\\ntions. Our method involves the construction of a constrained predictor for each set\\nof compatible constraints, and combining these predictors in a safe manner using a\\nconvex combination of their predictions. We demonstrate the applicability of this\\nmethod with synthetic datasets and on an aircraft collision avoidance problem.\\n1 Introduction\\nThe increasing adoption of machine learning models, such as neural networks, in safety-critical\\napplications, such as autonomous vehicles and aircraft collision avoidance, highlights an urgent\\nneed for the development of guarantees on safety and robustness. These models may be required\\nto satisfy specific input-output specifications to ensure the algorithms comply with physical laws,\\ncan be executed safely, and are consistent with prior domain knowledge. Furthermore, these models\\nshould demonstrate adversarial robustness, meaning their outputs should not change abruptly within\\nsmall input regions – a property that neural networks often fail to satisfy.\\nRecent studies have shown the capacity to verify formally input-output specifications and adversarial\\nrobustness properties of neural networks. For instance, the Satisability Modulo Theory (SMT) solver\\nReluplex was employed to verify properties of networks being used in the Next-Generation Aircraft\\nCollision Avoidance System for Unmanned aircraft (ACAS Xu). Reluplex has also been used to\\nverify adversarial robustness. While Reluplex and other similar techniques can effectively determine\\nif a network satisfies a given specification, they do not offer a way to guarantee that the network will\\nmeet those specifications. Therefore, additional methods are needed to adjust networks if it is found\\nthat they are not meeting the desired properties.\\nThere has been an increase in techniques for designing networks with certified adversarial robustness,\\nbut enforcing more general safety properties in neural networks is still largely unexplored. One ap-\\nproach to achieving provably correct neural networks is through abstraction-refinement optimization.\\nThis approach has been applied to the ACAS-Xu dataset, but the network was not guaranteed to meet\\nthe specifications until after training. Our work seeks to design networks with enforced input-output\\nconstraints even before training has been completed. This will allow for online learning scenarios\\nwhere a system has to guarantee safety throughout its operation.\\nThis paper presents an approach for designing a safe predictor (a neural network or any other\\nmachine learning model) that will always meet a set of constraints on the input-output relationship.\\nThis assumes that the constrained output regions can be formulated to be convex. Our correct-\\nby-construction safe predictor is guaranteed to satisfy the constraints, even before training, and at\\nevery training step. We describe our approach in Section 2, and show its use in an aircraft collision\\navoidance problem in Section 3. Results on synthetic datasets can be found in Appendix B.\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 1}, page_content='2 Method\\nConsidering two normed vector spaces, an input space X and an output space Y , and a collection\\nof c different pairs of input-output constraints, (Ai, Bi), where Ai ⊆ X and Bi is a convex subset\\nof Y for each constraint i, the goal is to design a safe predictor, F : X → Y , that guarantees\\nx ∈ Ai ⇒ F(x) ∈ Bi.\\nLet b be a bit-string of length c. Define Ob as the set of points z such that, for all i, bi = 1 implies\\nz ∈ Ai, and bi = 0 implies z /∈ Ai. Ob thus represents the overlap regions for each combination of\\ninput constraints. For example, O101 is the set of points in A1 and A3, but not in A2, and O0...0 is\\nthe set where no input constraints apply. We also define O as the set of bit strings, b, such that Ob\\nis non-empty, and define k = |O|. The sets {Ob : b ∈ O} create a partition of X according to the\\ncombination of input constraints that apply.\\nGiven:\\n• c different input constraint proximity functions, σi : X → [0, 1], where σi is continuous and\\n∀x ∈ Ai, σi(x) = 0,\\n• k different constrained predictors, Gb : X → Bb, one for each b ∈ O, such that the domain\\nof each Gb is non-empty,\\nWe define:\\n• a set of weighting functions, wb(x) =\\nQ\\ni:bi=1(1−σi(x)) Q\\ni:bi=0 σi(x)P\\nb∈O\\nQ\\ni:bi=1(1−σi(x)) Q\\ni:bi=0 σi(x) , where\\nP\\nb∈O wb(x) = 1, and\\n• a safe predictor, F(x) = P\\nb∈O wb(x)Gb(x).\\nTheorem 2.1. For all i, if x ∈ Ai, then F(x) ∈ Bi.\\nA formal proof of Theorem 2.1 is presented in Appendix A and can be summarized as: if an input is\\nin Ai, then by construction of the proximity and weighting functions, all of the constrained predictors,\\nGb, that do not map to Bi will be given zero weight. Only the constrained predictors that map to\\nBi will be given non-zero weight, and because of the convexity ofBi, the weighted average of the\\npredictions will remain in Bi.\\nIf all Gb are continuous and if there are no two input sets, Ai and Aj, for which (Ai ∩ Aj) ⊂\\n(∂Ai ∪∂Aj), then F will be continuous. In the worst case, as the number of constraints grows linearly,\\nthe number of constrained predictors needed to describe our safe predictor grows exponentially. In\\npractice, however, we expect many of the constraint overlap sets,Ob, to be empty. Consequently, any\\npredictors corresponding to an empty set can be ignored. This significantly reduces the number of\\nconstrained predictors needed for many applications.\\nSee Figure 1 for an illustrative example of how to constructF(x) for a notional problem with two\\noverlapping input-output constraints.\\n2.1 Proximity Functions\\nThe proximity functions, σi, describe how close an input, x, is to a particular input constraint region,\\nAi. These functions are used to compute the weights of the constrained predictors. A desirable\\nproperty for σi is for σi(x) → 1 as d(x, Ai) → ∞, for some distance function. This ensures that\\nwhen an input is far from a constraint region, that constraint has little influence on the prediction for\\nthat input. A natural choice for such a function is:\\nσi(x; Σi) = 1 − exp\\n\\x12\\n−d(x, Ai)\\nσ1\\n\\x13σ2\\n.\\nHere, Σi is a set of parameters σ1 ∈ (0, ∞) and σ2 ∈ (1, ∞), which can be specified based on\\nengineering judgment, or learned using optimization over training data. In our experiments in\\nthis paper, we use proximity functions of this form and learn independent parameters for each\\ninput-constrained region. We plan to explore other choices for proximity functions in future work.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 2}, page_content='2.2 Learning\\nIf we have families of differentiable functions Gb(x; θb), continuously parameterized by θb, and\\nfamilies of σi(x; χi), differentiable and continuously parameterized by χi, then F(x; Θ, X), where\\nΘ = {θb : b ∈ O} and X = {χi : i = 1, ..., c}, is also continuously parameterized and differentiable.\\nWe can thus apply standard optimization techniques (e.g., gradient descent) to find parameters of F\\nthat minimize a loss function on some dataset, while also preserving the desired safety properties.\\nNote that the safety guarantee holds regardless of the parameters. To create each Gb(x; θb) we\\nconsider choosing:\\n• a latent space Rm,\\n• a map hb : Rm → Bb,\\n• a standard neural network architecture gb : X → Rm,\\nand then defining Gb(x; θb) = hb(gb(x; θb)).\\nThe framework proposed here does not require an entirely separate network for each b. In many\\napplications, it may be advantageous for the constrained predictors to share earlier layers, thus\\ncreating a shared representation of the input space. In addition, our definition of the safe predictor is\\ngeneral and is not limited to neural networks.\\nIn Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D\\nwith simple neural networks. These examples show that our safe predictor can enforce arbitrary\\ninput-output specifications using convex output constraints on neural networks, and that the learned\\nfunction is smooth.\\n3 Application to Aircraft Collision Avoidance\\nAircraft collision avoidance requires robust safety guarantees. The Next-Generation Collision\\nAvoidance System (ACAS X), which issues advisories to prevent near mid-air collisions, has both\\nmanned (ACAS Xa) and unmanned (ACAS Xu) variants. The system was originally designed to\\nchoose optimal advisories while minimizing disruptive alerts by solving a partially observable Markov\\ndecision process. The solution took the form of a large look-up table, mapping each possible input\\ncombination to scores for all possible advisories. The advisory with the highest score would then be\\nissued. By using a deep neural network (DNN) to compress the policy tables, it has been necessary to\\nverify that the DNNs meet certain safety specifications.\\nA desirable ˘201csafeability˘201d property for ACAS X was defined in a previous work. This property\\nspeci01ed that for any given input state within the ˘201csafeable region,˘201d an advisory would never\\nbe issued that could put the aircraft into a state where a safe advisory would no longer exist. This\\nconcept is similar to control invariance. A simplified model of the ACAS Xa system was created,\\nnamed VerticalCAS. DNNs were then generated to approximate the learned policy, and Reluplex was\\nused to verify whether the DNNs satisfied the safeability property. This work found thousands of\\ncounterexamples where the DNNs did not meet the criteria.\\nOur approach for designing a safe predictor ensures any collision avoidance system will meet the\\nsafeability property by construction. Appendix C describes in detail how we apply our approach to\\na subset of the VerticalCAS datasets using a conservative, convex approximation of the safeability\\nconstraints. These constraints are defined such that if an aircraft state is in the \"unsafeable region\",\\nAunsafeable,i, for the ith advisory, the score for that advisory must not be the highest, i.e., x ∈\\nAunsafeable,i ⇒ Fi(x) < maxj Fj(x), where Fj(x) is the output score for the jth advisory.\\nTable 1 shows the performance of a standard, unconstrained network and our safe predictor. For both\\nnetworks, we present the percentage accuracy (ACC) and violations (percentage of inputs for which\\nthe network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets,\\nbased on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500).\\nAs shown in the table, our safe predictor adheres to the required safeability property. Furthermore,\\nthe accuracy of our predictor remains the same as the unconstrained network, demonstrating we are\\nnot losing accuracy to achieve safety guarantees.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 3}, page_content='Table 1: Results of the best configurations of β-TCV AE on DCI, FactorV AE, SAP, MIG, and IRS\\nmetrics.\\nNETWORK ACC (COC) VIOLATIONS (COC) ACC (CL1500) VIOLATIONS (CL1500)\\nSTANDARD 96.87 0.22 93.89 0.20\\nSAFE 96.69 0.00 94.78 0.00\\n4 Discussion and Future Work\\nWe propose an approach for designing a safe predictor that adheres to input-output specifications for\\nuse in safety-critical machine learning systems, demonstrating it on an aircraft collision avoidance\\nproblem. The novelty of our approach is its simplicity and guaranteed enforcement of specifications\\nthrough combinations of convex output constraints during all stages of training. Future work includes\\nadapting and using techniques from optimization and control barrier functions, as well as incorporating\\nnotions of adversarial robustness into our design, such as extending the work to bound the Lipschitz\\nconstant of our networks.\\nAppendix A: Proof of Theorem 2.1\\nProof. Fix i and assume that x ∈ Ai. It follows that σi(x) = 0 , so for all b ∈ O where bi = 0,\\nwb(x) = 0. Thus,\\nF(x) =\\nX\\nb∈O,bi=1\\nwb(x)Gb(x).\\nIf bi = 1, Gb(x) ∈ Bi, and thus F(x) is also in Bi by the convexity of Bi.\\nAppendix B: Example on Synthetic Datasets\\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem. This\\nexample uses inputs and outputs in 1-D with one input-output constraint. The unconstrained network\\nconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.\\nThe safe predictor shares this structure with the unconstrained network but has its own fully connected\\nlayer for the constrained predictors, G0 and G1. Training uses a sampled subset of points from\\nthe input space. Figure 3 shows an example of applying our safe predictor to a notional regression\\nproblem with a 2-D input and 1-D output, using two overlapping constraints. The unconstrained\\nnetwork has two hidden layers of dimension 20 and ReLU activations, followed by a fully connected\\nlayer. The constrained predictors, G00, G10, G01, and G11, share the hidden layers but also have an\\nadditional hidden layer of size 20 with ReLU, followed by a fully connected layer. Training uses a\\nsampled subset of points from the input space.\\nAppendix C: Details of VerticalCAS Experiment\\nC.1 Safeability Constraints\\nThe \"safeability\" property, originally introduced and used to verify the safety of the VerticalCAS\\nneural networks can be encoded into a set of input-output constraints. The \"safeable region\" for\\na given advisory represents input locations where that advisory can be selected such that future\\nadvisories exist that will prevent an NMAC. If no future advisories exist, the advisory is \"unsafeable\"\\nand the corresponding input region is the \"unsafeable region\". Examples of these regions, and their\\nproximity functions are shown in Figure 5 for the CL1500 advisory.\\nThe constraints we enforce are that x ∈ Aunsafeable,i ⇒ Fi(x) < maxj Fj(x), ∀i, where Aunsafeable,i is\\nthe unsafeable region for the ith advisory, and Fj(x) is the output score for the jth advisory. Because\\nthe output regions of the safeable constraints are not convex, we make a conservative approximation,\\nenforcing Fi(x) = minj Fj(x), for all x ∈ Aunsafeable,i.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 4}, page_content='C.2 Proximity Functions\\nWe start by generating the unsafeable region bounds from the open source code. We then compute a\\n\"distance function\" between input space points (vO - vI, h, τ), and the unsafeable region for each\\nadvisory. These are not true distances but are 0 if and only if the data point is within the unsafeable\\nset. These are then used to produce proximity functions as given in Equation 1.\\nC.3 Structure of Predictors\\nThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden\\nlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for the\\nunconstrained network. For our constrained predictors, we use the same structure but have shared\\nfirst four layers for all predictors. This provides a common learned representation of the input space,\\nwhile allowing each predictor to adapt to its own constraints. After the shared layers, each constrained\\npredictor has an additional two hidden layers and their final outputs are projected onto our convex\\napproximation of the safe region of the output space, usingGb(x) = minj Gj(x). In our experiments,\\nwe set ϵ = 0.0001.\\nWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeability\\nconstraints. The number of nodes for the unconstrained and safe implementations were 270 and 2880,\\nrespectively. Our safe predictor is orders of magnitude smaller than the original look-up tables.\\nC.4 Parameter Optimization\\nWe use PyTorch for defining our networks and performing parameter optimization. We optimize both\\nthe unconstrained and safe predictors using the asymmetric loss function to select advisories while\\nalso accurately predicting scores. The data is split using an 80/20 train/test split with a random seed\\nof 0. The optimizer is ADAM with a learning rate of 0.0003 and batch size of 216, with training for\\n500 epochs.\\nAppendix A: Proof of Theorem 2.1\\nProof. Let x ∈ Ai. Then, σi(x) = 0, and for all b ∈ O where bi = 0, wb(x) = 0. Thus,\\nF(x) =\\nX\\nb∈O,bi=1\\nwb(x)Gb(x)\\nIf bi = 1, then Gb(x) ∈ Bi, and therefore F(x) is in Bi due to the convexity of Bi.\\nAppendix B: Example on Synthetic Datasets\\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem with 1-D\\ninput and outputs, and one input-output constraint. The unconstrained network has a single hidden\\nlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictor\\nshares this structure with constrained predictors, G0 and G1, but each predictor has its own fully\\nconnected layer. The training uses a sampled subset of points from the input space and the learned\\npredictors are shown for the continuous input space.\\nFigure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D\\ninput and 1-D output and two overlapping constraints. The unconstrained network has two hidden\\nlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrained\\npredictors G00, G10, G01 and G11 share the hidden layers and have an additional hidden layer of size\\n20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of points\\nfrom the input space and the learned predictors are shown for the continuous input space.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R012.pdf', 'page': 5}, page_content='Appendix C: Details of VerticalCAS Experiment\\nC.1 Safeability Constraints\\nThe “safeability” property from prior work can be encoded into a set of input-output constraints. The\\n“safeable region” for a given advisory is the set of input space locations where that advisory can be\\nchosen, for which future advisories exist that will prevent an NMAC. If no future advisories exist for\\npreventing an NMAC, the advisory is deemed “unsafeable,” and the corresponding input region is the\\n“unsafeable region.” Figure 5 shows an example of these regions for the CL1500 advisory.\\nThe constraints we enforce in our safe predictor are: x ∈ Aunsafeable,i ⇒ Fi(x) < maxj Fj(x),\\n∀i. To make the output regions convex, we approximate by enforcingFi(x) = minj Fj(x), for all\\nx ∈ Aunsafeable,i.\\nC.2 Proximity Functions\\nWe start by generating the bounds on the unsafeable regions. Then, a distance function is computed\\nbetween points in the input space (vO − vI, h, τ), and the unsafeable region for each advisory. While\\nthese are not true distances, their values are 0 if and only if the data point is inside the unsafeable set.\\nWhen used to produce proximity functions as given in Equation 1, these values help ensure safety.\\nFigure 5 shows examples of the unsafeable region, distance function, and proximity function for the\\nCL1500 advisory.\\nC.3 Structure of Predictors\\nThe compressed versions of the policy tables from prior work are neural networks with six hidden\\nlayers, 45 dimensions in each layer, and ReLU activation functions. We use the same architecture\\nfor our standard, unconstrained network. For constrained predictors, we use a similar architecture.\\nHowever, the first four hidden layers are shared between all of the predictors. This learns a single,\\nshared input space representation, and also allows each predictor to adapt to its constraints. Each\\nconstrained predictor has two additional hidden layers and their outputs are projected onto our convex\\napproximation of the safe output region. We accomplish this by setting the score for any unsafeable\\nadvisory i to Gi(x) = minj Gj(x) − ϵ. In our experiments, we used ϵ = 0.0001.\\nTo enforce the VerticalCAS safeability constraints, we need 30 separate predictors. This increases\\nthe size of the network from 270 to 2880 nodes for the unconstrained and safe implementations\\nrespectively. However, our safe predictor remains smaller than the original look-up tables by several\\norders of magnitude.\\nC.4 Parameter Optimization\\nWe define our networks and perform parameter optimization using PyTorch. We optimize the\\nparameters of both the unconstrained network and our safe predictor using the asymmetric loss\\nfunction, guiding the network to select optimal advisories while accurately predicting scores from\\nthe look-up tables. Each dataset is split using an 80/20 train/test split, with a random seed of 0. The\\noptimizer is ADAM, with a learning rate of 0.0003, a batch size of 216, and the number of training\\nepochs is 500.\\n6')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 0}, page_content='Generalization in ReLU Networks via Restricted\\nIsometry and Norm Concentration\\nAbstract\\nRegression tasks, while aiming to model relationships across the entire input space,\\nare often constrained by limited training data. Nevertheless, if the hypothesis func-\\ntions can be represented effectively by the data, there is potential for identifying a\\nmodel that generalizes well. This paper introduces the Neural Restricted Isometry\\nProperty (NeuRIPs), which acts as a uniform concentration event that ensures all\\nshallow ReLU networks are sketched with comparable quality. To determine the\\nsample complexity necessary to achieve NeuRIPs, we bound the covering numbers\\nof the networks using the Sub-Gaussian metric and apply chaining techniques. As-\\nsuming the NeuRIPs event, we then provide bounds on the expected risk, applicable\\nto networks within any sublevel set of the empirical risk. Our results show that all\\nnetworks with sufficiently small empirical risk achieve uniform generalization.\\n1 Introduction\\nA fundamental requirement of any scientific model is a clear evaluation of its limitations. In recent\\nyears, supervised machine learning has seen the development of tools for automated model discovery\\nfrom training data. However, these methods often lack a robust theoretical framework to estimate\\nmodel limitations. Statistical learning theory quantifies the limitation of a trained model by the\\ngeneralization error. This theory uses concepts such as the VC-dimension and Rademacher complexity\\nto analyze generalization error bounds for classification problems. While these traditional complexity\\nnotions have been successful in classification problems, they do not apply to generic regression\\nproblems with unbounded risk functions, which are the focus of this study. Moreover, traditional\\ntools in statistical learning theory have not been able to provide a fully satisfying generalization\\ntheory for neural networks.\\nUnderstanding the risk surface during neural network training is crucial for establishing a strong\\ntheoretical foundation for neural network-based machine learning, particularly for understanding\\ngeneralization. Recent studies on neural networks suggest intriguing properties of the risk surface.\\nIn large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,\\nglobal minima exist in each connected component of the risk’s sublevel set and are path-connected.\\nIn this work, we contribute to a generalization theory for shallow ReLU networks, by giving uniform\\ngeneralization error bounds within the empirical risk’s sublevel set. We use methods from the analysis\\nof convex linear regression, where generalization bounds for empirical risk minimizers are derived\\nfrom recent advancements in stochastic processes’ chaining theory. Empirical risk minimization\\nfor non-convex hypothesis functions cannot generally be solved efficiently. However, under certain\\nassumptions, it is still possible to derive generalization error bounds, as we demonstrate in this paper\\nfor shallow ReLU networks. Existing works have applied methods from compressed sensing to\\nbound generalization errors for arbitrary hypothesis functions. However, they do not capture the\\nrisk’s stochastic nature through the more advanced chaining theory.\\nThis paper is organized as follows. We begin in Section II by outlining our assumptions about the\\nparameters of shallow ReLU networks and the data distribution to be interpolated. The expected and\\nempirical risk are introduced in Section III, where we define the Neural Restricted Isometry Property\\n.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 1}, page_content='(NeuRIPs) as a uniform norm concentration event. We present a bound on the sample complexity for\\nachieving NeuRIPs in Theorem 1, which depends on both the network architecture and parameter\\nassumptions. We provide upper bounds on the generalization error that are uniformly applicable\\nacross the sublevel sets of the empirical risk in Section IV . We prove this property in a network\\nrecovery setting in Theorem 2, and also an agnostic learning setting in Theorem 3. These results\\nensure a small generalization error, when any optimization algorithm finds a network with a small\\nempirical risk. We develop the key proof techniques for deriving the sample complexity of achieving\\nNeuRIPs in Section V , by using the chaining theory of stochastic processes. The derived results are\\nsummarized in Section VI, where we also explore potential future research directions.\\n2 Notation and Assumptions\\nIn this section, we will define the key notations and assumptions for the neural networks examined\\nin this study. A Rectified Linear Unit (ReLU) function ϕ : R → R is given by ϕ(x) := max(x, 0).\\nGiven a weight vector w ∈ Rd, a bias b ∈ R, and a sign κ ∈ {±1}, a ReLU neuron is a function\\nϕ(w, b, κ) : Rd → R defined as\\nϕ(w, b, κ)(x) = κϕ(wT x + b).\\nShallow neural networks are constructed as weighted sums of neurons. Typically they are represented\\nby a graph with n neurons in a single hidden layer. When using the ReLU activation function, we can\\napply a symmetry procedure to represent these as sums:\\n¯ϕ¯p(x) =\\nnX\\ni=0\\nϕpi (x),\\nwhere ¯p is the tuple (p1, . . . , pn).\\nAssumption 1. The parameters ¯p, which index shallow ReLU networks, are drawn from a set\\n¯P ⊆ (Rd × R × {±1})n.\\nFor ¯P, we assume there exist constants cw ≥ 0 and cb ∈ [1, 3], such that for all parameter tuples\\n¯p = {(w1, b1, κ1), . . . ,(wn, bn, κn)} ∈¯P, we have\\n∥wi∥ ≤cw and |bi| ≤cb.\\nWe denote the set of shallow networks indexed by a parameter set ¯P by\\nΦ ¯P := {ϕ¯p : ¯p ∈ ¯P}.\\nWe now equip the input spaceRd of the networks with a probability distribution. This distribution\\nreflects the sampling process and makes each neural network a random variable. Additionally, a\\nrandom label y takes its values in the output space R, for which we assume the following.\\nAssumption 2. The random sample x ∈ Rd and label y ∈ R follow a joint distribution µ such that\\nthe marginal distribution µx of sample x is standard Gaussian with density\\n1\\n(2π)d/2 exp\\n\\x12\\n−∥x∥2\\n2\\n\\x13\\n.\\nAs available data, we assume independent copies {(xj, yj)}m\\nj=1 of the random pair (x, y), each\\ndistributed by µ.\\n3 Concentration of the Empirical Norm\\nSupervised learning algorithms interpolate labels y for samples x, both distributed jointly by µ on\\nX × Y. This task is often solved under limited data accessibility. The training data, respecting\\nAssumption 2, consists of m independent copies of the random pair (x, y). During training, the\\ninterpolation quality of a hypothesis function f : X → Ycan only be assessed at the given random\\nsamples {xj}m\\nj=1. Any algorithm therefore accesses each function f through its sketch samples\\nS[f] = (f(x1), . . . , f(xm)),\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 2}, page_content='where S is the sample operator. After training, the quality of a resulting model is often measured by\\nits generalization to new data not used during training. With Rd × R as the input and output space,\\nwe quantify a function f’s generalization error with its expected risk:\\nEµ[f] := Eµ|y − f(x)|2.\\nThe functional || · ||µ, also gives the norm of the space L2(Rd, µx), which consists of functions\\nf : Rd → R with\\n∥f∥2\\nµ := Eµx [|f(x)|2].\\nIf the label y depends deterministically on the associated sample x, we can treat y as an element of\\nL2(Rd, µx), and the expected risk of any function f is the function’s distance to y. By sketching any\\nhypothesis function f with the sample operator S, we perform a Monte-Carlo approximation of the\\nexpected risk, which is termed the empirical risk:\\n∥f∥2\\nm := 1\\nm\\nmX\\nj=1\\n(f(xj) − yj)2 =\\n\\r\\r\\r\\r\\n1√m(y1, . . . , ym)T − S[f]\\n\\r\\r\\r\\r\\n2\\n2\\n.\\nThe random functional || · ||m also defines a seminorm on L2(Rd, µx), referred to as the empirical\\nnorm. Under mild assumptions, || · ||m fails to be a norm.\\nIn order to obtain a well generalizing model, the goal is to identify a function f with a low expected\\nrisk. However, with limited data, we are restricted to optimizing the empirical risk. Our strategy for\\nderiving generalization guarantees is based on the stochastic relation between both risks. If {xj}m\\nj=1\\nare independently distributed by µx, the law of large numbers implies that for any f ∈ L2(Rd, µx)\\nthe convergence\\nlim\\nm→∞\\n∥f∥m = ∥f∥µ.\\nWhile this establishes the asymptotic convergence of the empirical norm to the function norm for a\\nsingle function f, we have to consider two issues to formulate our concept of norm concentration:\\nFirst, we need non-asymptotic results, that is bounds on the distance |∥f∥m − ∥f∥µ| for a fixed\\nnumber of samples m. Second, the bounds on the distance need to be uniformly valid for all functions\\nf in a given set.\\nSample operators which have uniform concentration properties have been studied as restricted\\nisometries in the area of compressed sensing. For shallow ReLU networks of the form (1), we define\\nthe restricted isometry property of the sampling operator S as follows.\\nDefinition 1. Let s ∈ (0, 1) be a constant and ¯P be a parameter set. We say that the Neural Restricted\\nIsometry Property (NeuRIPs( ¯P)) is satisfied if, for all ¯p ∈ ¯P it holds that\\n(1 − s)∥ϕ¯p∥µ ≤ ∥ϕ¯p∥m ≤ (1 + s)∥ϕ¯p∥µ.\\nIn the following Theorem, we provide a bound on the number m of samples, which is sufficient for\\nthe operator S to satisfy NeuRIPs( ¯P).\\nTheorem 1. There exist universal constants C1, C2 ∈ R such that the following holds: For\\nany sample operator S, constructed from random samples {xj}, respecting Assumption 2, let\\n¯P ⊂ (Rd × R × {±1})n be any parameter set satisfying Assumption 1 and ||ϕ¯p||µ > 1 for all\\n¯p ∈ ¯P. Then, for any u > 2 and s ∈ (0, 1), NeuRIPs( ¯P) is satisfied with probability at least\\n1 − 17 exp(−u/4) provided that\\nm ≥ n3c2\\nw\\n(1 − s)2 max\\n\\x12\\nC1\\n(8cb + d + ln(2))\\nu , C2\\nn2c2\\nw\\n(u/s)2\\n\\x13\\n.\\nOne should notice that, in Theorem 1, there is a tradeoff between the parameter s, which limits the\\ndeviation |∥ · ∥m − ∥ · ∥µ|, and the confidence parameter u. The lower bound on the corresponding\\nsample size m is split into two scaling regimes when understanding the quotientu of |∥·∥ m −∥·∥ µ|/s\\nas a precision parameter. While in the regime of low deviations and high probabilities the sample size\\nm must scale quadratically with u/s, in the regime of less precise statements one observes a linear\\nscaling.\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 3}, page_content='4 Uniform Generalization of Sublevel Sets of the Empirical Risk\\nWhen the NeuRIPs event occurs, the function norm || · ||µ, which is related to the expected risk, is\\nclose to || · ||m, which corresponds to the empirical risk. Motivated by this property, we aim to find\\na shallow ReLU network ϕ¯p with small expected risk by solving the empirical risk minimization\\nproblem:\\nmin\\n¯p∈ ¯P\\n∥ϕ¯p − y∥2\\nm.\\nSince the set Φ ¯P of shallow ReLU networks is non-convex, this minimization cannot be solved\\nwith efficient convex optimizers. Therefore, instead of analyzing only the solution ϕ∗\\n¯p of the opti-\\nmization problem, we introduce a tolerance ϵ >0 for the empirical risk and provide bounds on the\\ngeneralization error, which hold uniformly on the sublevel set\\n¯Qy,ϵ :=\\n\\x08\\n¯p ∈ ¯P : ∥ϕ¯p − y∥2\\nm ≤ ϵ\\n\\t\\n.\\nBefore considering generic regression problems, we will initially assume the label y to be a neural\\nnetwork itself, parameterized by a tuplep∗ within the hypothesis set P. For all (x, y) in the support of\\nµ, we have y = ϕp∗ (x) and the expected risk’s minimum on P is zero. Using the sufficient condition\\nfor NeuRIPs from Theorem 1, we can provide generalization bounds for ϕ¯p ∈ ¯Qy,ϵ for any ϵ >0.\\nTheorem 2. Let ¯P be a parameter set that satisfies Assumption 1 and let u ≥ 2 and t ≥ ϵ >0 be\\nconstants. Furthermore, let the number m of samples satisfy\\nm ≥ 8n3c2\\nw (8cb + d + ln(2)) max\\n\\x12\\nC1\\nu\\n(t − ϵ)2 , C2\\nn2c2\\nwu\\n(t − ϵ)2\\n\\x13\\n,\\nwhere C1 and C2 are universal constants. Let {(xj, yj)}m\\nj=1 be a dataset respecting Assumption 2\\nand let there exist a ¯p∗ ∈ ¯P such that yj = ϕ¯p∗ (xj) holds for all j ∈ [m]. Then, with probability at\\nleast 1 − 17 exp(−u/4), we have for all ¯q ∈ ¯Qy,ϵ that\\n∥ϕ¯q − ϕ¯p∗ ∥2\\nµ ≤ t.\\nProof. We notice that ¯Qy,ϵ is a set of shallow neural networks with 2n neurons. We normalize such\\nnetworks with a function norm greater than t and parameterize them by\\n¯Rt := {ϕ¯p − ϕ¯p∗ : ¯p ∈ ¯P ,∥ϕ¯p − ϕ¯p∗ ∥µ > t}.\\nWe assume that NeuRIPs( ¯Rt) holds for s = (t − ϵ)2/t2. In this case, for all ¯q ∈ ¯Qy,ϵ, we have that\\n∥ϕ¯q − ϕ¯p∗ ∥m ≥ t and thus ¯q /∈ ¯Qϕ¯p∗ ,ϵ, which implies that ∥ϕ¯q − ϕ¯p∗ ∥µ ≤ t.\\nWe also note that ¯Rt satisfies Assumption 1 with a rescaled constantcw/t and normalization-invariant\\ncb, if ¯P satisfies it for cw and cb. Theorem 1 gives a lower bound on the sample complexity for\\nNeuRIPs( ¯Rt), completing the proof.\\nAt any network where an optimization method terminates, the concentration of the empirical risk\\nat the expected risk can be achieved with less data than needed to achieve an analogous NeuRIPs\\nevent. However, in the chosen stochastic setting, we cannot assume that the termination of an\\noptimization and the norm concentration at that network are independent events. We overcome this\\nby not specifying the outcome of an optimization method and instead stating uniform bounds on\\nthe norm concentration. The only assumption on an algorithm is therefore the identification of a\\nnetwork that permits an upper bound ϵ on its empirical risk. The event NeuRIPs( ¯Rt) then restricts the\\nexpected risk to be below the corresponding level t.\\nWe now discuss the empirical risk surface for generic distributionsµ that satisfy Assumption 2, where\\ny does not necessarily have to be a neural network.\\nTheorem 3. There exist constants C0, C1, C2, C3, C4, and C5 such that the following holds: Let ¯P\\nsatisfy Assumption 1 for some constants cw, cb, and let ¯p∗ ∈ ¯P be such that for some c¯p∗ ≥ 0 we\\nhave\\nEµ\\n\\x14\\nexp\\n\\x12(y − ϕ¯p∗ (x))2\\nc2\\n¯p∗\\n\\x13\\x15\\n≤ 2.\\nWe assume, for any s ∈ (0, 1) and confidence parameter u >0, that the number of samples m is\\nlarge enough such that\\nm ≥ 8\\n(1 − s)2 max\\n\\x12\\nC1\\n\\x12n3c2\\nw(8cb + d + ln(2))\\nu\\n\\x13\\n, C2n2c2\\nw\\n\\x10u\\ns\\n\\x11\\x13\\n.\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 4}, page_content='We further select confidence parameters v1, v2 > C0, and define for some ω ≥ 0 the parameter\\nη := 2(1 − s)∥ϕ¯p∗ − y∥µ + C3v1v2c¯p∗\\n1\\n(1 − s)1/4 + ω\\n√\\n1 − s.\\nIf we set ϵ = ∥ϕ¯p∗ − y∥2\\nm + ω2 as the tolerance for the empirical risk, then the probability that all\\n¯q ∈ ¯Qy,ϵ satisfy\\n∥ϕ¯q − y∥µ ≤ η\\nis at least\\n1 − 17 exp\\n\\x10\\n−u\\n4\\n\\x11\\n− C5v2 exp\\n\\x12\\n−C4mv2\\n2\\n2\\n\\x13\\n.\\nProof sketch.(Complete proof in Appendix E) We first define and decompose the excess risk by\\nE(¯q, ¯p∗) := ∥ϕ¯q − y∥2\\nµ − ∥ϕ¯p∗ − y∥2\\nµ = ∥ϕ¯q − ϕ¯p∗ ∥2\\nµ − 2\\nm\\nmX\\nj=1\\n(ϕ¯p∗ (xj) − yj)(ϕ¯q(xj) − ϕ¯p∗ (xj)).\\nIt suffices to show, that within the stated confidence level we have∥ϕ¯q − y∥µ > η. This implies the\\nclaim since ∥ϕ¯q − y∥m ≤ ϵ implies ∥ϕ¯q − y∥µ ≤ η. We have E[E(¯q, ¯p∗)] > 0. It now only remains\\nto strengthen the condition on η >3∥ϕ¯p∗ − y∥µ to achieve E(¯q, ¯p∗) > ω2. We apply Theorem 1\\nto derive a bound on the fluctuation of the first term. The concentration rate of the second term is\\nderived similar to Theorem 1 by using chaining techniques. Finally in Appendix E, Theorem 12 gives\\na general bound to achieve\\nE(¯q, ¯p∗) > ω2\\nuniformly for all ¯q with ∥ϕ¯q − ϕ¯p∗ ∥µ > η. Theorem 3 then follows as a simplification.\\nIt is important to notice that, in Theorem 3, as the data size m approaches infinity, one can select\\nan asymptotically small deviation constant s. In this limit, the bound η on the generalization error\\nconverges to 3∥ϕ¯p∗ − y∥µ + ω. This reflects a lower limit of the generalization bound, which is the\\nsum of the theoretically achievable minimum of the expected risk and the additional tolerance ω.\\nThe latter is an upper bound on the empirical risk, which real-world optimization algorithms can be\\nexpected to achieve.\\n5 Size Control of Stochastic Processes on Shallow Networks\\nIn this section, we introduce the key techniques for deriving concentration statements for the em-\\npirical norm, uniformly valid for sets of shallow ReLU networks. We begin by rewriting the event\\nNeuRIPs( ¯P) by treating µ as a stochastic process, indexed by the parameter set ¯P. The event\\nNeuRIPs( ¯P) holds if and only if we have\\nsup\\n¯p∈ ¯P\\n|∥ϕ¯p∥m − ∥ϕ¯p∥µ| ≤s sup\\n¯p∈ ¯P\\n∥ϕ¯p∥µ.\\nThe supremum of stochastic processes has been studied in terms of their size. To determine the size\\nof a process, it is essential to determine the correlation between its variables. To this end, we define\\nthe Sub-Gaussian metric for any parameter tuples ¯p, ¯q ∈ ¯P as\\ndψ2 (ϕ¯p, ϕ¯q) := inf\\n(\\nCψ2 ≥ 0 : E\\n\"\\nexp\\n \\n|ϕ¯p(x) − ϕ¯q(x)|2\\nC2\\nψ2\\n!#\\n≤ 2\\n)\\n.\\nA small Sub-Gaussian metric between random variables indicates that their values are likely to be\\nclose. To capture the Sub-Gaussian structure of a process, we introduce ϵ-nets in the Sub-Gaussian\\nmetric. For a given ϵ >0, these are subsets ¯Q ⊆ ¯P such that for every ¯p ∈ ¯P, there is a ¯q ∈ ¯Q\\nsatisfying\\ndψ2 (ϕ¯p, ϕ¯q) ≤ ϵ.\\nThe smallest cardinality of such an ϵ-net ¯Q is known as the Sub-Gaussian covering number\\nN(Φ ¯P , dψ2 , ϵ). The next Lemma offers a bound for such covering numbers specific to shallow\\nReLU networks.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 5}, page_content='Lemma 1. Let ¯P be a parameter set satisfying Assumption 1. Then there exists a set ˆP with ¯P ⊆ ˆP\\nsuch that\\nN(Φ ˆP , dψ2 , ϵ) ≤ 2n ·\\n\\x1216ncbcw\\nϵ + 1\\n\\x13n\\n·\\n\\x1232ncbcw\\nϵ + 1\\n\\x13n\\n·\\n\\x121\\nϵ sin\\n\\x12 1\\n16ncw\\n\\x13\\n+ 1\\n\\x13d\\n.\\nThe proof of this Lemma is based on the theory of stochastic processes and can be seen in Theorem 8\\nof Appendix C.\\nTo obtain bounds of the form (6) on the size of a process, we use the generic chaining method. This\\nmethod offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric.\\nWe define it as follows. A sequence T = (Tk)k∈N0 in a set T is admissible if T0 = 1 and Tk ≤ 2(2k).\\nThe Talagrand-functional of the metric space is then defined as\\nγ2(T, d) := inf\\n(Tk)\\nsup\\nt∈T\\n∞X\\nk=0\\n2kd(t, Tk),\\nwhere the infimum is taken across all admissible sequences.\\nWith the bounds on the Sub-Gaussian covering number from Lemma 1, we provide a bound on the\\nTalagrand-functional for shallow ReLU networks in the following Lemma. This bound is expected to\\nbe of independent interest.\\nLemma 2. Let ¯P satisfy Assumption 1. Then we have\\nγ2(Φ ¯P , dψ2 ) ≤\\nr\\n2\\nπ\\n\\x128n3/2cw(8cb + d + 1)\\nln(2)\\np\\n2 ln(2)\\n\\x13\\n.\\nThe key ideas to show this bound are similar to the ones used to prove Theorem 9 in Appendix C.\\nTo provide bounds for the empirical process, we use the following Lemma, which we prove in\\nAppendix D.\\nLemma 3. Let Φ be a set of real functions, indexed by a parameter set ¯P and define\\nN(Φ) :=\\nZ ∞\\n0\\nq\\nln N(Φ, dψ2 , ϵ)dϵ and ∆(Φ) := sup\\nϕ∈Φ\\n∥ϕ∥ψ2 .\\nThen, for any u ≥ 2, we have with probability at least 1 − 17 exp(−u/4) that\\nsup\\nϕ∈Φ\\n|∥ϕ∥m − ∥ϕ∥µ| ≤ u√m\\n\\x14\\nN(Φ) + 10\\n3 ∆(Φ)\\n\\x15\\n.\\nThe bounds on the sample complexity for achieving the NeuRIPs event, from Theorem 1, are proven\\nby applying these Lemmata.\\nProof of Theorem 1.Since we assume ||ϕ¯p||µ > 1 for all ¯p ∈ ¯P, we have\\nsup\\n¯p∈ ¯P\\n|∥ϕ¯p∥m − ∥ϕ¯p∥µ| ≤sup\\n¯p∈ ¯P\\n|∥ϕ¯p∥m − ∥ϕ¯p∥µ|/∥ϕ¯p∥µ.\\nApplying Lemma 3, and further applying the bounds on the covering numbers and the Talagrand-\\nfunctional for shallow ReLU networks, the NeuRIPs( ¯P) event holds in case of s >3. The sample\\ncomplexities that are provided in Theorem 1 follow from a refinement of this condition.\\n6 Uniform Generalization of Sublevel Sets of the Empirical Risk\\nIn case of the NeuRIPs event, the function norm || · ||µ corresponding to the expected risk is close\\nto || · ||m, which corresponds to the empirical risk. With the previous results, we can now derive\\nuniform generalization error bounds in the sublevel set of the empirical risk.\\nWe use similar techniques and we define the following sets.\\n∥f∥p = sup\\n1≤q≤p\\n∥f∥q\\nΛk0,u = inf\\n(Tk)\\nsup\\nf∈F\\n∞X\\nk0\\n2k∥f − Tk(f)∥u2k\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/NeurIPS/R013.pdf', 'page': 6}, page_content='and we need the following lemma:\\nLemma 9. For any set F of functions and u ≥ 1, we have\\nΛ0,u(F) ≤ 2√e(γ2(F, dψ2 ) + ∆(F)).\\nTheorem 10. Let P be a parameter set satisfying Assumption 1. Then, for any u ≥ 1, we have with\\nprobability at least 1 − 17 exp(−u/4) that\\nsup\\n¯p∈P\\n∥ϕ¯p∥m − ∥ϕ¯p∥µ ≤ u√m\\n\\x10\\n16n3/2cw(8cb + d + 1) + 2ncw\\n\\x11\\n.\\nProof. To this end we have to bound the Talagrand functional, where we can use Dudley’s inequality\\n(Lemma 6). To finish the proof, we apply the bounds on the covering numbers provided by Theorem\\n6.\\nTheorem 11. Let ¯P ⊆ (Rd × R × ±1)n satisfy Assumption 1. Then there exist universal constants\\nC1, C2 such that\\nsup\\n¯p∈P\\n∥ϕ¯p∥m − ∥ϕ¯p∥µ ≤\\nr\\n2\\nπ\\n\\x128n3/2cw(8cb + d + 1)\\nln(2)\\np\\n2 ln(2)\\n\\x13\\n.\\n7 Conclusion\\nIn this study, we investigated the empirical risk surface of shallow ReLU networks in terms of uniform\\nconcentration events for the empirical norm. We defined the Neural Restricted Isometry Property\\n(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs, which depends on\\nrealistic parameter bounds and the network architecture. We applied our findings to derive upper\\nbounds on the expected risk, which are valid uniformly across sublevel sets of the empirical risk.\\nIf a network optimization algorithm can identify a network with a small empirical risk, our results\\nguarantee that this network will generalize well. By deriving uniform concentration statements, we\\nhave resolved the problem of independence between the termination of an optimization algorithm at\\na certain network and the empirical risk concentration at that network. Future studies may focus on\\nperforming uniform empirical norm concentration on the critical points of the empirical risk, which\\ncould lead to even tighter bounds for the sample complexity.\\nWe also plan to apply our methods to input distributions more general than the Gaussian distribution.\\nIf generic Gaussian distributions can be handled, one could then derive bounds for the Sub-Gaussian\\ncovering number for deep ReLU networks by induction across layers. We also expect that our\\nresults on the covering numbers could be extended to more generic Lipschitz continuous activation\\nfunctions other than ReLU. This proposition is based on the concentration of measure phenomenon,\\nwhich provides bounds on the Sub-Gaussian norm of functions on normal concentrating input spaces.\\nBecause these bounds scale with the Lipschitz constant of the function, they can be used to findϵ-nets\\nfor neurons that have identical activation patterns.\\nBroader Impact\\nSupervised machine learning now affects both personal and public lives significantly. Generalization is\\ncritical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeper\\nunderstanding of the relationships between generalization, architectural design, and available data.\\nWe have discussed the concepts and demonstrated the effectiveness of using uniform concentration\\nevents for generalization guarantees of common supervised machine learning algorithms.\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 0}, page_content='The Importance of Written Explanations in\\nAggregating Crowdsourced Predictions\\nAbstract\\nThis study demonstrates that incorporating the written explanations provided by\\nindividuals when making predictions enhances the accuracy of aggregated crowd-\\nsourced forecasts. The research shows that while majority and weighted vote\\nmethods are effective, the inclusion of written justifications improves forecast\\naccuracy throughout most of a question’s duration, with the exception of its final\\nphase. Furthermore, the study analyzes the attributes that differentiate reliable and\\nunreliable justifications.\\n1 Introduction\\nThe concept of the \"wisdom of the crowd\" posits that combining information from numerous non-\\nexpert individuals can produce answers that are as accurate as, or even more accurate than, those\\nprovided by a single expert. A classic example of this concept is the observation that the median\\nestimate of an ox’s weight from a large group of fair attendees was remarkably close to the actual\\nweight. While generally supported, the idea is not without its limitations. Historical examples\\ndemonstrate instances where crowds behaved irrationally, and even a world chess champion was able\\nto defeat the combined moves of a crowd.\\nIn the current era, the advantages of collective intelligence are widely utilized. For example, Wikipedia\\nrelies on the contributions of volunteers, and community-driven question-answering platforms have\\ngarnered significant attention from the research community. When compiling information from\\nlarge groups, it is important to determine whether the individual inputs were made independently. If\\nnot, factors like group psychology and the influence of persuasive arguments can skew individual\\njudgments, thus negating the positive effects of crowd wisdom.\\nThis paper focuses on forecasts concerning questions spanning political, economic, and social\\ndomains. Each forecast includes a prediction, estimating the probability of a particular event, and\\na written justification that explains the reasoning behind the prediction. Forecasts with identical\\npredictions can have justifications of varying strength, which, in turn, affects the perceived reliability\\nof the predictions. For instance, a justification that simply refers to an external source without\\nexplanation may appear to rely heavily on the prevailing opinion of the crowd and might be considered\\nweaker than a justification that presents specific, verifiable facts from external resources.\\nTo clarify the terminology used: a \"question\" is defined as a statement that seeks information (e.g.,\\n\"Will new legislation be implemented before a certain date?\"). Questions have a defined start and\\nend date, and the period between these dates constitutes the \"life\" of the question. \"Forecasters\"\\nare individuals who provide a \"forecast,\" which consists of a \"prediction\" and a \"justification.\" The\\nprediction is a numerical representation of the likelihood of an event occurring. The justification\\nis the text provided by the forecaster to support their prediction. The central problem addressed in\\nthis work is termed \"calling a question,\" which refers to the process of determining a final prediction\\nby aggregating individual forecasts. Two strategies are employed for calling questions each day\\nthroughout their life: considering forecasts submitted on the given day (\"daily\") and considering the\\nlast forecast submitted by each forecaster (\"active\").'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 1}, page_content='Inspired by prior research on recognizing and fostering skilled forecasters, and analyzing written\\njustifications to assess the quality of individual or collective forecasts, this paper investigates the\\nautomated calling of questions throughout their duration based on the forecasts available each day.\\nThe primary contributions are empirical findings that address the following research questions:\\n* When making a prediction on a specific day, is it advantageous to include forecasts from previous\\ndays? (Yes) * Does the accuracy of the prediction improve when considering the question itself\\nand the written justifications provided with the forecasts? (Yes) * Is it easier to make an accurate\\nprediction toward the end of a question’s duration? (Yes) * Are written justifications more valuable\\nwhen the crowd’s predictions are less accurate? (Yes)\\nIn addition, this research presents an examination of the justifications associated with both accurate\\nand inaccurate forecasts. This analysis aims to identify the features that contribute to a justification\\nbeing more or less credible.\\n2 Related Work\\nThe language employed by individuals is indicative of various characteristics. Prior research includes\\nboth predictive models (using language samples to predict attributes about the author) and models\\nthat provide valuable insights (using language samples and author attributes to identify differentiating\\nlinguistic features). Previous studies have examined factors such as gender and age, political ideology,\\nhealth outcomes, and personality traits. In this paper, models are constructed to predict outcomes\\nbased on crowd-sourced forecasts without knowledge of individual forecasters’ identities.\\nPrevious research has also explored how language use varies depending on the relationships between\\nindividuals. For instance, studies have analyzed language patterns in social networks, online commu-\\nnities, and corporate emails to understand how individuals in positions of authority communicate.\\nSimilarly, researchers have examined how language provides insights into interpersonal interactions\\nand relationships. In terms of language form and function, prior research has investigated politeness,\\nempathy, advice, condolences, usefulness, and deception. Related to the current study’s focus,\\nresearchers have examined the influence of Wikipedia editors and studied influence levels within\\nonline communities. Persuasion has also been analyzed from a computational perspective, including\\nwithin the context of dialogue systems. The work presented here complements these previous studies.\\nThe goal is to identify credible justifications to improve the aggregation of crowdsourced forecasts,\\nwithout explicitly targeting any of the aforementioned characteristics.\\nWithin the field of computational linguistics, the task most closely related to this research is argumen-\\ntation. A strong justification for a forecast can be considered a well-reasoned supporting argument.\\nPrevious work in this area includes identifying argument components such as claims, premises,\\nbacking, rebuttals, and refutations, as well as mining arguments that support or oppose a particular\\nclaim. Despite these efforts, it was found that crowdsourced justifications rarely adhere to these\\nestablished argumentation frameworks, even though such justifications are valuable for aggregating\\nforecasts.\\nFinally, several studies have focused on forecasting using datasets similar or identical to the one used\\nin this research. From a psychological perspective, researchers have explored strategies for enhancing\\nforecasting accuracy, such as utilizing top-performing forecasters (often called \"superforecasters\"),\\nand have analyzed the traits that contribute to their success. These studies aim to identify and cultivate\\nsuperforecasters but do not incorporate the written justifications accompanying forecasts. In contrast,\\nthe present research develops models to call questions without using any information about the\\nforecasters themselves. Within the field of computational linguistics, researchers have evaluated the\\nlanguage used in high-quality justifications, focusing on aspects like rating, benefit, and influence.\\nOther researchers have developed models to predict forecaster skill using the textual justifications\\nfrom specific datasets, such as the Good Judgment Open data, and have also applied these models\\nto predict the accuracy of individual forecasts in other contexts, such as company earnings reports.\\nHowever, none of these prior works have specifically aimed to call questions throughout their entire\\nduration.\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 2}, page_content='3 Dataset\\nThe research utilizes data from the Good Judgment Open, a platform where questions are posted, and\\nindividuals submit their forecasts. The questions primarily revolve around geopolitics, encompassing\\nareas such as domestic and international politics, the economy, and social matters. For this study, all\\nbinary questions were collected, along with their associated forecasts, each comprising a prediction\\nand a justification. In total, the dataset contains 441 questions and 96,664 forecasts submitted\\nover 32,708 days. This dataset significantly expands upon previous research, nearly doubling the\\nnumber of forecasts analyzed. Since the objective is to accurately call questions throughout their\\nentire duration, all forecasts with written justifications are included, regardless of factors such as\\njustification length or the number of forecasts submitted by a single forecaster. Additionally, this\\napproach prioritizes privacy, as no information about the individual forecasters is utilized.\\nTable 1: Analysis of the questions from our dataset. Most questions are relatively long, contain two\\nor more named entities, and are open for over one month.\\nMetric Min Q1 Q2 (Median) Q3 Max Mean\\n# tokens 8 16 20 28 48 21.94\\n# entities 0 2 3 5 11 3.47\\n# verbs 0 2 2 3 6 2.26\\n# days open 2 24 59 98 475 74.16\\nTable 1 provides a basic analysis of the questions in the dataset. The majority of questions are\\nrelatively lengthy, containing more than 16 tokens and multiple named entities, with geopolitical,\\nperson, and date entities being the most frequent. In terms of duration, half of the questions remain\\nopen for nearly two months, and 75% are open for more than three weeks.\\nAn examination of the topics covered by the questions using Latent Dirichlet Allocation (LDA)\\nreveals three primary themes: elections (including terms like \"voting,\" \"winners,\" and \"candidate\"),\\ngovernment actions (including terms like \"negotiations,\" \"announcements,\" \"meetings,\" and \"passing\\n(a law)\"), and wars and violent crimes (including terms like \"groups,\" \"killing,\" \"civilian (casualties),\"\\nand \"arms\"). Although not explicitly represented in the LDA topics, the questions address both\\ndomestic and international events within these broad themes.\\nTable 2: Analysis of the 96,664 written justifications submitted by forecasters in our dataset. The\\nreadability scores indicate that most justifications are easily understood by high school students (11th\\nor 12th grade), although a substantial amount (>25%) require a college education (Flesch under 50 or\\nDale-Chall over 9.0).\\nMin Q1 Q2 Q3 Max\\n#sentences 1 1 1 3 56\\n#tokens 1 10 23 47 1295\\n#entities 0 0 2 4 154\\n#verbs 0 1 3 6 174\\n#adverbs 0 0 1 3 63\\n#adjectives 0 0 2 4 91\\n#negation 0 0 1 3 69\\nSentiment -2.54 0 0 0.20 6.50\\nReadability\\nFlesch -49.68 50.33 65.76 80.62 121.22\\nDale-Chall 0.05 6.72 7.95 9.20 19.77\\nTable 2 presents a fundamental analysis of the 96,664 forecast justifications in the dataset. The median\\nlength is relatively short, consisting of one sentence and 23 tokens. Justifications mention named\\nentities less frequently than the questions themselves. Interestingly, half of the justifications contain\\nat least one negation, and 25% include three or more. This suggests that forecasters sometimes base\\ntheir predictions on events that might not occur or have not yet occurred. The sentiment polarity of\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 3}, page_content='the justifications is generally neutral. In terms of readability, both the Flesch and Dale-Chall scores\\nsuggest that approximately a quarter of the justifications require a college-level education for full\\ncomprehension.\\nRegarding verbs and nouns, an analysis using WordNet lexical files reveals that the most common\\nverb classes are \"change\" (e.g., \"happen,\" \"remain,\" \"increase\"), \"social\" (e.g., \"vote,\" \"support,\"\\n\"help\"), \"cognition\" (e.g., \"think,\" \"believe,\" \"know\"), and \"motion\" (e.g., \"go,\" \"come,\" \"leave\").\\nThe most frequent noun classes are \"act\" (e.g., \"election,\" \"support,\" \"deal\"), \"communication\" (e.g.,\\n\"questions,\" \"forecast,\" \"news\"), \"cognition\" (e.g., \"point,\" \"issue,\" \"possibility\"), and \"group\" (e.g.,\\n\"government,\" \"people,\" \"party\").\\n4 Experiments and Results\\nExperiments are conducted to address the challenge of accurately calling a question throughout\\nits duration. The input consists of the question itself and the associated forecasts (predictions and\\njustifications), while the output is an aggregated answer to the question derived from all forecasts.\\nThe number of instances corresponds to the total number of days all questions were open. Both\\nsimple baselines and a neural network are employed, considering both (a) daily forecasts and (b)\\nactive forecasts submitted up to ten days prior.\\nThe questions are divided into training, validation, and test subsets. Subsequently, all forecasts\\nsubmitted throughout the duration of each question are assigned to their respective subsets. It’s\\nimportant to note that randomly splitting the forecasts would be an inappropriate approach. This is\\nbecause forecasts for the same question submitted on different days would be distributed across the\\ntraining, validation, and test subsets, leading to data leakage and inaccurate performance evaluation.\\n4.1 Baselines\\nTwo unsupervised baselines are considered. The \"majority vote\" baseline determines the answer to a\\nquestion based on the most frequent prediction among the forecasts. The \"weighted vote\" baseline,\\non the other hand, assigns weights to the probabilities in the predictions and then aggregates them.\\n4.2 Neural Network Architecture\\nA neural network architecture is employed, which consists of three main components: one to generate\\na representation of the question, another to generate a representation of each forecast, and an LSTM\\nto process the sequence of forecasts and ultimately call the question.\\nThe representation of a question is obtained using BERT, followed by a fully connected layer with 256\\nneurons, ReLU activation, and dropout. The representation of a forecast is created by concatenating\\nthree elements: (a) a binary flag indicating whether the forecast was submitted on the day the question\\nis being called or on a previous day, (b) the prediction itself (a numerical value between 0.0 and 1.0),\\nand (c) a representation of the justification. The representation of the justification is also obtained\\nusing BERT, followed by a fully connected layer with 256 neurons, ReLU activation, and dropout.\\nThe LSTM has a hidden state with a dimensionality of 256 and processes the sequence of forecasts\\nas its input. During the tuning process, it was discovered that providing the representation of the\\nquestion alongside each forecast is more effective than processing forecasts independently of the\\nquestion. Consequently, the representation of the question is concatenated with the representation of\\neach forecast before being fed into the LSTM. Finally, the last hidden state of the LSTM is connected\\nto a fully connected layer with a single neuron and sigmoid activation to produce the final prediction\\nfor the question.\\n4.3 Architecture Ablation\\nExperiments are carried out with the complete neural architecture, as described above, as well as\\nwith variations where certain components are disabled. Specifically, the representation of a forecast\\nis manipulated by incorporating different combinations of information:\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 4}, page_content='* Only the prediction. * The prediction and the representation of the question. * The prediction and\\nthe representation of the justification. * The prediction, the representation of the question, and the\\nrepresentation of the justification.\\n4.4 Quantitative Results\\nThe evaluation metric used is accuracy, which represents the average percentage of days a model\\ncorrectly calls a question throughout its duration. Results are reported for all days combined, as well\\nas for each of the four quartiles of the question’s duration.\\nTable 3: Results with the test questions (Accuracy: average percentage of days a model predicts a\\nquestion correctly). Results are provided for all days a question was open and for four quartiles (Q1:\\nfirst 25% of days, Q2: 25-50%, Q3: 50-75%, and Q4: last 25% of days).\\nDays When the Question Was Open\\nModel All Days Q1 Q2 Q3 Q4\\nUsing Daily Forecasts Only\\nBaselines\\nMajority V ote (predictions) 71.89 64.59 66.59 73.26 82.22\\nWeighted V ote (predictions) 73.79 67.79 68.71 74.16 83.61\\nNeural Network Variants\\nPredictions Only 77.96 77.62 77.93 78.23 78.61\\nPredictions + Question 77.61 75.44 76.77 78.05 81.56\\nPredictions + Justifications 80.23 77.87 78.65 79.26 84.67\\nPredictions + Question + Justifications 79.96 78.65 78.11 80.29 83.28\\nUsing Active Forecasts\\nBaselines\\nMajority V ote (predictions) 77.27 68.83 73.92 77.98 87.44\\nWeighted V ote (predictions) 77.97 72.04 72.17 78.53 88.22\\nNeural Network Variants\\nPredictions Only 78.81 77.31 78.04 78.53 81.11\\nPredictions + Question 79.35 76.05 78.53 79.56 82.94\\nPredictions + Justifications 80.84 77.86 79.07 79.74 86.17\\nPredictions + Question + Justifications 81.27 78.71 79.81 81.56 84.67\\nDespite their relative simplicity, the baseline methods achieve commendable results, demonstrating\\nthat aggregating forecaster predictions without considering the question or justifications is a viable\\nstrategy. However, the full neural network achieves significantly improved results.\\n**Using Daily or Active Forecasts** Incorporating active forecasts, rather than solely relying on\\nforecasts submitted on the day the question is called, proves advantageous for both baselines and all\\nneural network configurations, except for the one using only predictions and justifications.\\n**Encoding Questions and Justifications** The neural network that only utilizes the prediction\\nto represent a forecast surpasses both baseline methods. Notably, integrating the question, the\\njustification, or both into the forecast representation yields further improvements. These results\\nindicate that incorporating the question and forecaster-provided justifications into the model enhances\\nthe accuracy of question calling.\\n**Calling Questions Throughout Their Life** When examining the results across the four quartiles of\\na question’s duration, it’s observed that while using active forecasts is beneficial across all quartiles\\nfor both baselines and all network configurations, the neural networks surprisingly outperform the\\nbaselines only in the first three quartiles. In the last quartile, the neural networks perform significantly\\nworse than the baselines. This suggests that while modeling questions and justifications is generally\\nhelpful, it becomes detrimental toward the end of a question’s life. This phenomenon can be attributed\\nto the increasing wisdom of the crowd as more evidence becomes available and more forecasters\\ncontribute, making their aggregated predictions more accurate.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 5}, page_content='Table 4: Results with the test questions, categorized by question difficulty as determined by the best\\nbaseline model. The table presents the accuracy (average percentage of days a question is predicted\\ncorrectly) for all questions and for each quartile of difficulty: Q1 (easiest 25%), Q2 (25-50%), Q3\\n(50-75%), and Q4 (hardest 25%).\\nQuestion Difficulty (Based on Best Baseline)\\nAll Q1 Q2 Q3 Q4\\nUsing Active Forecasts\\nWeighted V ote Baseline (Predictions) 77.97 99.40 99.55 86.01 29.30\\nNeural Network with Components...\\nPredictions + Question 79.35 94.58 88.01 78.04 58.73\\nPredictions + Justifications 80.84 95.71 93.18 79.99 57.05\\nPredictions + Question + Justifications 81.27 94.17 90.11 78.67 64.41\\n**Calling Questions Based on Their Difficulty** The analysis is further refined by examining\\nresults based on question difficulty, determined by the number of days the best-performing baseline\\nincorrectly calls the question. This helps to understand which questions benefit most from the neural\\nnetworks that incorporate questions and justifications. However, it’s important to note that calculating\\nquestion difficulty during the question’s active period is not feasible, making these experiments\\nunrealistic before the question closes and the correct answer is revealed.\\nTable 4 presents the results for selected models based on question difficulty. The weighted vote\\nbaseline demonstrates superior performance for 75\\n5 Qualitative Analysis\\nThis section provides insights into the factors that make questions more difficult to forecast and\\nexamines the characteristics of justifications associated with incorrect and correct predictions.\\n**Questions** An analysis of the 88 questions in the test set revealed that questions called incorrectly\\non at least one day by the best model tend to have a shorter duration (69.4 days vs. 81.7 days) and a\\nhigher number of active forecasts per day (31.0 vs. 26.7). This suggests that the model’s errors align\\nwith the questions that forecasters also find challenging.\\n**Justifications** A manual review of 400 justifications (200 associated with incorrect predictions\\nand 200 with correct predictions) was conducted, focusing on those submitted on days when the best\\nmodel made an incorrect prediction. The following observations were made:\\n* A higher percentage of incorrect predictions (78%) were accompanied by short justifications\\n(fewer than 20 tokens), compared to 65% for correct predictions. This supports the idea that longer\\nuser-generated text often indicates higher quality. * References to previous forecasts (either by the\\nsame or other forecasters, or the current crowd’s forecast) were more common in justifications for\\nincorrect predictions (31.5%) than for correct predictions (16%). * A lack of a logical argument\\nwas prevalent in the justifications, regardless of the prediction’s accuracy. However, it was more\\nfrequent in justifications for incorrect predictions (62.5%) than for correct predictions (47.5%). *\\nSurprisingly, justifications with generic arguments did not clearly differentiate between incorrect and\\ncorrect predictions (16.0% vs. 14.5%). * Poor grammar and spelling or the use of non-English were\\ninfrequent but more common in justifications for incorrect predictions (24.5%) compared to correct\\npredictions (14.5%).\\n6 Conclusions\\nForecasting involves predicting future events, a capability highly valued by both governments and\\nindustries as it enables them to anticipate and address potential challenges. This study focuses on\\nquestions spanning the political, economic, and social domains, utilizing forecasts submitted by a\\ncrowd of individuals without specialized training. Each forecast comprises a prediction and a natural\\nlanguage justification.\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R009.pdf', 'page': 6}, page_content='The research demonstrates that aggregating the weighted predictions of forecasters is a solid baseline\\nfor calling a question throughout its duration. However, models that incorporate both the question\\nand the justifications achieve significantly better results, particularly during the first three quartiles of\\na question’s life. Importantly, the models developed in this study do not profile individual forecasters\\nor utilize any information about their identities. This work lays the groundwork for evaluating the\\ncredibility of anonymous forecasts, enabling the development of robust aggregation strategies that do\\nnot require tracking individual forecasters.\\n7')]\n",
            "--------------------\n",
            "--------------------\n",
            "[Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 0}, page_content='Advanced techniques for through and contextually\\nInterpreting Noun-Noun Compounds\\nAbstract\\nThis study examines the effectiveness of transfer learning and multi-task learning\\nin the context of a complex semantic classification problem: understanding the\\nmeaning of noun-noun compounds. Through a series of detailed experiments and\\nan in-depth analysis of errors, we demonstrate that employing transfer learning by\\ninitializing parameters and multi-task learning through parameter sharing enables a\\nneural classification model to better generalize across a dataset characterized by a\\nhighly uneven distribution of semantic relationships. Furthermore, we illustrate\\nhow utilizing dual annotations, which involve two distinct sets of relations applied\\nto the same compounds, can enhance the overall precision of a neural classifier and\\nimprove its F1 scores for less common yet more challenging semantic relations.\\n1 Introduction\\nNoun-noun compound interpretation involves determining the semantic connection between two\\nnouns (or noun phrases in multi-word compounds). For instance, in the compound \"street protest,\"\\nthe task is to identify the semantic relationship between \"street\" and \"protest,\" which is a locative\\nrelation in this example. Given the prevalence of noun-noun compounds in natural language and its\\nsignificance to other natural language processing (NLP) tasks like question answering and information\\nretrieval, understanding noun-noun compounds has been extensively studied in theoretical linguistics,\\npsycholinguistics, and computational linguistics.\\nIn computational linguistics, noun-noun compound interpretation is typically treated as an automatic\\nclassification task. Various machine learning (ML) algorithms and models, such as Maximum\\nEntropy, Support Vector Machines, and Neural Networks, have been employed to decipher the\\nsemantics of nominal compounds. These models utilize information from lexical semantics, like\\nWordNet-based features, and distributional semantics, such as word embeddings. However, noun-\\nnoun compound interpretation remains a challenging NLP problem due to the high productivity\\nof noun-noun compounding as a linguistic structure and the difficulty in deriving the semantics of\\nnoun-noun compounds from their constituents. Our research contributes to advancing NLP research\\non noun-noun compound interpretation through the application of transfer and multi-task learning.\\nThe application of transfer learning (TL) and multi-task learning (MTL) in NLP has gained significant\\nattention in recent years, yielding varying outcomes based on the specific tasks, model architectures,\\nand datasets involved. These varying results, combined with the fact that neither TL nor MTL has\\nbeen previously applied to noun-noun compound interpretation, motivate our thorough empirical\\ninvestigation into the use of TL and MTL for this task. Our aim is not only to add to the existing\\nresearch on the effectiveness of TL and MTL for semantic NLP tasks generally but also to ascertain\\ntheir specific advantages for compound interpretation.\\nA key reason for utilizing multi-task learning is to enhance generalization by making use of the\\ndomain-specific details present in the training data of related tasks. In this study, we demonstrate that\\nTL and MTL can serve as a form of regularization, enabling the prediction of infrequent relations\\nwithin a dataset marked by a highly skewed distribution of relations. This dataset is particularly\\nwell-suited for TL and MTL experimentation, as elaborated in Section 3.'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 1}, page_content='Our contributions are summarized as follows:\\n1. Through meticulous analysis of results, we discover that TL and MTL, especially when applied\\nto the embedding layer, enhance overall accuracy and F1 scores for less frequent relations in a\\nhighly skewed dataset, compared to a robust single-task learning baseline. 2. Although our research\\nconcentrates on TL and MTL, we present, to our knowledge, the first experimental results on the\\nrelatively recent dataset from Fares (2016).\\n2 Related Work\\nApproaches to interpreting noun-noun compounds differ based on the classification of compound\\nrelations, as well as the machine learning models and features employed to learn these relations. For\\ninstance, some define a broad set of relations, while others employ a more detailed classification.\\nSome researchers challenge the idea that noun-noun compounds can be interpreted using a fixed,\\npredetermined set of relations, proposing alternative methods based on paraphrasing. We center\\nour attention on methods that frame the interpretation problem as a classification task involving a\\nfixed, predetermined set of relations. Various machine learning models have been applied to this\\ntask, including nearest neighbor classifiers that use semantic similarity based on lexical resources,\\nkernel-based methods like SVMs that utilize lexical and relational features, Maximum Entropy\\nmodels that incorporate a wide range of lexical and surface form features, and neural networks that\\nrely on word embeddings or combine word embeddings with path embeddings. Among these studies,\\nsome have utilized the same dataset. To our knowledge, TL and MTL have not been previously\\napplied to compound interpretation. Therefore, we review prior research on TL and MTL in other\\nNLP tasks.\\nSeveral recent studies have conducted extensive experiments on the application of TL and MTL to a\\nvariety of NLP tasks, such as named entity recognition, semantic labeling, sentence-level sentiment\\nclassification, super-tagging, chunking, and semantic dependency parsing. The consensus among\\nthese studies is that the advantages of TL and MTL are largely contingent on the characteristics of the\\ntasks involved, including the unevenness of the data distribution, the semantic relatedness between\\nthe source and target tasks, the learning trajectory of the auxiliary and main tasks (where target tasks\\nthat quickly reach a plateau benefit most from non-plateauing auxiliary tasks), and the structural\\nsimilarity between the tasks. Besides differing in the NLP tasks they investigate, the aforementioned\\nstudies employ slightly varied definitions of TL and MTL. Our research aligns with certain studies in\\nthat we apply TL and MTL to learn different semantic annotations of noun-noun compounds using\\nthe same dataset. However, our experimental design is more akin to other work in that we experiment\\nwith initializing parameters across all layers of the neural network and concurrently train a single\\nMTL model on two sets of relations.\\n3 Task Definition and Dataset\\nThe objective of this task is to train a model to categorize the semantic relationships between pairs\\nof nouns in a labeled dataset, where each pair forms a noun-noun compound. The complexity of\\nthis task is influenced by factors such as the label set used and its distribution. For the experiments\\ndetailed in this paper, we utilize a noun-noun compounds dataset that features compounds annotated\\nwith two distinct taxonomies of relations. This means that each noun-noun compound is associated\\nwith two different relations, each based on different linguistic theories. This dataset is derived from\\nestablished linguistic resources, including NomBank and the Prague Czech-English Dependency\\nTreebank 2.0 (PCEDT). We chose this dataset for two primary reasons: firstly, the dual annotation of\\nrelations on the same set of compounds is ideal for exploring TL and MTL approaches; secondly,\\naligning two different annotation frameworks on the same data allows for a comparative analysis\\nacross these frameworks.\\nSpecifically, we use a portion of the dataset, focusing on type-based instances of two-word compounds.\\nThe original dataset also encompasses multi-word compounds (those made up of more than two\\nnouns) and multiple instances per compound type. We further divide the dataset into three parts:\\ntraining, development, and test sets. Table 1 details the number of compound types and the vocabulary\\nsize for each set, including a breakdown of words appearing in the right-most (right constituents)\\nand left-most (left constituents) positions. The two label sets consist of 35 PCEDT functors and 18\\n2'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 2}, page_content='NomBank argument and adjunct relations. As discussed in Section 7.1, these label sets have a highly\\nuneven distribution.\\nTable 1: Characteristics of the noun-noun compound dataset used in our experiments. The numbers\\nin this table correspond to a subset of the dataset, see Section 3.\\nTrain Dev Test\\nCompounds 6932 920 1759\\nV ocab size 4102 1163 1772\\nRight constituents 2304 624 969\\nLeft constituents 2405 618 985\\nMany relations in PCEDT and NomBank conceptually describe similar semantic ideas, as they are\\nused to annotate the semantics of the same text. For instance, the temporal and locative relations in\\nNomBank (ARGM-TMP and ARGM-LOC, respectively) and their PCEDT counterparts (TWHEN\\nand LOC) exhibit relatively consistent behavior across frameworks, as they annotate many of the\\nsame compounds. However, some relations that are theoretically similar do not align well in practice.\\nFor example, the functor AIM in PCEDT and the modifier argument ARGM-PNC in NomBank\\nexpress a somewhat related semantic concept (purpose), but there is minimal overlap between the\\nsets of compounds they annotate. Nevertheless, it is reasonable to assume that the semantic similarity\\nin the label sets, where it exists, can be leveraged through transfer and multi-task learning, especially\\nsince the overall distribution of relations differs between the two frameworks.\\n4 Transfer vs. Multi-Task Learning\\nIn this section, we employ the terminology and definitions established by Pan and Yang (2010) to\\narticulate our framework for transfer and multi-task learning. Our classification task can be described\\nin terms of all training pairs (X, Y) and a probability distribution P(X), where X represents the input\\nfeature space, Y denotes the set of all labels, and N is the training data size. The domain of a task is\\ndefined by X, P(X). Our goal is to learn a function f(X) that predicts Y based on the input features X.\\nConsidering two ML tasks, Ta and Tb, we would train two distinct models to learn separate functions\\nfa and fb for predicting Ya and Yb in a single-task learning scenario. However, if Ta and Tb are\\nrelated, either explicitly or implicitly, TL and MTL can enhance the generalization of either or both\\ntasks. Two tasks are deemed related when their domains are similar but their label sets differ, or when\\ntheir domains are dissimilar but their label sets are identical. Consequently, noun-noun compound\\ninterpretation using the dataset is well-suited for TL and MTL, as the training examples are identical,\\nbut the label sets are distinct.\\nFor clarity, we differentiate between transfer learning and multi-task learning in this paper, despite\\nthese terms sometimes being used interchangeably in the literature. We define TL as the utilization of\\nparameters from a model trained on Ta to initialize another model for Tb. In contrast, MTL involves\\ntraining parts of the same model to learn both Ta and Tb, essentially learning one set of parameters\\nfor both tasks. The concept is to train a single model simultaneously on both tasks, where one task\\nintroduces an inductive bias that aids the model in generalizing over the main task. It is important to\\nnote that this does not necessarily imply that we aim to use a single model to predict both label sets\\nin practice.\\n5 Neural Classification Models\\nThis section introduces the neural classification models utilized in our experiments. To discern the\\nimpact of TL and MTL, we initially present a single-task learning model, which acts as our baseline.\\nSubsequently, we employ this same model to implement TL and MTL.\\n5.1 Single-Task Learning Model\\nIn our single-task learning (STL) configuration, we train and fine-tune a feed-forward neural network\\ninspired by the neural classifier proposed by Dima and Hinrichs (2015). This network comprises four\\nlayers: 1) an input layer, 2) an embedding layer, 3) a hidden layer, and 4) an output layer. The input\\n3'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 3}, page_content='layer consists of two integers that indicate the indices of a compound’s constituents in the embedding\\nlayer, where the word embedding vectors are stored. These selected vectors are then passed to a fully\\nconnected hidden layer, the size of which matches the dimensionality of the word embedding vectors.\\nFinally, a softmax function is applied to the output layer to select the most probable relation.\\nThe compound’s constituents are represented using a 300-dimensional word embedding model trained\\non an English Wikipedia dump and the English Gigaword Fifth Edition. The embedding model was\\ntrained by Fares et al. (2017). If a word is not found during lookup in the embedding model, we\\ncheck if the word is uppercased and attempt to find the lowercase version. For hyphenated words\\nnot found in the embedding vocabulary, we split the word at the hyphen and average the vectors of\\nits parts, if they are present in the vocabulary. If the word remains unrepresented after these steps, a\\ndesignated vector for unknown words is employed.\\n5.1.1 Architecture and Hyperparameters\\nOur selection of hyperparameters is informed by multiple rounds of experimentation with the single-\\ntask learning model, as well as the choices made by prior work. The weights of the embedding layer\\nare updated during the training of all models. We utilize the Adaptive Moment Estimation (Adam)\\noptimization function across all models, with a learning rate set to 0.001. The loss function employed\\nis the negative-log likelihood. A Sigmoid activation function is used for the units in the hidden layer.\\nAll models are trained with mini-batches of size five. The maximum number of epochs is capped\\nat 50, but an early stopping criterion based on the model’s accuracy on the validation split is also\\nimplemented. This means that training is halted if the validation accuracy does not improve over five\\nconsecutive epochs. All models are implemented in Keras, using TensorFlow as the backend. The TL\\nand MTL models are trained using the same hyperparameters as the STL model.\\n5.2 Transfer Learning Models\\nIn our experiments, transfer learning involves training an STL model on PCEDT relations and then\\nusing some of its weights to initialize another model for NomBank relations. Given the neural\\nclassifier architecture detailed in Section 5.1, we identify three ways to implement TL: 1) TLE:\\nTransferring the embedding layer weights, 2) TLH: Transferring the hidden layer weights, and 3)\\nTLEH: Transferring both the embedding and hidden layer weights. Furthermore, we differentiate\\nbetween transfer learning from PCEDT to NomBank and vice versa. This results in six setups,\\nas shown in Table 2. We do not apply TL (or MTL) to the output layer because it is task- or\\ndataset-specific.\\n5.3 Multi-Task Learning Models\\nIn MTL, we train a single model to simultaneously learn both PCEDT and NomBank relations,\\nmeaning all MTL models have two objective functions and two output layers. We implement two\\nMTL setups: MTLE, which features a shared embedding layer but two task-specific hidden layers,\\nand MTLF, which has no task-specific layers aside from the output layer (i.e., both the embedding\\nand hidden layers are shared). We distinguish between the auxiliary and main tasks based on which\\nvalidation accuracy (NomBank’s or PCEDT’s) is monitored by the early stopping criterion. This\\nleads to a total of four MTL models, as shown in Table 3.\\n6 Experimental Results\\nTables 2 and 3 display the accuracies of the various TL and MTL models on the development and test\\nsplits for NomBank and PCEDT. The top row in both tables indicates the accuracy of the STL model.\\nAll models were trained solely on the training split. Several insights can be gleaned from these\\ntables. Firstly, the accuracy of the STL models decreases when evaluated on the test split for both\\nNomBank and PCEDT. Secondly, all TL models achieve improved accuracy on the NomBank test\\nsplit, although transfer learning does not significantly enhance accuracy on the development split of\\nthe same dataset. The MTL models, especially MTLF, have a detrimental effect on the development\\naccuracy of NomBank, yet we observe a similar improvement, as with TL, on the test split. Thirdly,\\nboth TL and MTL models demonstrate less consistent effects on PCEDT (on both development and\\ntest splits) compared to NomBank. For instance, all TL models yield an absolute improvement of\\n4'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 4}, page_content='about 1.25 points in accuracy on NomBank, whereas in PCEDT, TLE clearly outperforms the other\\ntwo TL models (TLE improves over the STL accuracy by 1.37 points).\\nTable 2: Accuracy (%) of the transfer learning models.\\nModel NomBank PCEDT\\nDev Test Dev Test\\nSTL 78.15 76.75 58.80 56.05\\nTLE 78.37 78.05 59.57 57.42\\nTLH 78.15 78.00 59.24 56.51\\nTLEH 78.48 78.00 59.89 56.68\\nTable 3: Accuracy (%) of the MTL models.\\nModel NomBank PCEDT\\nDev Test Dev Test\\nSTL 78.15 76.75 58.80 56.05\\nMTLE 77.93 78.45 59.89 56.96\\nMTLF 76.74 78.51 58.91 56.00\\nOverall, the STL models’ accuracy declines when tested on the NomBank and PCEDT test splits,\\ncompared to their performance on the development split. This could suggest overfitting, especially\\nsince our stopping criterion selects the model with the best performance on the development split.\\nConversely, TL and MTL enhance accuracy on the test splits, despite using the same stopping criterion\\nas STL. We interpret this as an improvement in the models’ ability to generalize. However, since\\nthese improvements are relatively minor, we further analyze the results to understand if and how TL\\nand MTL are beneficial.\\n7 Results Analysis\\nThis section provides a detailed analysis of the models’ performance, drawing on insights from the\\ndataset and the classification errors made by the models. The discussion in the following sections is\\nprimarily based on the results from the test split, as it is larger than the development split.\\n7.1 Relation Distribution\\nTo illustrate the complexity of the task, we depict the distribution of the most frequent relations in\\nNomBank and PCEDT across the three data splits in Figure 1. Notably, approximately 71.18% of the\\nrelations in the NomBank training split are of type ARG1 (prototypical patient), while 52.20% of the\\nPCEDT relations are of type RSTR (an underspecified adnominal modifier). Such a highly skewed\\ndistribution makes learning some of the other relations more challenging, if not impossible in certain\\ncases. In fact, out of the 15 NomBank relations observed in the test split, five are never predicted\\nby any of the STL, TL, or MTL models. Similarly, of the 26 PCEDT relations in the test split, only\\nsix are predicted. However, the unpredicted relations are extremely rare in the training split (e.g., 23\\nPCEDT functors appear less than 20 times), making it doubtful whether any ML model could learn\\nthem under any circumstances.\\nGiven this imbalanced distribution, it is evident that accuracy alone is insufficient to determine the\\nbest-performing model. Therefore, in the subsequent section, we report and analyze the F1 scores of\\nthe predicted NomBank and PCEDT relations across all STL, TL, and MTL models.\\n7.2 Per-Relation F1 Scores\\nTables 4 and 5 present the per-relation F1 scores for NomBank and PCEDT, respectively. We only\\ninclude results for relations that are actually predicted by at least one of the models.\\n5'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 5}, page_content='Table 4: Per-label F1 score on the NomBank test split.\\nA0 A1 A2 A3 LOC MNR TMP\\nCount 132 1282 153 75 25 25 27\\nSTL 49.82 87.54 45.78 60.81 28.57 29.41 66.67\\nTLE 55.02 87.98 41.61 60.14 27.91 33.33 63.83\\nTLH 54.81 87.93 42.51 60.00 25.00 35.29 65.31\\nTLEH 53.62 87.95 42.70 61.11 29.27 33.33 65.22\\nMTLE 54.07 88.34 42.86 61.97 30.00 28.57 66.67\\nMTLF 53.09 88.41 38.14 62.69 00.00 00.00 52.17\\nTable 5: Per-label F1 score on the PCEDT test split.\\nACT TWHEN APP PAT REG RSTR\\nCount 89 14 118 326 216 900\\nSTL 43.90 42.11 22.78 42.83 20.51 68.81\\nTLE 49.37 70.97 27.67 41.60 30.77 69.67\\nTLH 53.99 62.07 25.00 43.01 26.09 68.99\\nTLEH 49.08 64.52 28.57 42.91 28.57 69.08\\nMTLE 54.09 66.67 24.05 42.03 27.21 69.31\\nMTLF 47.80 42.11 25.64 40.73 19.22 68.89\\nSeveral noteworthy patterns emerge from Tables 4 and 5. Firstly, the MTLF model appears to be\\ndetrimental to both datasets, leading to significantly degraded F1 scores for four NomBank relations,\\nincluding the locative modifier ARGM-LOC and the manner modifier ARGM-MNR (abbreviated as\\nLOC and MNR in Table 4), which the model fails to predict altogether. This same model exhibits\\nthe lowest F1 score compared to all other models for two PCEDT relations: REG (expressing a\\ncircumstance) and PAT (patient). Considering that the MTLF model achieves the highest accuracy\\non the NomBank test split (as shown in Table 3), it becomes even more apparent that relying solely\\non accuracy scores is inadequate for evaluating the effectiveness of TL and MTL for this task and\\ndataset.\\nSecondly, with the exception of the MTLF model, all TL and MTL models consistently improve\\nthe F1 score for all PCEDT relations except PAT. Notably, the F1 scores for the relations TWHEN\\nand ACT show a substantial increase compared to other PCEDT relations when only the embedding\\nlayer’s weights are shared (MTLE) or transferred (TLE). This outcome can be partially understood\\nby examining the correspondence matrices between NomBank arguments and PCEDT functors,\\npresented in Tables 7 and 6. These tables illustrate how PCEDT functors map to NomBank arguments\\nin the training split (Table 6) and vice versa (Table 7). Table 6 reveals that 80% of the compounds\\nannotated as TWHEN in PCEDT were annotated as ARGM-TMP in NomBank. Additionally, 47% of\\nACT (Actor) relations map to ARG0 (Proto-Agent) in NomBank. While this mapping is not as distinct\\nas one might hope, it is still relatively high when compared to how other PCEDT relations map to\\nARG0. The correspondence matrices also demonstrate that the presumed theoretical similarities\\nbetween NomBank and PCEDT relations do not always hold in practice. Nevertheless, even such\\nimperfect correspondences can provide a training signal that assists the TL and MTL models in\\nlearning relations like TWHEN and ACT.\\nSince the TLE model outperforms STL in predicting REG by ten absolute points, we examined\\nall REG compounds correctly classified by TLE but misclassified by STL. We found that STL\\nmisclassified them as RSTR, indicating that TL from NomBank helps TLE recover from STL’s\\novergeneralization in RSTR prediction.\\nThe two NomBank relations that receive the highest boost in F1 score (about five absolute points)\\nare ARG0 and ARGM-MNR, but the improvement in the latter corresponds to only one additional\\ncompound, which might be a chance occurrence. Overall, TL and MTL from NomBank to PCEDT\\nare more helpful than the reverse. One explanation is that five PCEDT relations (including the four\\nmost frequent ones) map to ARG1 in NomBank in more than 60% of cases for each relation, as seen\\nin the first rows of Tables 6 and 7. This suggests that the weights learned to predict PCEDT relations\\n6'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 6}, page_content='Table 6: Correspondence matrix between PCEDT functors and NomBank arguments. Slots with ’-’\\nindicate zero, 0.00 represents a very small number but not zero.\\nA1 A2 A0 A3 LOC TMP MNR\\nRSTR 0.70 0.11 0.06 0.06 0.02 0.01 0.02\\nPAT 0.90 0.05 0.01 0.02 0.01 - 0.00\\nREG 0.78 0.10 0.04 0.06 0.00 0.00 0.00\\nAPP 0.62 0.21 0.13 0.02 0.01 0.00 -\\nACT 0.47 0.03 0.47 0.01 0.01 - 0.01\\nAIM 0.65 0.12 0.07 0.06 0.01 - -\\nTWHEN 0.10 0.03 - - - 0.80 -\\nCount 3617 1312 777 499 273 116 59\\nTable 7: Correspondence matrix between NomBank arguments and PCEDT functors.\\nRSTR PAT REG APP ACT AIM TWHEN\\nA1 0.51 0.54 0.12 0.06 0.03 0.02 0.00\\nA2 0.47 0.09 0.11 0.14 0.01 0.02 0.00\\nA0 0.63 0.03 0.07 0.13 0.26 0.02 -\\nA3 0.66 0.08 0.13 0.03 0.01 0.02 -\\nLOC 0.36 0.07 0.02 0.05 0.03 0.01 -\\nTMP 0.78 - 0.01 0.01 - - 0.01\\nMNR 0.24 0.05 0.01 - 0.03 - -\\nCount 4932 715 495 358 119 103 79\\noffer little to no inductive bias for NomBank relations. Conversely, the mapping from NomBank to\\nPCEDT shows that although many NomBank arguments map to RSTR in PCEDT, the percentages\\nare lower, making the mapping more diverse and discriminative, which seems to aid TL and MTL\\nmodels in learning less frequent PCEDT relations.\\nTo understand why the PCEDT functor AIM is never predicted despite being more frequent than\\nTWHEN, we found that AIM is almost always misclassified as RSTR by all models. Furthermore,\\nAIM and RSTR have the highest lexical overlap in the training set among all PCEDT relation pairs:\\n78.35% of left constituents and 73.26% of right constituents of compounds annotated as AIM occur\\nin other compounds annotated as RSTR. This explains the models’ inability to learn AIM but raises\\nquestions about their ability to learn relational representations, which we explore further in Section\\n7.3.\\nTable 8: Macro-average F1 score on the test split.\\nModel NomBank PCEDT\\nSTL 52.66 40.15\\nTLE 52.83 48.34\\nTLH 52.98 46.52\\nTLEH 53.31 47.12\\nMTLE 53.21 47.23\\nMTLF 42.07 40.73\\nFinally, to demonstrate the benefits of TL and MTL for NomBank and PCEDT, we report the F1\\nmacro-average scores in Table 8. This is arguably the appropriate evaluation measure for imbalanced\\nclassification problems. Note that relations not predicted by any model are excluded from the macro-\\naverage calculation. Table 8 clearly shows that TL and MTL on the embedding layer yield significant\\nimprovements for PCEDT, with about a 7-8 point increase in macro-average F1, compared to just\\n0.65 in the best case for NomBank.\\n7'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 7}, page_content='7.3 Generalization on Unseen Compounds\\nWe now analyze the models’ ability to generalize to compounds not seen during training. Recent\\nresearch suggests that gains in noun-noun compound interpretation using word embeddings and\\nsimilar neural classification models might be due to lexical memorization. In other words, the models\\nlearn that specific nouns are strong indicators of specific relations. To assess the role of lexical\\nmemorization in our models, we quantify the number of unseen compounds that the STL, TL, and\\nMTL models predict correctly.\\nWe differentiate between ’partly’ and ’completely’ unseen compounds. A compound is ’partly’\\nunseen if one of its constituents (left or right) is not present in the training data. A ’completely’\\nunseen compound is one where neither the left nor the right constituent appears in the training data.\\nOverall, nearly 20% of the compounds in the test split have an unseen left constituent, about 16%\\nhave an unseen right constituent, and 4% are completely unseen. Table 9 compares the performance\\nof the different models on these three groups in terms of the proportion of compounds misclassified\\nin each group.\\nTable 9: Generalization error on the subset of unseen compounds in the test split. L: Left constituent.\\nR: Right constituent. L&R: Completely unseen.\\nNomBank PCEDT\\nModel L R L&R L R L&R\\nCount 351 286 72 351 286 72\\nSTL 27.92 39.51 50.00 45.01 47.55 41.67\\nTLE 25.93 36.71 48.61 43.87 47.55 41.67\\nTLH 26.21 38.11 50.00 46.15 49.30 47.22\\nTLEH 26.50 38.81 52.78 45.87 47.55 43.06\\nMTLE 24.50 33.22 38.89 44.44 47.20 43.06\\nMTLF 22.79 34.27 40.28 44.16 47.90 38.89\\nTable 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce\\ngeneralization error in NomBank across all scenarios, with the exception of TLH and TLEH for\\ncompletely unseen compounds, where error increases. The greatest error reductions are achieved\\nby MTL models across all three types of unseen compounds. Specifically, MTLE reduces the error\\nby approximately six points for compounds with unseen right constituents and by eleven points for\\nfully unseen compounds. Moreover, MTLF reduces the error by five points when the left constituent\\nis unseen. It’s important to interpret these results in conjunction with the Count row in Table 9 for\\na comprehensive view. For example, the eleven-point error decrease in fully unseen compounds\\nrepresents eight compounds. In PCEDT, the largest error reduction is on unseen left constituents,\\nwhich is about 1.14 points, corresponding to four compounds; it’s 0.35 on unseen right constituents\\n(one compound) and 2.7 on fully unseen compounds, or two compounds.\\nUpon manual inspection of compounds that led to substantial reductions in the generalization error,\\nspecifically within NomBank, we examined the distribution of relations within correctly predicted\\nunseen compound sets. Compared to the STL model, MTLE reduces generalization error for\\ncompletely unseen compounds by a total of eight compounds, of which seven are annotated with the\\nrelation ARG1, which is the most common in NomBank. Regarding the unseen right constituents,\\nMTLE’s 24 improved compounds consist of 18 ARG1, 5 ARG0, and 1 ARG2 compounds. A\\nsimilar pattern arises when examining TLE model improvements, where most gains come from better\\npredictions of ARG1 and ARG0 relations.\\nA large portion of unseen compounds, whether partly or entirely unseen, that were misclassified by\\nevery model, were not of type ARG1 in NomBank, or RSTR in PCEDT. This pattern, along with\\ncorrectly predicted unseen compounds primarily annotated with the most common relations, suggests\\nthat classification models rely on lexical memorization to learn the compound relation interpretation.\\nTo better comprehend lexical memorization’s impact, we present the ratio of relation-specific con-\\nstituents in both NomBank and PCEDT, as depicted in Figure 2. We define a relation-specific\\nconstituent as a left or right constituent that appears with only one specific relation within the training\\ndata. Its ratio is calculated as its proportion in the full set of left or right constituents for each\\n8'), Document(metadata={'source': '/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Publishable/EMNLP/R008.pdf', 'page': 8}, page_content='relation. Analyzing Figure 2 reveals that NomBank relations possess higher ratios of relation-specific\\nconstituents compared to PCEDT. This potentially makes learning the former easier if the model\\nsolely relies on lexical memorization. Additionally, ARGM-TMP in NomBank and TWHEN in\\nPCEDT have distinctly high ratios compared to other relations in Figure 2. These relations also\\nhave the second-highest F1 score in their datasets—except for STL on PCEDT (see Tables 4 and\\n5). Lexical memorization is therefore a likely cause of these high F1 scores. We also observed that\\nlower ratios of relation-specific constituents correlate with lower F1 scores, such as APP and REG in\\nPCEDT. Based on these insights, we can’t dismiss the possibility that our models show some degree\\nof lexical memorization, despite manual analysis also presenting cases where models demonstrate\\ngeneralization and correct predictions in situations where lexical memorization is impossible.\\n8 Conclusion\\nThe application of transfer and multi-task learning in natural language processing has gained sig-\\nnificant traction, yet considerable ambiguity persists regarding the effectiveness of particular task\\ncharacteristics and experimental setups. This research endeavors to clarify the benefits of TL and\\nMTL in the context of semantic interpretation of noun-noun compounds. By executing a sequence of\\nminimally contrasting experiments and conducting thorough analysis of results and prediction errors,\\nwe demonstrate how both TL and MTL can mitigate the effects of class imbalance and drastically\\nenhance predictions for low-frequency relations. Overall, our TL, and particularly our MTL models,\\nare better at making predictions both quantitatively and qualitatively. Notably, the improvements are\\nobserved on the ’most challenging’ inputs that include at least one constituent that was not present in\\nthe training data. However, clear indications of ’lexical memorization’ effects are evident in our error\\nanalysis of unseen compounds.\\nTypically, the transfer of representations or sharing between tasks is more effective at the embedding\\nlayers, which represent the model’s internal representation of the compound constituents. Furthermore,\\nin multi-task learning, the complete sharing of model architecture across tasks degrades its capacity\\nto generalize when it comes to less frequent relations.\\nThe dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound\\ninterpretation because it links this sub-problem with broad-coverage semantic role labeling or\\nsemantic dependency parsing in PCEDT and NomBank. Future research will focus on incorporating\\nadditional natural language processing tasks defined using these frameworks to understand noun-noun\\ncompound interpretation using TL and MTL.\\n9')]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt7NfrluV6m9",
        "outputId": "d826972d-5653-474d-d5bc-1b64436e677d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.3.29)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.14)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.10.3)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.23.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.3)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.2.2)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "fqW9UpoNVHQ4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "2ynZqimtqSEb",
        "outputId": "59c8a6dc-2908-45e9-acac-8f92e73bb192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.27.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.3.29)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (3.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.2.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "ApaTXsekqUXs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "OjHmENBlqYfg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "rHsF57cXq-HZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "CPlVXQhHrxr5",
        "outputId": "2416ace3-de39-46f4-e2bf-f89b4e01eb7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "f1e2dbc7097e48e3b29ce4cf283575f4",
            "ae2eea5018624cd387b795984f8c67f6",
            "2e47d86c7981472fabb943f6ca37c365",
            "fb71f3c4cf514fb898b8a3eebd6e1fdf",
            "2ba9e4f813fd4842bc240d3842a9976d",
            "dd4c64153d054299a502c00f3974872a",
            "fdb439b8e6464d1da7d0cb3ed32dff1c",
            "cebb234a65194569a3b8473058957901",
            "148dea2d807c43038b255e414eda18fa",
            "5c3856ea7c7c4a84b38f1fb03e3057c2",
            "7c3bf3fc14214759ab54ce1664ea6491",
            "e168f3b81f234a6fac29826f15ac5daf",
            "161d7f2f88024cbfb26808ca49917836",
            "866a5806968b442a8208ba7eb627665d",
            "95bba2fb994943afb2d64f85cb811653",
            "729530f3ff424afca61e8c5c3903c5d8",
            "0eff1d2d2a964c97ba9d87ba260017b1",
            "140fc7ebea5d465dafaf3039ebab2b1f",
            "89c2ef67c4204bf1b0cab9804f36a5c3",
            "3735f928b1014db0b2a64679b316d2cb",
            "5aeedde62af846858225587c11e8cff4",
            "3b68b38f10b141939b9efe7010a5334e",
            "c043deaa42744b5dbdd7ce40bd0b96e3",
            "8782202762304a478451959e2a3665d8",
            "635b5c2714ed4f0eaf2991629cfe2118",
            "718d6cb50ec345acb3ffe314eda20374",
            "2a58719da0f2499ea6a98a0b74e52815",
            "8e14e2f34395491288dc8ec588e45330",
            "2921bde2dcce40b59e1965443b7aa117",
            "5ed798867644499e90c6a59394ff99c5",
            "4bf7b30a67684b3a83d3a7762a445e8e",
            "706b1b1659ff467b8cd7609762b1d883",
            "0ccadeda1a6b4d71bbf70b7d1b7f1251",
            "c02f19f5daad4ba3a2c815c2b52706c6",
            "5e86a0f28bc8456f80ccd8e392681c84",
            "11410dcb748347e59bbe35ce94236762",
            "fa92650d90c4467b8b83d4071964e8f6",
            "f6f6f23bd99d44189dd30f5a83ebf826",
            "a96c30d301684ee0bef8dd02d6f90686",
            "e3c4cdbf2ba347cc89544faddcc274f1",
            "671cf7cdda2e4c67b3039a19994794d8",
            "ea28ae37ddbd4de0a4e329a516be237f",
            "a23ec6188993433784ee1454afb29d83",
            "f3a46da21a594fe2b907011b71990b28",
            "ab6aa16c05fe44c08af8fea8622d5415",
            "38e45eb8125847b38e670c1897e193fc",
            "70b60239c0a8405da1fef8e3dbbc2942",
            "eedb7d83347f4a1c892bd492fb7ebbc8",
            "dcef677ca95d4a3f80c0cb736d5e5e11",
            "bc8c8cd12870410590124466054b493d",
            "72b8aaf708864588ae13386f6f25451e",
            "8a0cbec074dc4d54ac6e6001760766ff",
            "356566524a1d47f79a0ee8dc5e3c0a8c",
            "67678895d1364d9a93fe5467b1264701",
            "6c2e1b44d3c64074a417dc5c840f65ce",
            "7bc1d5b62e7a4dcb9c58064a6f7b7a5a",
            "f0589e52ee0c45138ab5a9b85b47c27d",
            "4302ba166bb14f62bc3f12fcd126866f",
            "89dba6251bcd4cdf9c8d54b0db04c1aa",
            "4fca89a599864bd9be4aa81d8babf220",
            "c3bf25f15d8d437aae6ae8c81a7c95f6",
            "0da8e46e4221459a8d9cbb865d856b73",
            "eeeee69c075e4a1ca8adcc995b2b2a17",
            "faa5cf4b79024095beab5db4af58c605",
            "7150a8e4edf54bc2b7c1df7b3f625d8e",
            "900941e17a1c4c27a93c755ccf7ccad6",
            "af009acf067f43248fb071614710116f",
            "8c26ee641f02434c91a8566ba4770648",
            "5d322c0a368f4a788d10ccaa7b711e27",
            "bae8b0d47ddf418290a05e02bb4d702f",
            "9efdd4755934439bb64b46d2ec78e735",
            "d0789b3354814e40bcb7e8c85c038d99",
            "ecfc1873d2c34c53bb09258c96c444be",
            "6c39a1e141104b13b6f761eadb52ee6b",
            "b546a3025c0e44159d19fc7973d875d4",
            "ad831f4978a84457b7cfbe309cbb33aa",
            "86e7958fda384eb99d9c4888ccfc885a",
            "f0330413031d47ff80155fb02a4ed907",
            "8ec4b93f57d6410ea47bd3e02e4be329",
            "7a5d713d35c44a6882f1701da3bc867d",
            "5d1f0dcf9ea74a1b8119cd5fc57b84d5",
            "8b89a0039237477ba5fa75a993b471de",
            "a00f4d6689f347e6a42139f88efeca9a",
            "6e023457518d4229a8a24f5c7a2ef312",
            "9cc1451f8ad44b5baae618c844a124bc",
            "da6f1cc346d54c98adfe3be452558529",
            "00ee60f53fd34bf29ce2fd165db5e62e",
            "1117d011ede14425beb35f87f26d4fec",
            "78298257b52e45f2ab68a6cd3439e295",
            "997518bd215941b0b9bbb91275992d68",
            "ade07784d7a948f580efa5e569a84cb9",
            "20199058dc04478d9c4f5ac3ae646b8e",
            "767b723c9cce4b4bac276d39b9a1cd42",
            "c001ad29e19f4952b34a129ec3990182",
            "cfdf8359bb29455baa660799ff696cba",
            "d2e4387fb7144391a2b389497e87f3d6",
            "5dd88371fcb94db886817b9e80e7083b",
            "aff331d868c145e88fbeba9e049ef5d4",
            "ba79bb95f62d41ea8912a27d30ea7aae",
            "5e968e750ef24b6f88186d8976b888a0",
            "af6811c7099342338c99e28aabc59920",
            "4b8e5b041ae2480693e5948e4234d3f8",
            "015b14b288694b21a32e7811524f6bb4",
            "e1e7bd92f5a544958266732f9a9a823f",
            "18f03ea11480453a975600a624232928",
            "b75aa641e792478f9066dab1a61c711a",
            "29bb68ce8e984188b9451721a11a4b28",
            "d60d2b0d674f4a9287a05a4976f120e8",
            "4428f31e9e5c4ab3a58bc55d9b32b268",
            "9f2b80e9598b40aa95d29b08832d96b9",
            "7020d01221e344e9bc0c610a1a95c552",
            "5338a7465bb84eaf97bc929a868d5d19",
            "3e81c0615cc947cc8ff398c33bd2511d",
            "6886a2cb6c164ef4996413762ad58f35",
            "860ee6d0fb9d423a9362eacfb339cabf",
            "52cdffef1c5747ada6b013d908cd2451",
            "6219bc17c0394bf3a81d5afc72fbad86",
            "80874ff8fb3f4446bd58fccde205e41a",
            "8befcb1d14614d05bc9a1d753df91c7c",
            "1cafe4de3eda4a23bd760113dc3fbbae",
            "bbe9f17571ec4409bca55422bb1db7ab"
          ]
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1e2dbc7097e48e3b29ce4cf283575f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e168f3b81f234a6fac29826f15ac5daf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c043deaa42744b5dbdd7ce40bd0b96e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c02f19f5daad4ba3a2c815c2b52706c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab6aa16c05fe44c08af8fea8622d5415"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bc1d5b62e7a4dcb9c58064a6f7b7a5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af009acf067f43248fb071614710116f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0330413031d47ff80155fb02a4ed907"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78298257b52e45f2ab68a6cd3439e295"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e968e750ef24b6f88186d8976b888a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7020d01221e344e9bc0c610a1a95c552"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)"
      ],
      "metadata": {
        "id": "7PEDyjuHtBHo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_text = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "\n",
        "split_docs1 = split_text.split_documents(document_list1[0])\n",
        "split_docs2 = split_text.split_documents(document_list2[0])"
      ],
      "metadata": {
        "id": "63_N8gE07Pes"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for index, doc in enumerate(split_docs1):\n",
        "  print(\"-\"*20)\n",
        "  print(\"PAGE NUMBER \" + str(index))\n",
        "  print(\"-\"*20)\n",
        "  print(doc.page_content)\n",
        "  print(\"-\"*20)"
      ],
      "metadata": {
        "id": "Wj_SqdxZ7rvJ",
        "outputId": "56c88c69-6e20-4b59-f197-4454b6def715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "PAGE NUMBER 0\n",
            "--------------------\n",
            "Detailed Action Identification in Baseball Game\n",
            "Recordings\n",
            "Abstract\n",
            "This research introduces MLB-YouTube, a new and complex dataset created for\n",
            "nuanced activity recognition in baseball videos. This dataset is structured to\n",
            "support two types of analysis: one for classifying activities in segmented videos\n",
            "and another for detecting activities in unsegmented, continuous video streams. This\n",
            "study evaluates several methods for recognizing activities, focusing on how they\n",
            "capture the temporal organization of activities in videos. This evaluation starts\n",
            "with categorizing segmented videos and progresses to applying these methods\n",
            "to continuous video feeds. Additionally, this paper assesses the effectiveness of\n",
            "different models in the challenging task of forecasting pitch velocity and type\n",
            "using baseball broadcast videos. The findings indicate that incorporating temporal\n",
            "dynamics into models is beneficial for detailed activity recognition.\n",
            "1 Introduction\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 1\n",
            "--------------------\n",
            "different models in the challenging task of forecasting pitch velocity and type\n",
            "using baseball broadcast videos. The findings indicate that incorporating temporal\n",
            "dynamics into models is beneficial for detailed activity recognition.\n",
            "1 Introduction\n",
            "Action recognition, a significant problem in computer vision, finds extensive use in sports. Profes-\n",
            "sional sporting events are extensively recorded for entertainment, and these recordings are invaluable\n",
            "for subsequent analysis by coaches, scouts, and media analysts. While numerous game statistics\n",
            "are currently gathered manually, the potential exists for these to be replaced by computer vision\n",
            "systems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)\n",
            "to automatically record pitch speed and movement, utilizing a network of high-speed cameras and\n",
            "radar to collect detailed data on each player. Access to much of this data is restricted from the public\n",
            "domain.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 2\n",
            "--------------------\n",
            "to automatically record pitch speed and movement, utilizing a network of high-speed cameras and\n",
            "radar to collect detailed data on each player. Access to much of this data is restricted from the public\n",
            "domain.\n",
            "This paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-\n",
            "ties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or\n",
            "detection, our dataset emphasizes fine-grained activity recognition. The differences between activities\n",
            "are often minimal, primarily involving the movement of a single individual, with a consistent scene\n",
            "structure across activities. The determination of activity is based on a single camera perspective. This\n",
            "study compares various methods for temporal feature aggregation, both for classifying activities in\n",
            "segmented videos and for detecting them in continuous video streams.\n",
            "2 Related Work\n",
            "The field of activity recognition has garnered substantial attention in computer vision research. Initial\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 3\n",
            "--------------------\n",
            "segmented videos and for detecting them in continuous video streams.\n",
            "2 Related Work\n",
            "The field of activity recognition has garnered substantial attention in computer vision research. Initial\n",
            "successes were achieved with hand-engineered features such as dense trajectories. The focus of more\n",
            "recent studies has shifted towards the application of Convolutional Neural Networks (CNNs) for\n",
            "activity recognition. Two-stream CNN architectures utilize both spatial RGB frames and optical\n",
            "flow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have been\n",
            "developed. The development of these advanced CNN models has been supported by large datasets\n",
            "such as Kinetics, THUMOS, and ActivityNet.\n",
            "Several studies have investigated the aggregation of temporal features for the purpose of activity\n",
            "recognition. Research has compared several pooling techniques and determined that both Long Short-\n",
            ".\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 4\n",
            "--------------------\n",
            "Term Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. It\n",
            "has been discovered that pooling intervals from varying locations and durations is advantageous for\n",
            "activity recognition. It was demonstrated that identifying and classifying key sub-event intervals can\n",
            "lead to better performance.\n",
            "Recently, segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently\n",
            "for activity detection. These methods depend on the 3D CNN to capture temporal dynamics, which\n",
            "typically span only 16 frames. Although longer-term temporal structures have been explored, this was\n",
            "usually accomplished with temporal pooling of localized features or (spatio-)temporal convolutions\n",
            "with extended fixed intervals. Recurrent Neural Networks (RNNs) have also been applied to represent\n",
            "transitions in activity between frames.\n",
            "3 MLB-YouTube Dataset\n",
            "We have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 5\n",
            "--------------------\n",
            "transitions in activity between frames.\n",
            "3 MLB-YouTube Dataset\n",
            "We have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\n",
            "on YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos\n",
            "intended for activity recognition and continuous videos designed for activity classification. The\n",
            "dataset’s complexity is amplified by the fact that it originates from televised baseball games, where a\n",
            "single camera perspective is shared among various activities. Additionally, there is minimal variance\n",
            "in motion and appearance among different activities, such as swinging a bat versus bunting. In\n",
            "contrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities\n",
            "with diverse settings, scales, and camera angles, our dataset features activities where a single frame\n",
            "might not be adequate to determine the activity.\n",
            "The minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 6\n",
            "--------------------\n",
            "with diverse settings, scales, and camera angles, our dataset features activities where a single frame\n",
            "might not be adequate to determine the activity.\n",
            "The minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between\n",
            "these actions requires identifying whether the batter swings or not, detecting the umpire’s signal\n",
            "(Figure 4) for a strike, or noting the absence of a signal for a ball. This is further complicated because\n",
            "the batter or catcher can obstruct the umpire, and each umpire has their unique style of signaling a\n",
            "strike.\n",
            "Our dataset for segmented video analysis comprises 4,290 clips. Each clip is annotated for multiple\n",
            "baseball actions, including swing, hit, ball, strike, and foul. Given that a single clip may contain\n",
            "several activities, this is considered a multi-label classification task. Table 1 presents the complete\n",
            "list of activities and their respective counts within the dataset. Additionally, clips featuring a pitch\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 7\n",
            "--------------------\n",
            "several activities, this is considered a multi-label classification task. Table 1 presents the complete\n",
            "list of activities and their respective counts within the dataset. Additionally, clips featuring a pitch\n",
            "were annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. Furthermore, a\n",
            "collection of 2,983 hard negative examples, where no action is present, was gathered. These instances\n",
            "include views of the crowd, the field, or players standing idly before or after a pitch. Examples of\n",
            "activities and hard negatives are depicted in Figure 2.\n",
            "Our continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Every\n",
            "frame in these videos is annotated with the baseball activities that occur. On average, each continuous\n",
            "clip contains 7.2 activities, amounting to over 15,000 activity instances in total.\n",
            "Table 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.\n",
            "Activity Count\n",
            "No Activity 2983\n",
            "Ball 1434\n",
            "Strike 1799\n",
            "Swing 2506\n",
            "Hit 1391\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 8\n",
            "--------------------\n",
            "clip contains 7.2 activities, amounting to over 15,000 activity instances in total.\n",
            "Table 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.\n",
            "Activity Count\n",
            "No Activity 2983\n",
            "Ball 1434\n",
            "Strike 1799\n",
            "Swing 2506\n",
            "Hit 1391\n",
            "Foul 718\n",
            "In Play 679\n",
            "Bunt 24\n",
            "Hit by Pitch 14\n",
            "2\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 9\n",
            "--------------------\n",
            "4 Segmented Video Recognition Approach\n",
            "We investigate different techniques for aggregating temporal features in segmented video activity\n",
            "recognition. In segmented videos, the classification task is simpler because each frame corresponds to\n",
            "an activity, eliminating the need for the model to identify the start and end of activities. Our methods\n",
            "are based on a CNN that generates a per-frame or per-segment representation, derived from standard\n",
            "two-stream CNNs using deep CNNs like I3D or InceptionV3.\n",
            "Given video features v of dimensions T × D, where T represents the video’s temporal length and D\n",
            "is the feature’s dimensionality, the usual approach for feature pooling involves max- or mean-pooling\n",
            "across the temporal dimension, followed by a fully-connected layer for video clip classification, as\n",
            "depicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,\n",
            "losing temporal information. An alternative is to employ a fixed temporal pyramid with various\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 10\n",
            "--------------------\n",
            "depicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,\n",
            "losing temporal information. An alternative is to employ a fixed temporal pyramid with various\n",
            "lengths, as shown in Fig 5(b), dividing the video into intervals of lengths 1/2, 1/4, and 1/8, and\n",
            "max-pooling each. The pooled features are concatenated, creating a K × D representation, where K\n",
            "is the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip.\n",
            "We also explore learning temporal convolution filters to aggregate local temporal structures. A kernel\n",
            "of size L×1 is applied to each frame, enabling each timestep representation to incorporate information\n",
            "from adjacent frames. After applying max-pooling to the output of the temporal convolution, a fully-\n",
            "connected layer is used for classification, as illustrated in Fig. 5(c).\n",
            "While temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 11\n",
            "--------------------\n",
            "connected layer is used for classification, as illustrated in Fig. 5(c).\n",
            "While temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.\n",
            "Previous studies have shown that learning the sub-interval to pool is beneficial for activity recognition.\n",
            "These learned intervals are defined by three parameters: a center g, a width σ, and a stride δ,\n",
            "parameterizing N Gaussians. Given the video length T, the positions of the strided Gaussians are\n",
            "first calculated as:\n",
            "gn = 0.5 − T − (gn + 1)\n",
            "N − 1 forn = 0, 1, . . . , N− 1\n",
            "pt,n = gn + (t − 0.5T + 0.5)1\n",
            "δ fort = 0, 1, . . . , T− 1\n",
            "The filters are then generated as:\n",
            "Fm[i, t] = 1\n",
            "Zm\n",
            "exp\n",
            "\u0012\n",
            "−(t − µi,m)2\n",
            "2σ2m\n",
            "\u0013\n",
            "i ∈ {0, 1, . . . , N− 1}, t∈ {0, 1, . . . , T− 1}\n",
            "where Zm is a normalization constant.\n",
            "We apply these filters F to the T × D video representation through matrix multiplication, yielding an\n",
            "N × D representation that serves as input to a fully-connected layer for classification. This method\n",
            "is shown in Fig 5(d).\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 12\n",
            "--------------------\n",
            "We apply these filters F to the T × D video representation through matrix multiplication, yielding an\n",
            "N × D representation that serves as input to a fully-connected layer for classification. This method\n",
            "is shown in Fig 5(d).\n",
            "Additionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state\n",
            "as input to a fully-connected layer for classification. We frame our tasks as multi-label classification\n",
            "and train these models to minimize binary cross-entropy:\n",
            "L(v) =\n",
            "X\n",
            "c\n",
            "zc log(p(c|G(v))) + (1− zc) log(1− p(c|G(v)))\n",
            "where G(v) is the function that pools the temporal information, and zc is the ground truth label for\n",
            "class c.\n",
            "5 Activity Detection in Continuous Videos\n",
            "Detecting activities in continuous videos poses a greater challenge. The goal here is to classify each\n",
            "frame according to the activities occurring. Unlike segmented videos, continuous videos feature\n",
            "multiple sequential activities, often interspersed with frames of inactivity. This necessitates that\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 13\n",
            "--------------------\n",
            "frame according to the activities occurring. Unlike segmented videos, continuous videos feature\n",
            "multiple sequential activities, often interspersed with frames of inactivity. This necessitates that\n",
            "the model learn to identify the start and end points of activities. As a baseline, we train a single\n",
            "fully-connected layer to serve as a per-frame classifier, which does not utilize temporal information\n",
            "beyond that contained in the features.\n",
            "3\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 14\n",
            "--------------------\n",
            "We adapt the methods developed for segmented video classification to continuous videos by imple-\n",
            "menting a temporal sliding window approach. We select a fixed window duration of L features, apply\n",
            "max-pooling to each window (similar to Fig. 5(a)), and classify each pooled segment. This approach\n",
            "is extended to temporal pyramid pooling by dividing the window of lengthL into segments of lengths\n",
            "L/2, L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,\n",
            "and the pooled features are concatenated, yielding a 14 × D-dimensional representation for each\n",
            "window, which is then used as input to the classifier.\n",
            "For temporal convolutional models in continuous videos, we modify the segmented video approach by\n",
            "learning a temporal convolutional kernel of length L and convolving it with the input video features.\n",
            "This operation transforms input of size T × D into output of size T × D, followed by a per-frame\n",
            "classifier. This enables the model to aggregate local temporal information.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 15\n",
            "--------------------\n",
            "This operation transforms input of size T × D into output of size T × D, followed by a per-frame\n",
            "classifier. This enables the model to aggregate local temporal information.\n",
            "To extend the sub-event model to continuous videos, we follow a similar approach but setT = L in\n",
            "Eq. 1, resulting in filters of length L. The T ×D video representation is convolved with the sub-event\n",
            "filters F, producing an N × D × T-dimensional representation used as input to a fully-connected\n",
            "layer for frame classification.\n",
            "The model is trained to minimize per-frame binary classification:\n",
            "L(v) =\n",
            "X\n",
            "t,c\n",
            "zt,c log(p(c|H(vt))) + (1− zt,c) log(1− p(c|H(vt)))\n",
            "where vt is the per-frame or per-segment feature at time t, H(vt) is the sliding window application of\n",
            "one of the feature pooling methods, and zt,c is the ground truth class at time t.\n",
            "A method to learn ’super-events’ (i.e., global video context) has been introduced and shown to be\n",
            "effective for activity detection in continuous videos. This approach involves learning a set of temporal\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 16\n",
            "--------------------\n",
            "A method to learn ’super-events’ (i.e., global video context) has been introduced and shown to be\n",
            "effective for activity detection in continuous videos. This approach involves learning a set of temporal\n",
            "structure filters modeled as N Cauchy distributions. Each distribution is defined by a center xn and a\n",
            "width γn. Given the video length T, the filters are constructed by:\n",
            "xn = (T − 1)(tanh(x′\n",
            "n) + 1)\n",
            "2\n",
            "fn(t) = 1\n",
            "Zn\n",
            "γn\n",
            "π((t − xn)2 + γ2n) exp(1 − 2|tanh(γ′\n",
            "n)|)\n",
            "where Zn is a normalization constant, t ∈ {1, 2, . . . , T}, and n ∈ {1, 2, . . . , N}.\n",
            "The filters are combined with learned per-class soft-attention weights A, and the super-event repre-\n",
            "sentation is computed as:\n",
            "Sc =\n",
            "X\n",
            "n\n",
            "Ac,n\n",
            "X\n",
            "t\n",
            "fn(t) · vt\n",
            "where v is the T × D video representation. These filters enable the model to focus on relevant\n",
            "intervals for temporal context. The super-event representation is concatenated to each timestep and\n",
            "used for classification. We also experiment with combining the super- and sub-event representations\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 17\n",
            "--------------------\n",
            "intervals for temporal context. The super-event representation is concatenated to each timestep and\n",
            "used for classification. We also experiment with combining the super- and sub-event representations\n",
            "to form a three-level hierarchy for event representation.\n",
            "6 Experiments\n",
            "6.1 Implementation Details\n",
            "For our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics\n",
            "datasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable\n",
            "feature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet\n",
            "and Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth\n",
            "compared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow\n",
            "was computed and clipped to [−20, 20]. For InceptionV3, features were computed every 3 frames\n",
            "(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 18\n",
            "--------------------\n",
            "was computed and clipped to [−20, 20]. For InceptionV3, features were computed every 3 frames\n",
            "(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\n",
            "3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam\n",
            "optimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50\n",
            "epochs.\n",
            "4\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 19\n",
            "--------------------\n",
            "6.2 Segmented Video Activity Recognition\n",
            "We initially conducted binary pitch/non-pitch classification for each video segment. This task is\n",
            "relatively straightforward due to the distinct differences between pitch and non-pitch frames. The\n",
            "results, detailed in Table 2, reveal minimal variation across different features or models.\n",
            "Table 2: Performance on segmented videos for binary pitch/non-pitch classification.\n",
            "Model RGB Flow Two-stream\n",
            "InceptionV3 97.46 98.44 98.67\n",
            "InceptionV3 + sub-events 98.67 98.73 99.36\n",
            "I3D 98.64 98.88 98.70\n",
            "I3D + sub-events 98.42 98.35 98.65\n",
            "6.2.1 Multi-label Classification\n",
            "We assessed various temporal feature aggregation methods by calculating the mean average precision\n",
            "(mAP) for each video clip, a standard metric for multi-label classification. Table 4 compares the\n",
            "performance of these methods. All methods surpass mean/max-pooling, highlighting the importance\n",
            "of preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 20\n",
            "--------------------\n",
            "performance of these methods. All methods surpass mean/max-pooling, highlighting the importance\n",
            "of preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs\n",
            "show some improvement. Temporal convolution offers a more significant performance boost but\n",
            "requires substantially more parameters (see Table 3). Learning sub-events, as per previous research,\n",
            "yields the best results. While LSTMs and temporal convolutions have been used before, they need\n",
            "more parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitate\n",
            "sequential processing of video features, whereas other methods can be fully parallelized.\n",
            "Table 3: Additional parameters required for models when added to the base model (e.g., I3D or\n",
            "Inception V3).\n",
            "Model # Parameters\n",
            "Max/Mean Pooling 16K\n",
            "Pyramid Pooling 115K\n",
            "LSTM 10.5M\n",
            "Temporal Conv 31.5M\n",
            "Sub-events 36K\n",
            "Table 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 21\n",
            "--------------------\n",
            "Inception V3).\n",
            "Model # Parameters\n",
            "Max/Mean Pooling 16K\n",
            "Pyramid Pooling 115K\n",
            "LSTM 10.5M\n",
            "Temporal Conv 31.5M\n",
            "Sub-events 36K\n",
            "Table 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.\n",
            "Learning sub-intervals for pooling is found to be crucial for activity recognition.\n",
            "Method RGB Flow Two-stream\n",
            "Random 16.3 16.3 16.3\n",
            "InceptionV3 + mean-pool 35.6 47.2 45.3\n",
            "InceptionV3 + max-pool 47.9 48.6 54.4\n",
            "InceptionV3 + pyramid 49.7 53.2 55.3\n",
            "InceptionV3 + LSTM 47.6 55.6 57.7\n",
            "InceptionV3 + temporal conv 47.2 55.2 56.1\n",
            "InceptionV3 + sub-events 56.2 62.5 62.6\n",
            "I3D + mean-pool 42.4 47.6 52.7\n",
            "I3D + max-pool 48.3 53.4 57.2\n",
            "I3D + pyramid 53.2 56.7 58.7\n",
            "I3D + LSTM 48.2 53.1 53.1\n",
            "I3D + temporal conv 52.8 57.1 58.4\n",
            "I3D + sub-events 55.5 61.2 61.3\n",
            "Table 5 shows the average precision for each activity class. Learning temporal structure is particularly\n",
            "beneficial for frame-based features (e.g., InceptionV3), which capture less temporal information\n",
            "5\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 22\n",
            "--------------------\n",
            "compared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detecting\n",
            "strikes, hits, foul balls, and hit-by-pitch events, which exhibit changes in video features post-event.\n",
            "For instance, after a hit, the camera often tracks the ball’s trajectory, while after a hit-by-pitch, it\n",
            "follows the player to first base, as illustrated in Fig. 6 and Fig. 7.\n",
            "Table 5: Per-class average precision for segmented videos using two-stream features in multi-\n",
            "label activity classification. Utilizing sub-events to discern temporal intervals of interest proves\n",
            "advantageous for activity recognition.\n",
            "Method Ball Strike Swing Hit Foul In Play Bunt Hit by Pitch\n",
            "Random 21.8 28.6 37.4 20.9 11.4 10.3 1.1 4.5\n",
            "InceptionV3 + max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7\n",
            "InceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2\n",
            "I3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2\n",
            "I3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0\n",
            "6.2.2 Pitch Speed Regression\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 23\n",
            "--------------------\n",
            "InceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2\n",
            "I3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2\n",
            "I3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0\n",
            "6.2.2 Pitch Speed Regression\n",
            "Estimating pitch speed from video frames is an exceptionally difficult problem, as it requires the\n",
            "network to pinpoint the pitch’s start and end, and derive the speed from a minimal signal. The baseball,\n",
            "often obscured by the pitcher, travels at speeds over 100mph and covers 60.5 feet in approximately 0.5\n",
            "seconds. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,\n",
            "proving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos, we\n",
            "recalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connected\n",
            "layer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\n",
            "and actual speeds, we achieved an average error of 3.6mph. Table 6 compares different models, and\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 24\n",
            "--------------------\n",
            "layer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\n",
            "and actual speeds, we achieved an average error of 3.6mph. Table 6 compares different models, and\n",
            "Fig. 8 illustrates the sub-events learned for various speeds.\n",
            "Table 6: Results for pitch speed regression on segmented videos, reporting root-mean-squared errors.\n",
            "Method Two-stream\n",
            "I3D 4.3 mph\n",
            "I3D + LSTM 4.1 mph\n",
            "I3D + sub-events 3.9 mph\n",
            "InceptionV3 5.3 mph\n",
            "InceptionV3 + LSTM 4.5 mph\n",
            "InceptionV3 + sub-events 3.6 mph\n",
            "6.2.3 Pitch Type Classification\n",
            "We conducted experiments to determine the feasibility of predicting pitch types from video, a task\n",
            "made challenging by pitchers’ efforts to disguise their pitches from batters and the subtle differences\n",
            "between pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\n",
            "utilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 25\n",
            "--------------------\n",
            "between pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\n",
            "utilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.\n",
            "Pose features were considered due to variations in body mechanics between different pitches. Our\n",
            "dataset includes six pitch types, with results presented in Table 7. LSTMs performed worse than the\n",
            "baseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the\n",
            "easiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult\n",
            "(12%).\n",
            "6.3 Continuous Video Activity Detection\n",
            "We evaluate models extended for continuous videos using per-frame mean average precision (mAP),\n",
            "with results shown in Table 8. This setting is more challenging than segmented videos, requiring\n",
            "the model to identify activity start and end times and handle ambiguous negative examples. All\n",
            "models improve upon the baseline per-frame classification, confirming the importance of temporal\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 26\n",
            "--------------------\n",
            "the model to identify activity start and end times and handle ambiguous negative examples. All\n",
            "models improve upon the baseline per-frame classification, confirming the importance of temporal\n",
            "information. Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal\n",
            "6\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 27\n",
            "--------------------\n",
            "Table 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose\n",
            "heatmaps.\n",
            "Method Accuracy\n",
            "Random 17.0%\n",
            "I3D 25.8%\n",
            "I3D + LSTM 18.5%\n",
            "I3D + sub-events 34.5%\n",
            "Pose 28.4%\n",
            "Pose + LSTM 27.6%\n",
            "Pose + sub-events 36.4%\n",
            "convolution appear to overfit. Convolutional sub-events, especially when combined with super-event\n",
            "representation, significantly enhance performance, particularly for frame-based features.\n",
            "Table 8: Performance on continuous videos for multi-label activity classification (per-frame mAP).\n",
            "Method RGB Flow Two-stream\n",
            "Random 13.4 13.4 13.4\n",
            "I3D 33.8 35.1 34.2\n",
            "I3D + max-pooling 34.9 36.4 36.8\n",
            "I3D + pyramid 36.8 37.5 39.7\n",
            "I3D + LSTM 36.2 37.3 39.4\n",
            "I3D + temporal conv 35.2 38.1 39.2\n",
            "I3D + sub-events 35.5 37.5 38.5\n",
            "I3D + super-events 38.7 38.6 39.1\n",
            "I3D + sub+super-events 38.2 39.4 40.4\n",
            "InceptionV3 31.2 31.8 31.9\n",
            "InceptionV3 + max-pooling 31.8 34.1 35.2\n",
            "InceptionV3 + pyramid 32.2 35.1 36.8\n",
            "InceptionV3 + LSTM 32.1 33.5 34.1\n",
            "InceptionV3 + temporal conv 28.4 34.4 33.4\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 28\n",
            "--------------------\n",
            "I3D + super-events 38.7 38.6 39.1\n",
            "I3D + sub+super-events 38.2 39.4 40.4\n",
            "InceptionV3 31.2 31.8 31.9\n",
            "InceptionV3 + max-pooling 31.8 34.1 35.2\n",
            "InceptionV3 + pyramid 32.2 35.1 36.8\n",
            "InceptionV3 + LSTM 32.1 33.5 34.1\n",
            "InceptionV3 + temporal conv 28.4 34.4 33.4\n",
            "InceptionV3 + sub-events 32.1 35.8 37.3\n",
            "InceptionV3 + super-events 31.5 36.2 39.6\n",
            "InceptionV3 + sub+super-events 34.2 40.2 40.9\n",
            "7 Conclusion\n",
            "This paper introduces MLB-YouTube, a novel and challenging dataset designed for detailed activity\n",
            "recognition in videos. We conduct a comparative analysis of various recognition techniques that\n",
            "employ temporal feature pooling for both segmented and continuous videos. Our findings reveal that\n",
            "learning sub-events to pinpoint temporal regions of interest significantly enhances performance in\n",
            "segmented video classification. In the context of activity detection in continuous videos, we establish\n",
            "that incorporating convolutional sub-events with a super-event representation, creating a three-level\n",
            "--------------------\n",
            "--------------------\n",
            "PAGE NUMBER 29\n",
            "--------------------\n",
            "segmented video classification. In the context of activity detection in continuous videos, we establish\n",
            "that incorporating convolutional sub-events with a super-event representation, creating a three-level\n",
            "activity hierarchy, yields the most favorable outcomes.\n",
            "7\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorestore1 = FAISS.from_documents(split_docs1, embeddings)\n",
        "vectorestore2 = FAISS.from_documents(split_docs2, embeddings)"
      ],
      "metadata": {
        "id": "AQ06wHyOymTG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retreiver1 = vectorestore1.as_retriever()\n",
        "retreiver2 = vectorestore2.as_retriever()"
      ],
      "metadata": {
        "id": "3E4MLaDy77zF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "id": "DnIk5o5k8Bhc",
        "outputId": "c705975e-ae38-4972-92b2-b900d4258ef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.29)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (3.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_groq) (2.2.3)\n",
            "Downloading langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain_groq\n",
            "Successfully installed groq-0.13.1 langchain_groq-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "api_key = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "D0PTs4IfB4bk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(groq_api_key=api_key, model=\"Llama3-8b-8192\", streaming=True)"
      ],
      "metadata": {
        "id": "k9C1mjuNCZNj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "id": "nnKNkVhPPBH2",
        "outputId": "bed65e4e-ce55-4efb-efd9-74a0283cab08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x787b3ccbd600>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x787b3ccbe8f0>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'), streaming=True)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "hy2lGJG0MNDB",
        "outputId": "0800e6d8-1ee0-4c8c-a94b-41522bf63499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m163.8/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n"
      ],
      "metadata": {
        "id": "C_FL-9TBMVlm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as pdf_file:\n",
        "        reader = PyPDF2.PdfReader(pdf_file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "M2tL6ubxOFGf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Function to load and extract text from PDF\n",
        "def load_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as pdf_file:\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text\n",
        "        if not text.strip():\n",
        "            raise ValueError(\"No text found in PDF. Check if it's scanned or encrypted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "# Query retrievers for Publishable and Not Publishable examples\n",
        "try:\n",
        "    publishable_context = vectorestore1.similarity_search(\"Methodology, coherence, and validity examples\", k=1)\n",
        "    not_publishable_context = vectorestore2.similarity_search(\"Methodology, coherence, and validity examples\", k=1)\n",
        "\n",
        "    # Fallback if no results\n",
        "    paper1 = publishable_context[0].page_content if publishable_context else \"Default Publishable Example\"\n",
        "    paper2 = not_publishable_context[0].page_content if not_publishable_context else \"Default Not Publishable Example\"\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving context: {e}\")\n",
        "    paper1 = \"Default Publishable Example\"\n",
        "    paper2 = \"Default Not Publishable Example\"\n",
        "\n",
        "# Updated Prompt\n",
        "prompt = f\"\"\"\n",
        "You will be given a research paper, and your task is to classify it as \"Publishable\" or \"Not Publishable\".\n",
        "For reference, I have provided two examples:\n",
        "\n",
        "1. Publishable Paper:\n",
        "{paper1}\n",
        "\n",
        "2. Not Publishable Paper:\n",
        "{paper2}\n",
        "\n",
        "Criteria for classification:\n",
        "- Methodology clarity: Is the methodology well-defined and replicable?\n",
        "- Coherence: Does the paper follow a logical structure?\n",
        "- Validity: Are the results justified with evidence?\n",
        "\n",
        "Now, analyze the following research paper and provide your classification along with a brief justification.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "3vE3eCo0CtjE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_chunk_size=1500):\n",
        "    \"\"\"\n",
        "    Splits the text into smaller chunks with a max size limit (in characters).\n",
        "    You may adjust max_chunk_size based on your token limit.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    text_length = len(text)\n",
        "\n",
        "    for start in range(0, text_length, max_chunk_size):\n",
        "        chunks.append(text[start:start + max_chunk_size])\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def process_chunks_with_confidence(input_paper):\n",
        "    \"\"\"\n",
        "    Process the input paper in chunks and aggregate classification with confidence scores.\n",
        "    \"\"\"\n",
        "    # Split the input paper into chunks\n",
        "    chunks = chunk_text(input_paper)\n",
        "\n",
        "    publishable_confidence = 0\n",
        "    not_publishable_confidence = 0\n",
        "    num_publishable = 0\n",
        "    num_not_publishable = 0\n",
        "\n",
        "    # Iterate over each chunk and classify it\n",
        "    for chunk in chunks:\n",
        "        context = f\"{prompt}\\n\\nResearch Paper:\\n{chunk}\"\n",
        "\n",
        "        try:\n",
        "            # Pass the chunk to the model and get the classification and confidence score\n",
        "            response = llm.invoke(context)  # Assuming the response includes a classification and confidence\n",
        "\n",
        "            # Print the raw response to debug its structure\n",
        "            print(\"Response:\", response)\n",
        "\n",
        "            # Extract classification from the response string\n",
        "            if \"Classification: Publishable\" in response.content:\n",
        "                classification = \"Publishable\"\n",
        "                confidence = 1.0  # Assuming high confidence if it's classified as publishable\n",
        "            elif \"Classification: Not Publishable\" in response.content:\n",
        "                classification = \"Not Publishable\"\n",
        "                confidence = 1.0  # Assuming high confidence if it's classified as not publishable\n",
        "            else:\n",
        "                classification = \"Unknown\"\n",
        "                confidence = 0.0\n",
        "\n",
        "            # Now process confidence scores based on classification\n",
        "            if classification == \"Publishable\":\n",
        "                publishable_confidence += confidence\n",
        "                num_publishable += 1\n",
        "            else:\n",
        "                not_publishable_confidence += confidence\n",
        "                num_not_publishable += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in RAG chain: {e}\")\n",
        "            return \"Error processing the paper.\"\n",
        "\n",
        "    # Calculate average confidence for each classification\n",
        "    avg_publishable_confidence = publishable_confidence / num_publishable if num_publishable else 0\n",
        "    avg_not_publishable_confidence = not_publishable_confidence / num_not_publishable if num_not_publishable else 0\n",
        "\n",
        "    # Final classification based on average confidence\n",
        "    if avg_publishable_confidence > avg_not_publishable_confidence:\n",
        "        return \"Publishable\"\n",
        "    else:\n",
        "        return \"Not Publishable\"\n",
        "\n",
        "# Example Input\n",
        "file_path = \"/content/drive/MyDrive/KDSH_2025_Dataset/Reference/Non-Publishable/R001.pdf\"\n",
        "input_paper = load_pdf(file_path)\n",
        "\n",
        "if input_paper.strip():\n",
        "    response = process_chunks_with_confidence(input_paper)\n",
        "    print(\"Final Classification Result with Confidence:\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"No content extracted from the input paper.\")\n"
      ],
      "metadata": {
        "id": "m6ob1fVPLJt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c485256-6263-4dd1-e88e-d009765f4190"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification based on the criteria:\\n\\n* Methodology clarity: The paper lacks any clear methodology or explanation of how the research was conducted. The text is filled with vague and unrelated concepts, making it impossible to replicate the study.\\n* Coherence: The paper does not follow a logical structure. The abstract and introduction are filled with unrelated concepts, and the text jumps abruptly from discussing graphite to discussing dolphins, dark matter, and sentient toasters.\\n* Validity: The results presented in the paper are not justified with evidence. The claims made are absurd and unfounded, and the paper provides no concrete data or supporting research to back up its assertions.\\n\\nOverall, the paper lacks a clear research question, methodology, and logical structure, making it unsuitable for publication in a reputable academic journal.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-d2a56ab1-e563-473c-9eca-43afdec2b367-0' usage_metadata={'input_tokens': 774, 'output_tokens': 173, 'total_tokens': 947}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear and well-defined methodology. The concepts of superposition and entanglement from quantum mechanics are applied to the baking of croissants, but it is unclear how this is done or what specific measurements or experiments were conducted. The paper jumps abruptly from discussing croissants to Shakespeare\\'s sonnets and graphitic carbon without providing a clear connection between these topics.\\n* Coherence: The paper\\'s structure is disjointed and lacks a logical flow. The author presents a series of seemingly unrelated ideas and concepts without providing a clear thread or thesis statement to tie them together.\\n* Validity: The results presented in the paper are not justified with evidence. The claims made about the influence of quantum mechanics on croissant baking, the connection between sonnets and pastry crusts, and the role of graphite in wind turbine blades are not supported by scientific data or empirical research.\\n\\nOverall, the paper appears to be a collection of unrelated ideas and concepts strung together without a clear purpose or methodology. As such, it does not meet the criteria for a publishable paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-e34b7f1e-ddfc-47d7-8710-433b40f04bd2-0' usage_metadata={'input_tokens': 774, 'output_tokens': 239, 'total_tokens': 1013}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear methodology. The text is filled with vague statements, such as \"unique properties of graphitic carbon\" and \"curiously, has been found to be inversely proportional to the number of times one listens to the music of Mozart\". There is no explanation of how the relationships between Mozart\\'s music, graphite, and fungal mycelium were established.\\n* Coherence: The paper\\'s structure is disjointed and lacks a logical flow. The text jumps abruptly from discussing the properties of graphitic carbon to the impact of Mozart\\'s music, and then to the themes of Kafka\\'s works, and so on. The connections between these topics are not clear.\\n* Validity: The paper does not provide any evidence or justification for its claims. The statements made are often vague, unsubstantiated, and seem to be more focused on showcasing the author\\'s creativity rather than presenting a rigorous scientific argument.\\n\\nOverall, the paper lacks a clear and coherent scientific argument, and its claims are not supported by evidence. It appears to be more of a collection of unrelated and fantastical ideas rather than a legitimate research paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-bfa54961-0a77-4d07-8d05-e833081be447-0' usage_metadata={'input_tokens': 774, 'output_tokens': 248, 'total_tokens': 1022}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n1. Methodology clarity: The methodology is not clearly defined, and it\\'s unclear how the authors arrived at their conclusions. The paper jumps abruptly from discussing the principles of chaos theory to proposing a new theory of graphitic carbon, without providing a clear explanation of the research design, data collection and analysis methods, or the specific hypotheses tested.\\n2. Coherence: The paper lacks a logical structure. The authors introduce various concepts and ideas without connecting them to a clear research question or hypothesis. The writing is anecdotal and lacks a clear narrative thread.\\n3. Validity: The results presented in the paper are not justified with evidence. The claims made about the relationship between graphitic carbon and chaos theory, the Fibonacci sequence, and the electrical conductivity of graphite are not supported by empirical data or credible sources. The inclusion of seemingly unrelated topics, such as the impact of a specific type of flower on graphite conductivity and the themes of \"Oliver Twist\" on economic theory, further undermines the validity of the research.\\n\\nOverall, the paper lacks a clear research question, methodology, and results, making it difficult to evaluate its validity and relevance.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-2a3031ea-fbba-48de-8dfd-260c98a4e2ea-0' usage_metadata={'input_tokens': 767, 'output_tokens': 246, 'total_tokens': 1013}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nJustification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. It appears to be a collection of loosely connected ideas and concepts, without a clear research question, design, or analysis plan.\\n* Coherence: The paper does not follow a logical structure. It jumps abruptly from discussing the strategic incorporation of graphite into industrial processes to exploring the ontological implications of graphitic carbon, and then to the practical applications of graphite in sports equipment. The connections between these topics are not clear, and the paper lacks a cohesive narrative.\\n* Validity: The paper does not provide sufficient evidence to justify its claims. While it mentions some interesting analogies and connections between graphite and mythological figures, it does not provide any empirical evidence or data to support these claims. The paper\\'s reliance on abstract concepts and philosophical ideas makes it difficult to evaluate its validity.\\n\\nOverall, the paper lacks a clear research question, methodology, and analysis, and its connections between ideas are unclear. It appears to be more of a speculative essay than a research paper, and as such, it is not publishable in its current form.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-a1a90118-e367-496f-900e-0a20609b31f9-0' usage_metadata={'input_tokens': 765, 'output_tokens': 236, 'total_tokens': 1001}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The paper lacks a clear methodology or research design. The language is ambiguous and descriptive, with no explicit statement of the research question, objectives, or methods used to collect and analyze data.\\n* Coherence: The paper\\'s structure is disjointed and lacks a clear logical flow. The author jumps between different ideas and concepts, such as fractal patterns, quantum mechanics, Shakespeare, and existentialism, without providing a clear connection between them.\\n* Validity: The paper lacks empirical evidence to support its claims. The statements made about graphitic carbon, its properties, and its connections to various fields and theories are not substantiated with data or references to existing research.\\n\\nThe paper appears to be more of a philosophical or poetic exploration of the concept of graphitic carbon, rather than a scientific research paper. The language is often vague and descriptive, and the author makes grand claims about the significance of graphitic carbon without providing a clear justification or evidence to support them.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-cd2e7179-c15f-4094-975f-0cedc5a80e77-0' usage_metadata={'input_tokens': 769, 'output_tokens': 214, 'total_tokens': 983}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear methodology. The author jumps from discussing ancient mythology to modern applications of graphite and then to the connection between Mozart\\'s music and graphite\\'s properties. There is no clear explanation of how the research was conducted, what data was collected, or how the results were analyzed.\\n* Coherence: The paper\\'s structure is disjointed and lacks a logical flow. The author presents a series of unrelated ideas and concepts without connecting them to a broader research question or hypothesis.\\n* Validity: The results presented in the paper are not justified with evidence. The author makes sweeping claims about the properties of graphitic carbon and its connection to ancient mythology, but there is no empirical support for these claims. The mention of the inverse relationship between Mozart\\'s music and graphite\\'s thermal conductivity is particularly egregious, as it is a seemingly arbitrary and untestable claim.\\n\\nOverall, the paper lacks a clear research question, a logical structure, and empirical evidence to support its claims. It appears to be a collection of loosely connected ideas and anecdotes rather than a well-designed research study.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-2e4d75e3-1b79-4a8c-87f6-23309fab64ce-0' usage_metadata={'input_tokens': 771, 'output_tokens': 237, 'total_tokens': 1008}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nJustification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The connection between the fungus\\'s affinity for Kafka\\'s works and the properties of graphitic carbon is not explained, and the research seems to be based on anecdotal evidence rather than systematic analysis.\\n* Coherence: The paper jumps between different topics and disciplines without a clear logical structure. The connection between graphitic carbon, fractal geometry, chaos theory, and the Fibonacci sequence is not well-established, and the paper lacks a clear research question or objective.\\n* Validity: The results are not justified with evidence. The paper presents a series of loosely connected ideas and concepts without providing concrete data or empirical support. The connections between the properties of graphitic carbon and the abstract concepts of mathematics and philosophy are not well-supported, and the paper relies heavily on vague statements and speculative claims.\\n\\nOverall, the paper lacks a clear research question, methodology, or results, and the connections between the different concepts and ideas presented are not well-established. As a result, I classify it as \"Not Publishable\".' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-c2b1298d-36f6-4fda-ac19-26b60fa26d27-0' usage_metadata={'input_tokens': 768, 'output_tokens': 228, 'total_tokens': 996}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nJustification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The connections between graphite, the siren\\'s song, the specific type of flower, and the works of Dickens are not scientifically grounded and appear to be arbitrary.\\n* Coherence: The paper does not follow a logical structure. The text jumps abruptly from discussing the properties of graphite to Scandinavian furniture designers, badminton shuttlecocks, 19th-century French pastry recipes, and chrono-botany, without any clear connection between these topics.\\n* Validity: The results presented are not justified with evidence. The claims made about the properties of graphite, its connection to the siren\\'s song and the specific type of flower, and the alleged sentient properties of the mineral are not supported by empirical data or credible scientific research.\\n\\nOverall, the paper lacks a clear scientific framework, and the connections between the various topics discussed are not convincing. The writing style is also overly creative and lacks the rigor and precision expected in a scientific research paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-a0a4cc76-0f17-4af0-966f-337b5bc8fec6-0' usage_metadata={'input_tokens': 777, 'output_tokens': 216, 'total_tokens': 993}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification based on the criteria:\\n\\n* Methodology clarity: The paper lacks a clear and well-defined methodology. The connection between graphite and various seemingly unrelated topics (narwhal-based algorithms, Byzantine mosaic art, traditional Kenyan basket-weaving, monarch butterflies, and Mongolian throat singing) is not justified or explained in a logical manner.\\n* Coherence: The paper does not follow a logical structure. The topics are presented in a disjointed and unrelated manner, making it difficult to follow the author\\'s train of thought.\\n* Validity: The results presented in the paper are not justified with evidence. The connections between graphite and these various topics are not supported by empirical data or scientific evidence. The paper appears to be more focused on showcasing intriguing and unconventional relationships rather than conducting rigorous research.\\n\\nOverall, the paper lacks a clear research question, methodology, and evidence-based results, making it difficult to evaluate its validity and relevance to the scientific community.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-d27b3df4-aceb-45e3-afd7-b79a7686320d-0' usage_metadata={'input_tokens': 764, 'output_tokens': 206, 'total_tokens': 970}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The connections between ancient Sumerian pottery inscriptions, graphite mining in Cornwall, and supersonic aircraft aerodynamics are not well-explained or supported by empirical evidence.\\n* Coherence: The paper jumps abruptly from discussing ancient Sumerian pottery to modern-day aircraft and high-performance computing applications, without providing a clear logical structure or transition between topics.\\n* Validity: The paper makes several claims that are not justified by evidence, such as the existence of a \"hidden graphitic quantum realm\" and the potential for graphitic materials to create quantum entanglement-based cryptography. These claims are speculative and lack empirical support.\\n\\nOverall, the paper appears to be a collection of unrelated ideas and concepts that are not well-integrated or supported by rigorous research methods. As a result, it does not meet the standards for publishability in a reputable scientific journal.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-1eaa1073-a6d2-4c9c-96ac-687e8594428d-0' usage_metadata={'input_tokens': 766, 'output_tokens': 204, 'total_tokens': 970}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n1. Methodology clarity: The paper lacks a clear and well-defined methodology. The connections between graphitic materials, mandalas, Scandinavian reindeer, ancient Egyptian scroll-making, and coffee makers are not adequately explained or justified.\\n2. Coherence: The paper\\'s structure is disjointed and lacks a clear logical flow. The author jumps from one seemingly unrelated topic to another without providing any cohesion or transitions between sections.\\n3. Validity: The results presented in the paper are not justified with evidence or empirical data. The connections between graphitic materials and various unrelated topics are speculative and lack any scientific basis.\\n4. Lack of rigor: The paper\\'s tone is more like a science fiction novel than a scientific research paper. The language is overly dramatic, and the claims made are exaggerated and lack any scientific credibility.\\n\\nOverall, this paper lacks the rigor, coherence, and validity expected in a publishable research paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-b1aaa8df-3dbf-4328-8243-2df3c956d06f-0' usage_metadata={'input_tokens': 766, 'output_tokens': 202, 'total_tokens': 968}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear methodology. The claims made about the \"hidden graphitic code\" and \"novel form of graphitic-based cryptography\" are not supported by any concrete evidence or explanation of how they were discovered or tested.\\n* Coherence: The paper jumps abruptly from discussing graphite-based cryptography to the migratory patterns of monarch butterflies, and then to the history of ancient Mesopotamian irrigation systems. The connections between these topics are not clear, and the paper lacks a logical structure.\\n* Validity: The paper makes several unsubstantiated claims, such as the supposed connection between graphite and ancient irrigation systems, or the resemblance between graphitic materials and traditional Inuit throat singing. These claims are not supported by empirical evidence or credible sources.\\n\\nOverall, the paper lacks a clear and replicable methodology, is incoherent, and makes unsubstantiated claims, making it difficult to classify as \"Publishable\".' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-95c0c0c5-76df-4779-875a-a2cb88ba47d7-0' usage_metadata={'input_tokens': 758, 'output_tokens': 207, 'total_tokens': 965}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear and well-defined methodology. The descriptions of the research are vague and seem to be a collection of unrelated statements, making it difficult to understand how the research was conducted.\\n* Coherence: The paper does not follow a logical structure. The topics discussed are unrelated, and the transitions between sentences and paragraphs are abrupt and unclear.\\n* Validity: The results presented in the paper are not justified with evidence. The claims made are speculative and lack concrete data or empirical support. The inclusion of unrelated topics, such as 19th-century Australian sheep herding and theoretical jellyfish mechanics, further undermines the validity of the research.\\n\\nOverall, the paper appears to be a collection of unrelated ideas and concepts, rather than a cohesive and well-structured research paper. The lack of clarity, coherence, and evidence-based results makes it difficult to justify the paper as publishable.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-02f792d3-d8a2-4c2e-adae-10b052d07176-0' usage_metadata={'input_tokens': 771, 'output_tokens': 199, 'total_tokens': 970}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nJustification:\\n\\n1. Methodology clarity: The paper lacks a clear methodology or explanation of how the connections between graphitic materials and Scandinavian reindeer patterns, traditional Chinese calligraphy, ancient Greek olive oil production, and aerodynamics of hot air balloons were made. The reader is presented with a series of seemingly unrelated facts and theories without any logical connection or explanation.\\n2. Coherence: The paper does not follow a logical structure. The topics mentioned are diverse and unrelated, making it difficult to follow the author\\'s train of thought.\\n3. Validity: The results are not justified with evidence. The paper presents a series of unsubstantiated claims and theories without providing any supporting data or references to credible sources. The connection between graphitic materials and the topics mentioned is not scientifically justified.\\n\\nThe paper appears to be a collection of unrelated and unsubstantiated claims, which is not acceptable for publication in a reputable scientific journal.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-81876f1d-e2fe-424b-9bf3-da55189c0173-0' usage_metadata={'input_tokens': 759, 'output_tokens': 196, 'total_tokens': 955}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The methodology is unclear and seems to be a mix of unrelated fields (quantum physics, pastry arts, and professional snail training). The use of simultaneous analysis of graphite samples and recitation of 19th-century French poetry is not justified or explained. The inclusion of ambient jazz music and vintage harmonica playing is also unexplained and seems unrelated to the research.\\n* Coherence: The paper lacks a clear logical structure. The connection between graphite, sound waves, and Tibetan throat singing is not clear, and the inclusion of pastry arts and professional snail training is not justified.\\n* Validity: The results are not justified with evidence, and the claims made in the paper seem to be speculative and unsubstantiated.\\n\\nOverall, the paper lacks a clear research question, methodology, and justification for the results. It appears to be a collection of unrelated ideas and claims that are not supported by evidence.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-952ff088-8efc-4d5f-8de5-bf23e664e734-0' usage_metadata={'input_tokens': 753, 'output_tokens': 204, 'total_tokens': 957}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification for my classification:\\n\\n* Methodology clarity: The paper lacks a clear and well-defined methodology. The description of the \"puzzle-solving and juggling components\" and the \"culinary applications of graphite\" is vague and unclear, making it difficult to understand how the data was collected and analyzed.\\n* Coherence: The paper appears to jump abruptly from discussing cryptography and juggling to culinary applications of graphite, without any clear connection between the different topics. The logic of the paper is unclear, and it is difficult to see how the various components fit together to form a cohesive whole.\\n* Validity: The paper relies heavily on anecdotal evidence and lacks any empirical support for its claims. The \"fascinating discovery\" regarding the synergistic effects of graphite and cucumber sauce on the human palate is not backed up by any data or research, and the connection between graphite and the evolution of human taste preferences is highly speculative.\\n\\nOverall, the paper lacks a clear methodology, is incoherent, and relies on unverified claims, making it difficult to justify as publishable.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-5c1069a6-e111-4690-9ce9-75254dcc6267-0' usage_metadata={'input_tokens': 764, 'output_tokens': 232, 'total_tokens': 996}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification for my classification:\\n\\n* Methodology clarity: The paper lacks a clear description of the methodology used to collect and analyze the data. It seems to be a collection of unrelated observations and findings, without any clear explanation of how the data was gathered or analyzed.\\n* Coherence: The paper\\'s structure is disjointed and lacks a logical flow. It jumps from discussing the human-graphite interface to exploring the connections between graphite, the global economy, and whale migration patterns, and then to the relationship between graphite and music genres. This makes it difficult to follow the author\\'s arguments and understand the relevance of the findings.\\n* Validity: The paper\\'s claims are not supported by evidence, and the connections between graphite and the various topics discussed seem arbitrary and unfounded. For example, the connection between graphite and professional wrestling is not explained, and the claim that the physical properties of graphite are similar to those of the human body during a wrestling match is not supported by any scientific evidence.\\n\\nOverall, the paper lacks a clear and replicable methodology, is poorly structured, and makes unsubstantiated claims. As a result, it is not publishable in its current form.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-bea6bbc8-7a43-48d9-9725-21c1576c8880-0' usage_metadata={'input_tokens': 757, 'output_tokens': 250, 'total_tokens': 1007}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The paper lacks a clear description of the methodology used to develop novel graphite-based materials, their application in wrestling costumes, and the graphite-inspired strategies in competitive wrestling matches. The paper jumps abruptly from discussing the properties of graphite to its applications in various fields without explaining how the research was conducted.\\n* Coherence: The paper\\'s structure is disjointed, switching abruptly between discussing wrestling, ancient mythology, and aerospace engineering. There is no clear thread connecting these topics, making it difficult to follow the author\\'s argument.\\n* Validity: The paper\\'s claims are not supported by evidence, and the connection between graphite and ancient mythology is not explained. The study\\'s findings are presented as surprising and groundbreaking, but there is no data or analysis to back up these claims.\\n\\nOverall, the paper lacks a clear research question, methodology, and results, making it difficult to evaluate its validity and relevance. The paper\\'s structure is also confusing, making it hard to follow the author\\'s argument.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-4b0ca76f-9b58-498e-90da-ca304b5fcce2-0' usage_metadata={'input_tokens': 741, 'output_tokens': 217, 'total_tokens': 958}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The author claims to have used a \"groundbreaking approach\" that incorporates multiple disciplines, but the specific methods used are not described. This makes it impossible for readers to replicate the study or evaluate the validity of the results.\\n* Coherence: The paper has a disjointed structure, jumping abruptly from discussing graphite\\'s properties in aerospace applications to its role in shaping human history. The connections between these topics are not clear, and the paper lacks a logical flow.\\n* Validity: The results presented in the paper are not justified with evidence. The author makes grand claims about the importance of graphite in shaping the course of human history, but there is no supporting data or analysis to back up these claims. The paper also lacks references to existing literature on the topic, which further undermines its validity.\\n\\nOverall, the paper lacks a clear and replicable methodology, is poorly structured, and presents unsubstantiated claims. These issues make it difficult to evaluate the validity of the research and its findings, and therefore, I would classify it as \"Not Publishable\".' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-6a7ba559-6a89-411c-a3fa-83e014126bf2-0' usage_metadata={'input_tokens': 756, 'output_tokens': 242, 'total_tokens': 998}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nJustification:\\n\\n* Methodology clarity: The paper lacks a clear description of the methodology used in the investigation. The text only mentions the \"discovery\" of graphite\\'s properties and applications, but does not provide any details on how these discoveries were made or what data was collected.\\n* Coherence: The paper jumps abruptly from discussing graphite\\'s applications in medicine to its role in modern art, with no clear connection between the two topics. The text lacks a logical structure and does not provide a clear research question or thesis statement.\\n* Validity: The paper\\'s claims are not justified with evidence. There is no mention of data analysis, statistical methods, or any other form of empirical evidence to support the claims made about graphite\\'s properties and applications. The text relies heavily on anecdotal evidence and personal opinions.\\n\\nOverall, the paper lacks the necessary methodological clarity, coherence, and validity to be considered publishable in a reputable scientific journal.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-46192d5e-ded4-41f0-af01-7be113856ad3-0' usage_metadata={'input_tokens': 751, 'output_tokens': 199, 'total_tokens': 950}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The paper lacks a clear definition of the methodology used in the research. The text jumps abruptly from discussing graphite\\'s properties in environmental sustainability to exploring its role in shaping human consciousness. The transition between these two topics is unclear, and the reader is left wondering how the research was designed and executed.\\n* Coherence: The paper\\'s structure is disjointed, and the ideas presented seem to be unrelated. The text bounces between discussing graphite\\'s properties, its potential applications, and its role in spiritual practices. This lack of coherence makes it difficult to follow the author\\'s argument and understand the significance of their findings.\\n* Validity: The paper lacks any concrete evidence or data to support its claims. The author presents vague statements about graphite\\'s properties and their potential applications, but there is no empirical evidence to back up these claims. The inclusion of claims about the efficacy of graphite-based spiritual practices and psychedelic devices is particularly concerning, as it raises questions about the validity and reliability of these findings.\\n\\nOverall, the paper lacks a clear methodology, coherent structure, and credible evidence, making it difficult to classify as \"Publishable\".' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-d7954292-9db9-4d52-b0f8-2d730c9b9883-0' usage_metadata={'input_tokens': 748, 'output_tokens': 244, 'total_tokens': 992}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks clear methodology on how the advanced mathematical models were applied to the dataset, making it difficult to replicate the results.\\n* Coherence: The paper\\'s structure is disjointed and lacks a logical flow. The connections between graphite, the human brain, and the global economy are not convincingly demonstrated. The sudden introduction of clowning and mythological studies seems unrelated to the initial topic of graphite market analysis.\\n* Validity: The paper relies heavily on unfounded or unproven relationships between graphite and various phenomena, such as literature, clowning, and mythology. The claims made are not supported by concrete evidence or empirical data. The use of vague terms like \"complex network of relationships\" and \"intricate web of causality\" does not provide sufficient justification for the results.\\n\\nOverall, the paper lacks a clear and replicable methodology, is incoherent in its structure, and makes unsubstantiated claims, making it difficult to classify as \"Publishable\".' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-09468f3d-30f4-4eb8-ab68-bfbd6041ef7c-0' usage_metadata={'input_tokens': 758, 'output_tokens': 218, 'total_tokens': 976}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The methodology is not well-defined and replicable. The paper describes a series of unconnected events, including a dance routine, a discussion on aerodynamics, and an analysis of monarch butterflies and chocolate supply. It is unclear how these events relate to the research question or how they contribute to the main findings.\\n* Coherence: The paper lacks a logical structure and jumps between unrelated topics. The connections between the different sections are not clear, and the reader is left wondering how the various elements are related to each other or to the research question.\\n* Validity: The results are not justified with evidence. The paper presents a series of seemingly unrelated facts and observations without providing any concrete evidence or data to support the claims. The use of unconnected concepts and events raises questions about the validity and reliability of the research.\\n\\nOverall, the paper appears to be a collection of unrelated ideas and observations rather than a coherent and well-structured research paper. The lack of methodology clarity, coherence, and validity make it difficult to evaluate the research and its findings.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-d7c912ee-ab51-4c3e-9935-51769a59c36e-0' usage_metadata={'input_tokens': 762, 'output_tokens': 232, 'total_tokens': 994}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nJustification:\\n\\n* Methodology clarity: The methodology is not well-defined and appears to be unreplicable. The use of trained seals to balance balls on their noses to measure electrical conductivity is an unconventional and unclear technique. Similarly, the simulation of a black hole using a vintage typewriter and supercomputers is not a conventional or scientifically valid approach.\\n* Coherence: The paper lacks a logical structure and appears to be a collection of unrelated experiments and findings. The connections between graphite, musical notes, harmonica playing, trained seals, black holes, Edgar Allan Poe, professional snail racers, and cheese production are not clear or scientifically justified.\\n* Validity: The results presented in the paper are not justified with evidence and appear to be anecdotal or speculative. The discovery of a new species of fungi, the surprising conclusion about the importance of graphite in cheese production, and the breakthrough in quantum computing are not supported by rigorous scientific methods or data.\\n\\nOverall, the paper lacks scientific rigor, methodology, and coherence, making it difficult to classify as publishable in a reputable scientific journal.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-22a1fae8-a44d-4d7c-a3d1-8c065b7fd13e-0' usage_metadata={'input_tokens': 766, 'output_tokens': 231, 'total_tokens': 997}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification for each of the criteria:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The use of an abacus, juggling pins, Ouija board, crystal ball, and other unconventional tools makes it impossible to replicate the experiments. The paper does not provide a clear explanation of how the statistical technique was developed or how the results were analyzed.\\n* Coherence: The paper does not follow a logical structure. The introduction is vague, and the experiments described are unrelated to each other. The paper jumps abruptly from discussing the use of graphite in time machines to conducting an experiment on plant growth.\\n* Validity: The results presented in the paper are not justified with evidence. The correlations and patterns described are unexplained and seem to be unrelated to the actual properties of graphite. The paper relies on absurd and unrelated associations, such as the connection between graphite density and the airspeed velocity of an unladen swallow.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-20e8247f-f9c8-4539-81d8-29e8716e125b-0' usage_metadata={'input_tokens': 797, 'output_tokens': 204, 'total_tokens': 1001}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The methodology is unclear and unreplicable. The experiment appears to involve using a pinball machine and tarot cards, which is not a conventional or scientifically supported method. The use of a didgeridoo and a secret location guarded by ninjas also raises questions about the validity of the experiment.\\n* Coherence: The paper lacks a logical structure and jumps abruptly from discussing the properties of graphite to using a pinball machine and tarot cards to analyze the data. The connection between the acoustic properties of graphite and the migratory patterns of wildebeests is not clear.\\n* Validity: The results are not justified with evidence. The use of unconventional methods and the lack of transparency about the experimental design and procedures make it difficult to assess the validity of the findings.\\n\\nOverall, this paper appears to be more of a creative writing exercise than a serious scientific research paper. While it may be an entertaining read, it does not meet the standards for a publishable research paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-8f2b732c-aa9c-4d73-9697-8d6de108ea5c-0' usage_metadata={'input_tokens': 799, 'output_tokens': 218, 'total_tokens': 1017}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n1. Methodology clarity: The methodology is unclear and appears to be a jumbled mix of unrelated concepts and techniques. The use of a network of supercomputers, expert gamers, and a virtual reality version of Pac-Man to test a theory is not a valid or replicable method.\\n2. Coherence: The paper lacks a logical structure and jumps abruptly from discussing a phenomenon called \"Induced Quantum Fluctuations\" to describing a complex computer simulation involving games and a deck of cards. The connection between these two topics is not clear.\\n3. Validity: The results are not justified with evidence and appear to be based on a series of unrelated and unconnected factors. The use of regression analysis, factor analysis, and a novel method involving a deck of cards and a crystal ball is unclear and lacks a clear connection to the research question.\\n\\nOverall, the paper lacks a clear research question, a valid methodology, and a coherent structure, making it difficult to understand or replicate the results.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-45526d14-ef6c-4abb-a071-ed8f5eb9c7d0-0' usage_metadata={'input_tokens': 794, 'output_tokens': 218, 'total_tokens': 1012}\n",
            "Response: content='Classification: Not Publishable\\n\\nJustification:\\n\\n1. Methodology clarity: The methodology is unclear and lacks detail. The paper mentions the use of a team of expert typists to transcribe documents, but it does not provide any information on how the documents were selected, how the typists were trained, or how the transcription process was validated. The paper also mentions the use of expert musicians to analyze the properties of the Super-Graphite Material, but it does not provide any information on how this analysis was conducted or how the results were validated.\\n\\n2. Coherence: The paper lacks a clear logical structure. The introduction does not provide a clear research question or hypothesis, and the methodology section appears to be a collection of unrelated events and actions. The results section is also unclear and does not provide any evidence to support the claims made in the paper.\\n\\n3. Validity: The paper lacks any evidence to support its claims. The results are based on the analysis of a 1000-page report that is only available in a limited edition of 10 copies, and the study relies on the expertise of a team of typists and musicians who are not qualified to conduct scientific research. The paper also makes claims about the potential uses of graphite in high-performance sports equipment and aerospace components, but it does not provide any evidence to support these claims.\\n\\nOverall, the paper lacks methodological clarity, coherence, and validity, and it is therefore not publishable in its current form.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-9f193015-e9e8-435f-93b8-07ec457fbd77-0' usage_metadata={'input_tokens': 785, 'output_tokens': 296, 'total_tokens': 1081}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The methodology used in this research is unclear and lacks replicability. The use of a deck of cards and a crystal ball, as well as a novel technique involving expert typists, raises significant questions about the validity and reliability of the results. Additionally, the experiment involving the Graphite Mega-Structure (GMS) is not well-defined, and it\\'s unclear how the results were analyzed.\\n* Coherence: The paper lacks a logical structure, jumping abruptly from one idea to another without providing a clear connection between the different sections. The inclusion of unrelated topics, such as the history of ancient Egyptian medicine and the art of astrology, further contributes to the lack of coherence.\\n* Validity: The results presented in the paper are not justified with evidence. The claims made about the thermal conductivity of graphite and the average lifespan of a domestic cat are not supported by any credible data or analysis. The use of unproven and unconventional methods, such as the deck of cards and the crystal ball, further undermines the validity of the results.\\n\\nOverall, this paper lacks the clarity, coherence, and validity required for a publishable research paper. The methodology is unclear, the results are not supported by evidence, and the paper lacks a logical structure.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-e9520a15-f60a-42f6-b2e9-3efdb9151791-0' usage_metadata={'input_tokens': 787, 'output_tokens': 271, 'total_tokens': 1058}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification for my classification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The study is described as a \"comprehensive study\" but the details of the experimental design, sampling procedure, and data collection methods are not provided. This makes it difficult to assess the validity and reliability of the results.\\n* Coherence: The paper\\'s structure is disjointed and lacks a clear logical flow. The introduction jumps abruptly from discussing the properties of graphite to mentioning a limited edition book and secret agents. The results section is similarly unclear, with no clear connection between the findings on graphite\\'s affinity for 19th-century French literature and the migratory patterns of monarch butterflies.\\n* Validity: The results presented in the paper are not justified with evidence. The claims made about graphite\\'s properties and its potential applications are not supported by credible scientific data or research.\\n\\nOverall, the paper lacks the clarity, coherence, and validity required for a publishable research paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-5a758daf-971c-440d-837e-1b67040cbe93-0' usage_metadata={'input_tokens': 770, 'output_tokens': 210, 'total_tokens': 980}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear methodology or research design. It appears to be a collection of loosely related ideas and concepts rather than a structured investigation.\\n* Coherence: The paper jumps between different topics, including the properties of graphite, the human brain, learning and memory, and materials science, without a clear connection between them. The transitions between sections are abrupt and lack logical flow.\\n* Validity: The paper does not present any original research or evidence to support its claims. Instead, it relies on vague statements and analogies, such as comparing the properties of graphite to the process of learning and memory in the human brain. The paper does not provide any concrete data or results to justify its conclusions.\\n\\nOverall, the paper lacks a clear research question, methodology, and results, making it difficult to evaluate its validity and relevance.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-72dc99af-a8f3-4003-b1b1-24930da555ab-0' usage_metadata={'input_tokens': 774, 'output_tokens': 188, 'total_tokens': 962}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks any clear methodology or experimental design. The claims made about graphite\\'s properties and its effects on pastry culinary arts, time travel, and the human brain\\'s ability to process mathematical equations are not supported by any empirical evidence or credible research.\\n* Coherence: The paper\\'s structure is unclear and lacks a logical flow. The author jumps from discussing the structure of graphite to its potential applications in pastry culinary arts, time travel, and quantum mechanics, without providing any coherent explanation or evidence to support these claims.\\n* Validity: The results presented in the paper are not justified with evidence and appear to be fictional or speculative. The claims made about graphite\\'s ability to manipulate the space-time continuum, create portable wormholes, and impart unique flavor profiles to artisanal cheeses are not supported by any credible research or evidence.\\n\\nOverall, the paper lacks scientific rigor, clarity, and validity, making it unsuitable for publication in a reputable academic journal.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-4d225897-54a6-47b1-b9b9-fb591d6c9df5-0' usage_metadata={'input_tokens': 772, 'output_tokens': 212, 'total_tokens': 984}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The paper lacks a clear and well-defined methodology. The claims made are vague and seem to be based on non-existent or unverifiable data. There is no mention of experiments, data collection, or statistical analysis that would support the conclusions drawn.\\n* Coherence: The paper\\'s structure is illogical and lacks a clear narrative. The author jumps from discussing the effects of graphite on math problem-solving to its impact on plant biology, and then to its effects on human emotions and nostalgia. The connections between these topics are unclear and seem arbitrary.\\n* Validity: The results presented are not justified by evidence. The claims made are unsubstantiated and seem to be based on unverifiable assumptions. There is no mention of empirical data or statistical analysis that would support the conclusions drawn.\\n\\nOverall, the paper lacks the rigor and methodology required to support the claims made. The language used is also sensationalized and lacks academic rigor, making it difficult to take the paper seriously.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-98b5fd54-7dc2-4ed4-b6d4-40e64c1185fe-0' usage_metadata={'input_tokens': 773, 'output_tokens': 216, 'total_tokens': 989}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification for my classification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The claims made about the effects of graphite on emotional healing, the properties of graphite-based meta-materials, and the enhancement of flavor profile are not supported by a rigorous scientific approach. The paper relies on vague statements and unverifiable claims.\\n* Coherence: The paper jumps between different topics, such as the use of graphite in materials science, culinary arts, and emotional healing, without a clear logical structure. The connections between these topics are not well-established, and the paper lacks a clear hypothesis or research question.\\n* Validity: The paper lacks empirical evidence to support its claims. The results are not justified with evidence, and the paper relies on anecdotal statements and unsubstantiated claims. The paper also makes claims that are not empirically testable, such as the relationship between graphite and the presence of rare species of fungi.\\n\\nOverall, the paper lacks the rigor and scientific merit required for publication in a reputable journal. The claims made are not supported by empirical evidence, and the methodology is unclear and unreplicable.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-c88be3c6-526d-4ccf-8e68-9c4a22977d13-0' usage_metadata={'input_tokens': 780, 'output_tokens': 242, 'total_tokens': 1022}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The paper lacks a clear and well-defined methodology. The concepts mentioned, such as the correlation between graphite and the human brain\\'s ability to process complex mathematical equations, are not supported by any concrete evidence or research.\\n* Coherence: The paper\\'s structure is disjointed and lacks a logical flow. The author jumps from discussing graphite\\'s unique properties to making far-fetched claims about its potential applications in fields like time travel, artificial intelligence, and music composition.\\n* Validity: The paper\\'s results are not justified with evidence. The statements made about graphite\\'s properties and potential applications are not supported by any scientific research or data.\\n\\nThe paper appears to be a collection of unrelated ideas and concepts that are not grounded in reality. The language is also overly vague and lacks specific details, making it difficult to understand the author\\'s claims. Overall, the paper lacks the clarity, coherence, and validity required for a publishable research paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-31ae1aad-fdb7-4dc4-b13d-e8b26e0767bc-0' usage_metadata={'input_tokens': 756, 'output_tokens': 208, 'total_tokens': 964}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The claims made about graphite\\'s properties and its potential applications are not supported by empirical evidence or a clear explanation of the methods used to study graphite.\\n* Coherence: The paper jumps abruptly from discussing the creation of advanced musical instruments to the study of graphite\\'s sentience and its potential applications in art and biology. The connections between these topics are not clear, and the paper lacks a logical structure.\\n* Validity: The claims made in the paper are not justified by evidence. There is no mention of empirical research or data to support the claims about graphite\\'s properties or its potential applications. The paper reads more like a series of speculative ideas rather than a rigorous scientific study.\\n\\nOverall, the paper lacks the clarity, coherence, and validity required for a publishable research paper. The claims made are speculative and lack empirical support, and the paper does not follow a clear methodology or logical structure.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-221be109-2c5f-4ad0-993c-793dc8b2f039-0' usage_metadata={'input_tokens': 761, 'output_tokens': 211, 'total_tokens': 972}\n",
            "Response: content='I classify this research paper as \"Not Publishable\".\\n\\nHere\\'s my justification:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The claims made are vague and lack specific details on how the research was conducted. For example, how was the relationship between graphite and the human emotional spectrum established? What specific studies or experiments were conducted to demonstrate the impact of graphite on emotions?\\n* Coherence: The paper lacks a logical structure. The claims made are disjointed and lack a clear connection between each other. The paper jumps abruptly from discussing the graphite-powered harmonica to the relationship between graphite and emotions to the development of new materials. There is no clear thread of logic or reasoning connecting these topics.\\n* Validity: The paper lacks evidence to support its claims. The findings mentioned, such as the relationship between graphite and emotions, are not supported by credible research or data. The paper relies on vague statements and unsubstantiated claims, which lack validity.\\n\\nOverall, the paper lacks the rigor and clarity required to be considered publishable. The methodology is unclear, the paper lacks coherence, and the claims made are not supported by evidence.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-98375477-7ea9-48b5-a2f6-8d17ea585972-0' usage_metadata={'input_tokens': 771, 'output_tokens': 231, 'total_tokens': 1002}\n",
            "Response: content='I would classify this research paper as \"Not Publishable\".\\n\\nHere\\'s a brief justification:\\n\\n* Methodology clarity: The methodology is not clearly defined, and the paper lacks a clear description of how the research was conducted, what data was collected, and how it was analyzed. The claims made in the paper are not supported by any empirical evidence or data.\\n* Coherence: The paper lacks a logical structure, jumping abruptly from discussing the symbiotic relationship between fungi and graphite to the culinary applications of graphite. The connections between these topics are not clearly explained, and the paper does not provide a clear thesis statement or research question.\\n* Validity: The results presented in the paper are not justified with evidence, and many of the claims made are unverifiable or lack any scientific basis. The paper relies heavily on anecdotal evidence and makes sweeping statements about the effects of graphite on human emotions and behavior without providing any supporting data.\\n\\nOverall, the paper lacks a clear research question, methodology, and results, and the claims made are not supported by empirical evidence. It is not a publishable paper in its current form.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-5270706c-a411-4630-b7fb-167cad71ba1d-0' usage_metadata={'input_tokens': 771, 'output_tokens': 224, 'total_tokens': 995}\n",
            "Response: content='Based on the criteria provided, I would classify this research paper as \"Not Publishable\".\\n\\nThe main reasons for this classification are:\\n\\n* Methodology clarity: The paper lacks a clear and replicable methodology. The discussion of graphite, time travel, and jazz music does not provide a well-defined research approach or methodology.\\n* Coherence: The paper\\'s structure is unclear and lacks a logical flow. The transition from discussing graphite\\'s properties to time travel and jazz music is abrupt and disconnected.\\n* Validity: The results presented are not justified with evidence. The claims made about the potential applications of graphite, such as creating sentient beings or advanced materials, are not supported by scientific evidence or data.\\n\\nThe paper appears to be more of a speculative and imaginative exercise rather than a rigorous scientific research paper. The language used is often vague and lacks technical precision, making it difficult to evaluate the paper\\'s claims. Overall, the paper does not meet the standards of a publishable research paper.' additional_kwargs={} response_metadata={'finish_reason': 'stop'} id='run-cb1e2bb5-cfa8-4c6b-bdf8-f41c7c86c154-0' usage_metadata={'input_tokens': 643, 'output_tokens': 196, 'total_tokens': 839}\n",
            "Final Classification Result with Confidence:\n",
            "Not Publishable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TEY8K-t400-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}